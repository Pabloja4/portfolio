{
  
    
        "post0": {
            "title": "Forecasting de precios usando FB Prophet",
            "content": "Un análisis sencillo de ventas y un forecasting de crecimiento, según la época del año y diferentes tiendas . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import datetime import prophet . Se cargan dos data-sets . sales_train_df = pd.read_csv(r&quot;C: Users pablo Documents Data_Science ventas train.csv&quot;, low_memory=False) . sales_train_df.head() . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | 0 | 1 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | 0 | 1 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | 0 | 1 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | 0 | 1 | . sales_train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1017209 entries, 0 to 1017208 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Store 1017209 non-null int64 1 DayOfWeek 1017209 non-null int64 2 Date 1017209 non-null object 3 Sales 1017209 non-null int64 4 Customers 1017209 non-null int64 5 Open 1017209 non-null int64 6 Promo 1017209 non-null int64 7 StateHoliday 1017209 non-null object 8 SchoolHoliday 1017209 non-null int64 dtypes: int64(7), object(2) memory usage: 69.8+ MB . sales_train_df.describe() . Store DayOfWeek Sales Customers Open Promo SchoolHoliday . count 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | . mean 5.584297e+02 | 3.998341e+00 | 5.773819e+03 | 6.331459e+02 | 8.301067e-01 | 3.815145e-01 | 1.786467e-01 | . std 3.219087e+02 | 1.997391e+00 | 3.849926e+03 | 4.644117e+02 | 3.755392e-01 | 4.857586e-01 | 3.830564e-01 | . min 1.000000e+00 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.800000e+02 | 2.000000e+00 | 3.727000e+03 | 4.050000e+02 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 50% 5.580000e+02 | 4.000000e+00 | 5.744000e+03 | 6.090000e+02 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 75% 8.380000e+02 | 6.000000e+00 | 7.856000e+03 | 8.370000e+02 | 1.000000e+00 | 1.000000e+00 | 0.000000e+00 | . max 1.115000e+03 | 7.000000e+00 | 4.155100e+04 | 7.388000e+03 | 1.000000e+00 | 1.000000e+00 | 1.000000e+00 | . store_info_df = pd.read_csv(r&quot;C: Users pablo Documents Data_Science ventas store.csv&quot;, low_memory=False) . store_info_df.head() . Store StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 1 2 | a | a | 570.0 | 11.0 | 2007.0 | 1 | 13.0 | 2010.0 | Jan,Apr,Jul,Oct | . 2 3 | a | a | 14130.0 | 12.0 | 2006.0 | 1 | 14.0 | 2011.0 | Jan,Apr,Jul,Oct | . 3 4 | c | c | 620.0 | 9.0 | 2009.0 | 0 | NaN | NaN | NaN | . 4 5 | a | a | 29910.0 | 4.0 | 2015.0 | 0 | NaN | NaN | NaN | . store_info_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1115 entries, 0 to 1114 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 Store 1115 non-null int64 1 StoreType 1115 non-null object 2 Assortment 1115 non-null object 3 CompetitionDistance 1112 non-null float64 4 CompetitionOpenSinceMonth 761 non-null float64 5 CompetitionOpenSinceYear 761 non-null float64 6 Promo2 1115 non-null int64 7 Promo2SinceWeek 571 non-null float64 8 Promo2SinceYear 571 non-null float64 9 PromoInterval 571 non-null object dtypes: float64(5), int64(2), object(3) memory usage: 87.2+ KB . store_info_df.describe() . Store CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear . count 1115.00000 | 1112.000000 | 761.000000 | 761.000000 | 1115.000000 | 571.000000 | 571.000000 | . mean 558.00000 | 5404.901079 | 7.224704 | 2008.668857 | 0.512108 | 23.595447 | 2011.763573 | . std 322.01708 | 7663.174720 | 3.212348 | 6.195983 | 0.500078 | 14.141984 | 1.674935 | . min 1.00000 | 20.000000 | 1.000000 | 1900.000000 | 0.000000 | 1.000000 | 2009.000000 | . 25% 279.50000 | 717.500000 | 4.000000 | 2006.000000 | 0.000000 | 13.000000 | 2011.000000 | . 50% 558.00000 | 2325.000000 | 8.000000 | 2010.000000 | 1.000000 | 22.000000 | 2012.000000 | . 75% 836.50000 | 6882.500000 | 10.000000 | 2013.000000 | 1.000000 | 37.000000 | 2013.000000 | . max 1115.00000 | 75860.000000 | 12.000000 | 2015.000000 | 1.000000 | 50.000000 | 2015.000000 | . EDA . sales_train_df.isna().sum() . Store 0 DayOfWeek 0 Date 0 Sales 0 Customers 0 Open 0 Promo 0 StateHoliday 0 SchoolHoliday 0 dtype: int64 . store_info_df.isna().sum() . Store 0 StoreType 0 Assortment 0 CompetitionDistance 3 CompetitionOpenSinceMonth 354 CompetitionOpenSinceYear 354 Promo2 0 Promo2SinceWeek 544 Promo2SinceYear 544 PromoInterval 544 dtype: int64 . Algunos nulos de los que hay que encargarse . sales_train_df.hist(bins = 30, figsize=(20,20), color= &quot;r&quot;) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;Store&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;DayOfWeek&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Sales&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Customers&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Open&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;SchoolHoliday&#39;}&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . sales_open= sales_train_df[sales_train_df[&quot;Open&quot;]== 1] sales_closed = sales_train_df[sales_train_df[&quot;Open&quot;]== 0] . print(&quot;Porcentaje de tiendas cerradas = {}%&quot;.format(100.0*len(sales_closed)/len(sales_train_df))) . Porcentaje de tiendas cerradas = 16.98933060954042% . sales_train_df = sales_train_df[sales_train_df[&quot;Open&quot;]==1] . sales_train_df.drop(columns=&quot;Open&quot;, axis=1, inplace=True) . sales_train_df.describe() . Store DayOfWeek Sales Customers Promo SchoolHoliday . count 844392.000000 | 844392.000000 | 844392.000000 | 844392.000000 | 844392.000000 | 844392.000000 | . mean 558.422920 | 3.520361 | 6955.514291 | 762.728395 | 0.446352 | 0.193580 | . std 321.731914 | 1.723689 | 3104.214680 | 401.227674 | 0.497114 | 0.395103 | . min 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 280.000000 | 2.000000 | 4859.000000 | 519.000000 | 0.000000 | 0.000000 | . 50% 558.000000 | 3.000000 | 6369.000000 | 676.000000 | 0.000000 | 0.000000 | . 75% 837.000000 | 5.000000 | 8360.000000 | 893.000000 | 1.000000 | 0.000000 | . max 1115.000000 | 7.000000 | 41551.000000 | 7388.000000 | 1.000000 | 1.000000 | . store_info_df.isna().sum() . Store 0 StoreType 0 Assortment 0 CompetitionDistance 3 CompetitionOpenSinceMonth 354 CompetitionOpenSinceYear 354 Promo2 0 Promo2SinceWeek 544 Promo2SinceYear 544 PromoInterval 544 dtype: int64 . store_info_df.head() . Store StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 1 2 | a | a | 570.0 | 11.0 | 2007.0 | 1 | 13.0 | 2010.0 | Jan,Apr,Jul,Oct | . 2 3 | a | a | 14130.0 | 12.0 | 2006.0 | 1 | 14.0 | 2011.0 | Jan,Apr,Jul,Oct | . 3 4 | c | c | 620.0 | 9.0 | 2009.0 | 0 | NaN | NaN | NaN | . 4 5 | a | a | 29910.0 | 4.0 | 2015.0 | 0 | NaN | NaN | NaN | . Completamos los valores nan . replacing_cols = [&quot;Promo2SinceWeek&quot;, &quot;Promo2SinceYear&quot;, &quot;PromoInterval&quot;, &quot;CompetitionOpenSinceMonth&quot;, &quot;CompetitionOpenSinceYear&quot;] for col in replacing_cols: store_info_df[col].fillna(0, inplace=True) . store_info_df[&quot;CompetitionDistance&quot;].fillna(store_info_df[&quot;CompetitionDistance&quot;].mean(), inplace=True) . store_info_df.isnull().sum() . Store 0 StoreType 0 Assortment 0 CompetitionDistance 0 CompetitionOpenSinceMonth 0 CompetitionOpenSinceYear 0 Promo2 0 Promo2SinceWeek 0 Promo2SinceYear 0 PromoInterval 0 dtype: int64 . store_info_df.hist(bins= 30, figsize=(20,20), color = &quot;g&quot;) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;Store&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;CompetitionDistance&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;CompetitionOpenSinceMonth&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;CompetitionOpenSinceYear&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo2&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo2SinceWeek&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo2SinceYear&#39;}&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . sales_store_df = pd.merge(sales_train_df, store_info_df, how = &quot;inner&quot;, on= &quot;Store&quot;) . sales_store_df.to_csv(&quot;sales_store.csv&quot;, index=False) . sales_store_df.head() . Store DayOfWeek Date Sales Customers Promo StateHoliday SchoolHoliday StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 1 1 | 4 | 2015-07-30 | 5020 | 546 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 2 1 | 3 | 2015-07-29 | 4782 | 523 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 3 1 | 2 | 2015-07-28 | 5011 | 560 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 4 1 | 1 | 2015-07-27 | 6102 | 612 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . f, ax = plt.subplots(figsize= (20,20)) df_corr = sales_store_df.corr().loc[:, [&#39;Sales&#39;]].sort_values(&#39;Sales&#39;, ascending=False) sns.heatmap(df_corr, annot=True, cmap=&quot;RdYlGn&quot;, vmin=-1, vmax=1) . &lt;AxesSubplot:&gt; . Preparamos las fechas para el an&#225;lisis . sales_store_df[&quot;Year&quot;] = pd.DatetimeIndex(sales_train_df[&quot;Date&quot;]).year sales_store_df[&quot;Month&quot;] = pd.DatetimeIndex(sales_train_df[&quot;Date&quot;]).month sales_store_df[&quot;Day&quot;] = pd.DatetimeIndex(sales_train_df[&quot;Date&quot;]).day . sales_month = sales_store_df.groupby(&quot;Month&quot;)[&quot;Sales&quot;].mean().plot(figsize = (10, 5), marker=&quot;x&quot;, color=&quot;b&quot;) sales_month.set_title(&quot;Ventas promedio al mes&quot;) plt.figure() sales_month = sales_store_df.groupby(&quot;Month&quot;)[&quot;Customers&quot;].mean().plot(figsize = (10, 5), marker=&quot;o&quot;, color=&quot;g&quot;) sales_month.set_title(&quot;Clientes promedio al mes&quot;) . Text(0.5, 1.0, &#39;Clientes promedio al mes&#39;) . sales_day = sales_store_df.groupby(&quot;Day&quot;)[&quot;Sales&quot;].mean().plot(figsize = (10, 5), marker=&quot;x&quot;, color=&quot;b&quot;) sales_day.set_title(&quot;Ventas promedio al día&quot;) plt.figure() sales_day = sales_store_df.groupby(&quot;Day&quot;)[&quot;Customers&quot;].mean().plot(figsize = (10, 5), marker=&quot;o&quot;, color=&quot;g&quot;) sales_day.set_title(&quot;Clientes promedio al día&quot;) . Text(0.5, 1.0, &#39;Clientes promedio al día&#39;) . sales_day_week = sales_store_df.groupby(&quot;DayOfWeek&quot;)[&quot;Sales&quot;].mean().plot(figsize = (10, 5), marker=&quot;x&quot;, color=&quot;b&quot;) sales_day_week.set_title(&quot;Ventas promedio al día&quot;) plt.figure() sales_day_week = sales_store_df.groupby(&quot;DayOfWeek&quot;)[&quot;Customers&quot;].mean().plot(figsize = (10, 5), marker=&quot;o&quot;, color=&quot;g&quot;) sales_day_week.set_title(&quot;Clientes promedio al día&quot;) . Text(0.5, 1.0, &#39;Clientes promedio al día&#39;) . Las ventas y los clientes están correctamente correlacionados | Los domingos se disparan las ventas | Hay estacionalidad en las ventas | . sales_store_df.head() . Store DayOfWeek Date Sales Customers Promo StateHoliday SchoolHoliday StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval Year Month Day . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 1 1 | 4 | 2015-07-30 | 5020 | 546 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 2 1 | 3 | 2015-07-29 | 4782 | 523 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 3 1 | 2 | 2015-07-28 | 5011 | 560 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 4 1 | 1 | 2015-07-27 | 6102 | 612 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . fig, ax = plt.subplots(figsize=(20,10)) sales_stores = sales_store_df.groupby([&quot;Date&quot;, &quot;StoreType&quot;]).mean()[&quot;Sales&quot;].unstack().plot(ax=ax) . La tienda B es la que más factura. | La tienda A tiene el peor desempeño. | . Desplegamos un modelo de Prophet . def sales_predictions(Store_ID, data_frame, periods): data_frame = data_frame[data_frame[&quot;Store&quot;] == Store_ID] data_frame = data_frame[[&quot;Date&quot;, &quot;Sales&quot;]].rename(columns= {&quot;Date&quot;: &quot;ds&quot;, &quot;Sales&quot;:&quot;y&quot;}) data_frame = data_frame.sort_values(&quot;ds&quot;) m = prophet.Prophet(interval_width = 0.95, daily_seasonality=True) m.fit(data_frame) future = m.make_future_dataframe(periods = periods) forecast = m.predict(future) fig = m.plot(forecast, xlabel = &quot;Fecha&quot;, ylabel = &quot;Ventas&quot;) fig2 = m.plot_components(forecast) . sales_predictions(10, sales_store_df, 60) . def sales_predictions(Store_ID, data_frame, periods, holidays): data_frame = data_frame[data_frame[&quot;Store&quot;] == Store_ID] data_frame = data_frame[[&quot;Date&quot;, &quot;Sales&quot;]].rename(columns= {&quot;Date&quot;: &quot;ds&quot;, &quot;Sales&quot;:&quot;y&quot;}) data_frame = data_frame.sort_values(&quot;ds&quot;) m = prophet.Prophet(interval_width = 0.95, daily_seasonality=True, holidays=holidays) m.fit(data_frame) future = m.make_future_dataframe(periods = periods) forecast = m.predict(future) fig = m.plot(forecast, xlabel = &quot;Fecha&quot;, ylabel = &quot;Ventas&quot;) fig2 = m.plot_components(forecast) . school_holidays = sales_store_df[sales_store_df[&quot;SchoolHoliday&quot;] == 1].loc[:, &quot;Date&quot;].values school_holidays = np.unique(school_holidays) school_holidays = pd.DataFrame({&quot;ds&quot;:pd.to_datetime(school_holidays), &quot;holiday&quot;: &quot;school_holiday&quot;}) school_holidays . ds holiday . 0 2013-01-01 | school_holiday | . 1 2013-01-02 | school_holiday | . 2 2013-01-03 | school_holiday | . 3 2013-01-04 | school_holiday | . 4 2013-01-05 | school_holiday | . ... ... | ... | . 472 2015-07-27 | school_holiday | . 473 2015-07-28 | school_holiday | . 474 2015-07-29 | school_holiday | . 475 2015-07-30 | school_holiday | . 476 2015-07-31 | school_holiday | . 477 rows × 2 columns . state_holidays = sales_store_df[(sales_store_df[&quot;StateHoliday&quot;] == &quot;a&quot;)| (sales_store_df[&quot;StateHoliday&quot;] == &quot;b&quot;)| (sales_store_df[&quot;StateHoliday&quot;] == &quot;c&quot;)].loc[:, &quot;Date&quot;].values state_holidays = np.unique(state_holidays) state_holidays = pd.DataFrame({&quot;ds&quot;:pd.to_datetime(state_holidays), &quot;holiday&quot;: &quot;state_holiday&quot;}) state_holidays . ds holiday . 0 2013-01-01 | state_holiday | . 1 2013-01-06 | state_holiday | . 2 2013-03-29 | state_holiday | . 3 2013-04-01 | state_holiday | . 4 2013-05-01 | state_holiday | . 5 2013-05-09 | state_holiday | . 6 2013-05-20 | state_holiday | . 7 2013-05-30 | state_holiday | . 8 2013-08-15 | state_holiday | . 9 2013-10-03 | state_holiday | . 10 2013-10-31 | state_holiday | . 11 2013-11-01 | state_holiday | . 12 2013-12-25 | state_holiday | . 13 2013-12-26 | state_holiday | . 14 2014-01-01 | state_holiday | . 15 2014-01-06 | state_holiday | . 16 2014-04-18 | state_holiday | . 17 2014-04-21 | state_holiday | . 18 2014-05-01 | state_holiday | . 19 2014-05-29 | state_holiday | . 20 2014-06-09 | state_holiday | . 21 2014-06-19 | state_holiday | . 22 2014-10-03 | state_holiday | . 23 2014-10-31 | state_holiday | . 24 2014-11-01 | state_holiday | . 25 2014-12-25 | state_holiday | . 26 2014-12-26 | state_holiday | . 27 2015-01-01 | state_holiday | . 28 2015-01-06 | state_holiday | . 29 2015-04-03 | state_holiday | . 30 2015-04-06 | state_holiday | . 31 2015-05-01 | state_holiday | . 32 2015-05-14 | state_holiday | . 33 2015-05-25 | state_holiday | . 34 2015-06-04 | state_holiday | . holidays_df = pd.concat((school_holidays, state_holidays), axis=0) . sales_predictions (6, sales_store_df, 90, holidays_df) .",
            "url": "https://pabloja4.github.io/portfolio/2022/02/15/Forecasting-ventas.html",
            "relUrl": "/2022/02/15/Forecasting-ventas.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Cancelación de reservas de una empresa hotelera",
            "content": "La cancelación de reservas es un inconveniente para el gremio hotelero. Para sopesar las pérdidas es común la práctica de la sobre reserva, en la que una habitación se agenda a dos clientes contando con que uno de ellos cancele. Esto puede causar molestias a los visitantes del hotel si no se hace con cuidado, por lo que una solución basada en ML podría aumentar la cantidad de habitaciones ocupadas en épocas de muy alta demanda con el mínimo de inconvenientes. . import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns import scipy as sp from scipy.stats import randint import random mpl.style.use(&#39;seaborn&#39;) sns.set(rc={&quot;figure.figsize&quot;:(35, 20)}) from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV from lightgbm import LGBMClassifier from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.metrics import classification_report, accuracy_score from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline from sklearn.decomposition import PCA, TruncatedSVD from sklearn.metrics import roc_curve, auc from sklearn.feature_selection import VarianceThreshold from sklearn.metrics import confusion_matrix from sklearn.model_selection import RandomizedSearchCV from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer pd.set_option(&quot;max_columns&quot;, None) %matplotlib inline . df = pd.read_csv(r&quot;C: Users pablo Documents Data_Science Data_sets hotel_booking.csv&quot;) . df.head(3) . hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies meal country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled reserved_room_type assigned_room_type booking_changes deposit_type agent company days_in_waiting_list customer_type adr required_car_parking_spaces total_of_special_requests reservation_status reservation_status_date name email phone-number credit_card . 0 Resort Hotel | 0 | 342 | 2015 | July | 27 | 1 | 0 | 0 | 2 | 0.0 | 0 | BB | PRT | Direct | Direct | 0 | 0 | 0 | C | C | 3 | No Deposit | NaN | NaN | 0 | Transient | 0.0 | 0 | 0 | Check-Out | 2015-07-01 | Ernest Barnes | Ernest.Barnes31@outlook.com | 669-792-1661 | ************4322 | . 1 Resort Hotel | 0 | 737 | 2015 | July | 27 | 1 | 0 | 0 | 2 | 0.0 | 0 | BB | PRT | Direct | Direct | 0 | 0 | 0 | C | C | 4 | No Deposit | NaN | NaN | 0 | Transient | 0.0 | 0 | 0 | Check-Out | 2015-07-01 | Andrea Baker | Andrea_Baker94@aol.com | 858-637-6955 | ************9157 | . 2 Resort Hotel | 0 | 7 | 2015 | July | 27 | 1 | 0 | 1 | 1 | 0.0 | 0 | BB | GBR | Direct | Direct | 0 | 0 | 0 | A | C | 0 | No Deposit | NaN | NaN | 0 | Transient | 75.0 | 0 | 0 | Check-Out | 2015-07-02 | Rebecca Parker | Rebecca_Parker@comcast.net | 652-885-2745 | ************3734 | . EDA . El problema . Este data set muestra el historial de reservas de dos hoteles de Portugal, un hotel en la ciudad y otro a las afueras. La columna is_canceled muestra si la recerva se canceló. . df.shape . (119390, 36) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 119390 entries, 0 to 119389 Data columns (total 36 columns): # Column Non-Null Count Dtype -- -- 0 hotel 119390 non-null object 1 is_canceled 119390 non-null int64 2 lead_time 119390 non-null int64 3 arrival_date_year 119390 non-null int64 4 arrival_date_month 119390 non-null object 5 arrival_date_week_number 119390 non-null int64 6 arrival_date_day_of_month 119390 non-null int64 7 stays_in_weekend_nights 119390 non-null int64 8 stays_in_week_nights 119390 non-null int64 9 adults 119390 non-null int64 10 children 119386 non-null float64 11 babies 119390 non-null int64 12 meal 119390 non-null object 13 country 118902 non-null object 14 market_segment 119390 non-null object 15 distribution_channel 119390 non-null object 16 is_repeated_guest 119390 non-null int64 17 previous_cancellations 119390 non-null int64 18 previous_bookings_not_canceled 119390 non-null int64 19 reserved_room_type 119390 non-null object 20 assigned_room_type 119390 non-null object 21 booking_changes 119390 non-null int64 22 deposit_type 119390 non-null object 23 agent 103050 non-null float64 24 company 6797 non-null float64 25 days_in_waiting_list 119390 non-null int64 26 customer_type 119390 non-null object 27 adr 119390 non-null float64 28 required_car_parking_spaces 119390 non-null int64 29 total_of_special_requests 119390 non-null int64 30 reservation_status 119390 non-null object 31 reservation_status_date 119390 non-null object 32 name 119390 non-null object 33 email 119390 non-null object 34 phone-number 119390 non-null object 35 credit_card 119390 non-null object dtypes: float64(4), int64(16), object(16) memory usage: 32.8+ MB . En este dataset se encuentran datos personales de las personas que reservaron tales como nombre, teléfono, email y los últimos numeros de su tarjeta de crédito. Teniendo ya columnas que nos cuentan el rango de edad de las personas, podemos borrar la información personal. . Agrupaci&#243;n de los datos . Variables categ&#243;ricas . Tipo de hotel . df.hotel.unique() . array([&#39;Resort Hotel&#39;, &#39;City Hotel&#39;], dtype=object) . Hay dos tipos de hoteles, puede convertirse en 1 y 0 . sns.histplot(data=df, x= &quot;hotel&quot;, kde=False, bins=5) . &lt;AxesSubplot:xlabel=&#39;hotel&#39;, ylabel=&#39;Count&#39;&gt; . df.groupby(&quot;hotel&quot;)[&quot;is_canceled&quot;].mean()*100 . hotel City Hotel 41.726963 Resort Hotel 27.763355 Name: is_canceled, dtype: float64 . City hotel tiene un 42% de las entradas . Target . df.is_canceled.unique() . array([0, 1], dtype=int64) . Este es el target, esto nos permite desplegar modelos supervisados de clasificación . sns.histplot(data=df, x= &quot;is_canceled&quot;, kde=False, bins=5) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;Count&#39;&gt; . df.groupby(&quot;is_canceled&quot;).describe() . lead_time arrival_date_year arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies is_repeated_guest previous_cancellations previous_bookings_not_canceled booking_changes agent company days_in_waiting_list adr required_car_parking_spaces total_of_special_requests . count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max . is_canceled . 0 75166.0 | 79.984687 | 91.109888 | 0.0 | 9.0 | 45.0 | 124.0 | 737.0 | 75166.0 | 2016.147514 | 0.703124 | 2015.0 | 2016.0 | 2016.0 | 2017.0 | 2017.0 | 75166.0 | 27.080143 | 13.902478 | 1.0 | 16.0 | 28.0 | 38.0 | 53.0 | 75166.0 | 15.839529 | 8.776422 | 1.0 | 8.0 | 16.0 | 23.0 | 31.0 | 75166.0 | 0.928971 | 0.993371 | 0.0 | 0.0 | 1.0 | 2.0 | 19.0 | 75166.0 | 2.464053 | 1.924803 | 0.0 | 1.0 | 2.0 | 3.0 | 50.0 | 75166.0 | 1.829737 | 0.510451 | 0.0 | 2.0 | 2.0 | 2.0 | 4.0 | 75166.0 | 0.102347 | 0.390836 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 75166.0 | 0.010377 | 0.113007 | 0.0 | 0.0 | 0.0 | 0.0 | 10.0 | 75166.0 | 0.043344 | 0.203632 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 75166.0 | 0.015792 | 0.272421 | 0.0 | 0.0 | 0.0 | 0.0 | 13.0 | 75166.0 | 0.202977 | 1.810713 | 0.0 | 0.0 | 0.0 | 0.0 | 72.0 | 75166.0 | 0.293364 | 0.736266 | 0.0 | 0.0 | 0.0 | 0.0 | 21.0 | 62856.0 | 94.055794 | 113.947162 | 1.0 | 9.0 | 14.0 | 240.0 | 535.0 | 5606.0 | 190.519265 | 132.349286 | 6.0 | 51.0 | 183.0 | 270.0 | 541.0 | 75166.0 | 1.589868 | 14.784875 | 0.0 | 0.0 | 0.0 | 0.0 | 379.0 | 75166.0 | 99.987693 | 49.206263 | -6.38 | 67.500 | 92.5 | 125.00 | 510.0 | 75166.0 | 0.0993 | 0.303176 | 0.0 | 0.0 | 0.0 | 0.0 | 8.0 | 75166.0 | 0.714060 | 0.833887 | 0.0 | 0.0 | 1.0 | 1.0 | 5.0 | . 1 44224.0 | 144.848815 | 118.624829 | 0.0 | 48.0 | 113.0 | 214.0 | 629.0 | 44224.0 | 2016.171920 | 0.714557 | 2015.0 | 2016.0 | 2016.0 | 2017.0 | 2017.0 | 44224.0 | 27.309696 | 13.083155 | 1.0 | 17.0 | 27.0 | 38.0 | 53.0 | 44224.0 | 15.728066 | 8.787969 | 1.0 | 8.0 | 16.0 | 23.0 | 31.0 | 44224.0 | 0.925267 | 1.007468 | 0.0 | 0.0 | 1.0 | 2.0 | 16.0 | 44224.0 | 2.561912 | 1.878296 | 0.0 | 1.0 | 2.0 | 3.0 | 40.0 | 44224.0 | 1.901728 | 0.678038 | 0.0 | 2.0 | 2.0 | 2.0 | 55.0 | 44220.0 | 0.106513 | 0.411352 | 0.0 | 0.0 | 0.0 | 0.0 | 10.0 | 44224.0 | 0.003821 | 0.062429 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 44224.0 | 0.012482 | 0.111024 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 44224.0 | 0.208348 | 1.332346 | 0.0 | 0.0 | 0.0 | 0.0 | 26.0 | 44224.0 | 0.025122 | 0.678941 | 0.0 | 0.0 | 0.0 | 0.0 | 58.0 | 44224.0 | 0.098340 | 0.451008 | 0.0 | 0.0 | 0.0 | 0.0 | 16.0 | 40194.0 | 75.179927 | 104.589834 | 1.0 | 9.0 | 9.0 | 149.0 | 531.0 | 1191.0 | 183.371117 | 128.226814 | 9.0 | 67.0 | 169.0 | 270.0 | 543.0 | 44224.0 | 3.564083 | 21.488768 | 0.0 | 0.0 | 0.0 | 0.0 | 391.0 | 44224.0 | 104.964333 | 52.571142 | 0.00 | 72.415 | 96.2 | 127.62 | 5400.0 | 44224.0 | 0.0000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 44224.0 | 0.328826 | 0.649234 | 0.0 | 0.0 | 0.0 | 0.0 | 5.0 | . Las reservas no canceladas son mayoría. . Temporalidad . df.arrival_date_year.unique() . array([2015, 2016, 2017], dtype=int64) . sns.barplot(data=df, x= &quot;arrival_date_year&quot;, y=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;arrival_date_year&#39;, ylabel=&#39;is_canceled&#39;&gt; . df.arrival_date_month.unique() . array([&#39;July&#39;, &#39;August&#39;, &#39;September&#39;, &#39;October&#39;, &#39;November&#39;, &#39;December&#39;, &#39;January&#39;, &#39;February&#39;, &#39;March&#39;, &#39;April&#39;, &#39;May&#39;, &#39;June&#39;], dtype=object) . plt.figure(figsize=[20,10]) arrival_date_graph = sns.barplot(data=df, x= &quot;arrival_date_month&quot;, y=&quot;is_canceled&quot;, hue=&quot;arrival_date_year&quot;) . plt.figure(figsize=[20,10]) sns.lineplot(data=df, x= &quot;arrival_date_month&quot;, y=&quot;is_canceled&quot;, hue=&quot;arrival_date_year&quot;) . &lt;AxesSubplot:xlabel=&#39;arrival_date_month&#39;, ylabel=&#39;is_canceled&#39;&gt; . Los datos están tomados desde julio del 2015 hasta agosto del 2017. Temporadas como el verano parecen tener un mayor . Tipo de comida . df.meal.unique() . array([&#39;BB&#39;, &#39;FB&#39;, &#39;HB&#39;, &#39;SC&#39;, &#39;Undefined&#39;], dtype=object) . No parece ser una variable muy relevante, vemos su correlación con el target. . df[[&quot;meal&quot;, &quot;is_canceled&quot;]].groupby(&quot;meal&quot;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . meal is_canceled . 1 FB | 0.598997 | . 0 BB | 0.373849 | . 3 SC | 0.372394 | . 2 HB | 0.344603 | . 4 Undefined | 0.244654 | . Solo uno de los valores tiene una correlación &gt;0.5 . plt.figure(figsize=[20,10]) sns.boxplot(data=df, x= &quot;meal&quot;, y=df[&quot;meal&quot;].index, hue=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;meal&#39;&gt; . Es una columna con muchos datos anómalos. . df.groupby(&quot;meal&quot;).mean() . is_canceled lead_time arrival_date_year arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies is_repeated_guest previous_cancellations previous_bookings_not_canceled booking_changes agent company days_in_waiting_list adr required_car_parking_spaces total_of_special_requests . meal . BB 0.373849 | 101.797010 | 2016.143961 | 27.164413 | 15.839097 | 0.891485 | 2.435955 | 1.844654 | 0.111954 | 0.007475 | 0.037623 | 0.078226 | 0.171087 | 0.208721 | 87.411964 | 187.552977 | 2.338122 | 99.407041 | 0.066472 | 0.565258 | . FB 0.598997 | 116.666667 | 2015.656642 | 30.713033 | 15.523810 | 1.140351 | 2.855890 | 1.958647 | 0.097744 | 0.030075 | 0.011278 | 1.567669 | 0.026316 | 0.302005 | 114.411326 | 167.823529 | 0.041353 | 109.040476 | 0.068922 | 0.233083 | . HB 0.344603 | 143.338865 | 2016.046118 | 28.570836 | 15.675309 | 1.210676 | 3.124525 | 1.932932 | 0.120791 | 0.012584 | 0.012860 | 0.120307 | 0.022056 | 0.319782 | 126.154098 | 137.307018 | 3.689345 | 120.307041 | 0.068312 | 0.531840 | . SC 0.372394 | 70.678779 | 2016.447512 | 25.408920 | 15.522723 | 0.815775 | 2.115962 | 1.850235 | 0.017559 | 0.003474 | 0.010329 | 0.010610 | 0.021033 | 0.172394 | 17.057674 | 242.101695 | 0.032394 | 98.295869 | 0.022629 | 0.746667 | . Undefined 0.244654 | 87.341317 | 2016.207870 | 23.412318 | 16.790419 | 1.150556 | 3.117194 | 1.823781 | 0.048760 | 0.013687 | 0.027374 | 0.065013 | 0.009410 | 0.368691 | 225.139591 | 441.824561 | 6.461078 | 91.948306 | 0.037639 | 0.176219 | . Pa&#237;ses . df.country.unique() . array([&#39;PRT&#39;, &#39;GBR&#39;, &#39;USA&#39;, &#39;ESP&#39;, &#39;IRL&#39;, &#39;FRA&#39;, nan, &#39;ROU&#39;, &#39;NOR&#39;, &#39;OMN&#39;, &#39;ARG&#39;, &#39;POL&#39;, &#39;DEU&#39;, &#39;BEL&#39;, &#39;CHE&#39;, &#39;CN&#39;, &#39;GRC&#39;, &#39;ITA&#39;, &#39;NLD&#39;, &#39;DNK&#39;, &#39;RUS&#39;, &#39;SWE&#39;, &#39;AUS&#39;, &#39;EST&#39;, &#39;CZE&#39;, &#39;BRA&#39;, &#39;FIN&#39;, &#39;MOZ&#39;, &#39;BWA&#39;, &#39;LUX&#39;, &#39;SVN&#39;, &#39;ALB&#39;, &#39;IND&#39;, &#39;CHN&#39;, &#39;MEX&#39;, &#39;MAR&#39;, &#39;UKR&#39;, &#39;SMR&#39;, &#39;LVA&#39;, &#39;PRI&#39;, &#39;SRB&#39;, &#39;CHL&#39;, &#39;AUT&#39;, &#39;BLR&#39;, &#39;LTU&#39;, &#39;TUR&#39;, &#39;ZAF&#39;, &#39;AGO&#39;, &#39;ISR&#39;, &#39;CYM&#39;, &#39;ZMB&#39;, &#39;CPV&#39;, &#39;ZWE&#39;, &#39;DZA&#39;, &#39;KOR&#39;, &#39;CRI&#39;, &#39;HUN&#39;, &#39;ARE&#39;, &#39;TUN&#39;, &#39;JAM&#39;, &#39;HRV&#39;, &#39;HKG&#39;, &#39;IRN&#39;, &#39;GEO&#39;, &#39;AND&#39;, &#39;GIB&#39;, &#39;URY&#39;, &#39;JEY&#39;, &#39;CAF&#39;, &#39;CYP&#39;, &#39;COL&#39;, &#39;GGY&#39;, &#39;KWT&#39;, &#39;NGA&#39;, &#39;MDV&#39;, &#39;VEN&#39;, &#39;SVK&#39;, &#39;FJI&#39;, &#39;KAZ&#39;, &#39;PAK&#39;, &#39;IDN&#39;, &#39;LBN&#39;, &#39;PHL&#39;, &#39;SEN&#39;, &#39;SYC&#39;, &#39;AZE&#39;, &#39;BHR&#39;, &#39;NZL&#39;, &#39;THA&#39;, &#39;DOM&#39;, &#39;MKD&#39;, &#39;MYS&#39;, &#39;ARM&#39;, &#39;JPN&#39;, &#39;LKA&#39;, &#39;CUB&#39;, &#39;CMR&#39;, &#39;BIH&#39;, &#39;MUS&#39;, &#39;COM&#39;, &#39;SUR&#39;, &#39;UGA&#39;, &#39;BGR&#39;, &#39;CIV&#39;, &#39;JOR&#39;, &#39;SYR&#39;, &#39;SGP&#39;, &#39;BDI&#39;, &#39;SAU&#39;, &#39;VNM&#39;, &#39;PLW&#39;, &#39;QAT&#39;, &#39;EGY&#39;, &#39;PER&#39;, &#39;MLT&#39;, &#39;MWI&#39;, &#39;ECU&#39;, &#39;MDG&#39;, &#39;ISL&#39;, &#39;UZB&#39;, &#39;NPL&#39;, &#39;BHS&#39;, &#39;MAC&#39;, &#39;TGO&#39;, &#39;TWN&#39;, &#39;DJI&#39;, &#39;STP&#39;, &#39;KNA&#39;, &#39;ETH&#39;, &#39;IRQ&#39;, &#39;HND&#39;, &#39;RWA&#39;, &#39;KHM&#39;, &#39;MCO&#39;, &#39;BGD&#39;, &#39;IMN&#39;, &#39;TJK&#39;, &#39;NIC&#39;, &#39;BEN&#39;, &#39;VGB&#39;, &#39;TZA&#39;, &#39;GAB&#39;, &#39;GHA&#39;, &#39;TMP&#39;, &#39;GLP&#39;, &#39;KEN&#39;, &#39;LIE&#39;, &#39;GNB&#39;, &#39;MNE&#39;, &#39;UMI&#39;, &#39;MYT&#39;, &#39;FRO&#39;, &#39;MMR&#39;, &#39;PAN&#39;, &#39;BFA&#39;, &#39;LBY&#39;, &#39;MLI&#39;, &#39;NAM&#39;, &#39;BOL&#39;, &#39;PRY&#39;, &#39;BRB&#39;, &#39;ABW&#39;, &#39;AIA&#39;, &#39;SLV&#39;, &#39;DMA&#39;, &#39;PYF&#39;, &#39;GUY&#39;, &#39;LCA&#39;, &#39;ATA&#39;, &#39;GTM&#39;, &#39;ASM&#39;, &#39;MRT&#39;, &#39;NCL&#39;, &#39;KIR&#39;, &#39;SDN&#39;, &#39;ATF&#39;, &#39;SLE&#39;, &#39;LAO&#39;], dtype=object) . df.country.nunique() . 177 . df.country.value_counts().reset_index() . index country . 0 PRT | 48590 | . 1 GBR | 12129 | . 2 FRA | 10415 | . 3 ESP | 8568 | . 4 DEU | 7287 | . ... ... | ... | . 172 DJI | 1 | . 173 BWA | 1 | . 174 HND | 1 | . 175 VGB | 1 | . 176 NAM | 1 | . 177 rows × 2 columns . Este dataset es de un grupo hotelero de Portugal, tiene sentido que haya tantos visitantes de este país y naciones aledañas. . Segmento de mercado . df.market_segment.unique() . array([&#39;Direct&#39;, &#39;Corporate&#39;, &#39;Online TA&#39;, &#39;Offline TA/TO&#39;, &#39;Complementary&#39;, &#39;Groups&#39;, &#39;Undefined&#39;, &#39;Aviation&#39;], dtype=object) . plt.figure(figsize=[20,10]) sns.boxplot(data=df, x=df[&quot;market_segment&quot;].index, y=df[&quot;market_segment&quot;]) . &lt;AxesSubplot:ylabel=&#39;market_segment&#39;&gt; . df.distribution_channel.unique() . array([&#39;Direct&#39;, &#39;Corporate&#39;, &#39;TA/TO&#39;, &#39;Undefined&#39;, &#39;GDS&#39;], dtype=object) . plt.figure(figsize=[20,10]) sns.boxplot(data=df, x=df[&quot;distribution_channel&quot;].index, y=df[&quot;distribution_channel&quot;]) . &lt;AxesSubplot:ylabel=&#39;distribution_channel&#39;&gt; . Aviación representa un segmento de mercado importante. . df.is_repeated_guest.unique() . array([0, 1], dtype=int64) . plt.figure(figsize=[20,10]) sns.lineplot(data=df, hue=&#39;is_repeated_guest&#39;, x=&quot;arrival_date_month&quot;, y=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;arrival_date_month&#39;, ylabel=&#39;is_canceled&#39;&gt; . df[[&#39;is_repeated_guest&#39;, &quot;is_canceled&quot;]].groupby(&#39;is_repeated_guest&#39;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . is_repeated_guest is_canceled . 0 0 | 0.377851 | . 1 1 | 0.144882 | . Los clientes que ya se han quedado en el hotel tienden a cancelar menos, salvo en verano. . Asignaci&#243;n de habitaciones . df.reserved_room_type.unique() . array([&#39;C&#39;, &#39;A&#39;, &#39;D&#39;, &#39;E&#39;, &#39;G&#39;, &#39;F&#39;, &#39;H&#39;, &#39;L&#39;, &#39;P&#39;, &#39;B&#39;], dtype=object) . df.assigned_room_type.unique() . array([&#39;C&#39;, &#39;A&#39;, &#39;D&#39;, &#39;E&#39;, &#39;G&#39;, &#39;F&#39;, &#39;I&#39;, &#39;B&#39;, &#39;H&#39;, &#39;P&#39;, &#39;L&#39;, &#39;K&#39;], dtype=object) . La habitación tipo &#39;I&#39; y &#39;K&#39; no estan en la columna de reservadas . Cancelaciones previas . df.deposit_type.unique() . array([&#39;No Deposit&#39;, &#39;Refundable&#39;, &#39;Non Refund&#39;], dtype=object) . plt.figure(figsize=[20,10]) sns.barplot(data=df, x=&#39;deposit_type&#39;, hue=&quot;arrival_date_year&quot;, y=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;deposit_type&#39;, ylabel=&#39;is_canceled&#39;&gt; . Agentes o compa&#241;&#237;as de viajes . df.agent.unique() . array([ nan, 304., 240., 303., 15., 241., 8., 250., 115., 5., 175., 134., 156., 243., 242., 3., 105., 40., 147., 306., 184., 96., 2., 127., 95., 146., 9., 177., 6., 143., 244., 149., 167., 300., 171., 305., 67., 196., 152., 142., 261., 104., 36., 26., 29., 258., 110., 71., 181., 88., 251., 275., 69., 248., 208., 256., 314., 126., 281., 273., 253., 185., 330., 334., 328., 326., 321., 324., 313., 38., 155., 68., 335., 308., 332., 94., 348., 310., 339., 375., 66., 327., 387., 298., 91., 245., 385., 257., 393., 168., 405., 249., 315., 75., 128., 307., 11., 436., 1., 201., 183., 223., 368., 336., 291., 464., 411., 481., 10., 154., 468., 410., 390., 440., 495., 492., 493., 434., 57., 531., 420., 483., 526., 472., 429., 16., 446., 34., 78., 139., 252., 270., 47., 114., 301., 193., 182., 135., 350., 195., 352., 355., 159., 363., 384., 360., 331., 367., 64., 406., 163., 414., 333., 427., 431., 430., 426., 438., 433., 418., 441., 282., 432., 72., 450., 180., 454., 455., 59., 451., 254., 358., 469., 165., 467., 510., 337., 476., 502., 527., 479., 508., 535., 302., 497., 187., 13., 7., 27., 14., 22., 17., 28., 42., 20., 19., 45., 37., 61., 39., 21., 24., 41., 50., 30., 54., 52., 12., 44., 31., 83., 32., 63., 60., 55., 56., 89., 87., 118., 86., 85., 210., 214., 129., 179., 138., 174., 170., 153., 93., 151., 119., 35., 173., 58., 53., 133., 79., 235., 192., 191., 236., 162., 215., 157., 287., 132., 234., 98., 77., 103., 107., 262., 220., 121., 205., 378., 23., 296., 290., 229., 33., 286., 276., 425., 484., 323., 403., 219., 394., 509., 111., 423., 4., 70., 82., 81., 74., 92., 99., 90., 112., 117., 106., 148., 158., 144., 211., 213., 216., 232., 150., 267., 227., 247., 278., 280., 285., 289., 269., 295., 265., 288., 122., 294., 325., 341., 344., 346., 359., 283., 364., 370., 371., 25., 141., 391., 397., 416., 404., 299., 197., 73., 354., 444., 408., 461., 388., 453., 459., 474., 475., 480., 449.]) . Estos son IDs . df.company.unique() . array([ nan, 110., 113., 270., 178., 240., 154., 144., 307., 268., 59., 204., 312., 318., 94., 174., 274., 195., 223., 317., 281., 118., 53., 286., 12., 47., 324., 342., 373., 371., 383., 86., 82., 218., 88., 31., 397., 392., 405., 331., 367., 20., 83., 416., 51., 395., 102., 34., 84., 360., 394., 457., 382., 461., 478., 386., 112., 486., 421., 9., 308., 135., 224., 504., 269., 356., 498., 390., 513., 203., 263., 477., 521., 169., 515., 445., 337., 251., 428., 292., 388., 130., 250., 355., 254., 543., 531., 528., 62., 120., 42., 81., 116., 530., 103., 39., 16., 92., 61., 501., 165., 291., 290., 43., 325., 192., 108., 200., 465., 287., 297., 490., 482., 207., 282., 437., 225., 329., 272., 28., 77., 338., 72., 246., 319., 146., 159., 380., 323., 511., 407., 278., 80., 403., 399., 14., 137., 343., 346., 347., 349., 289., 351., 353., 54., 99., 358., 361., 362., 366., 372., 365., 277., 109., 377., 379., 22., 378., 330., 364., 401., 232., 255., 384., 167., 212., 514., 391., 400., 376., 402., 396., 302., 398., 6., 370., 369., 409., 168., 104., 408., 413., 148., 10., 333., 419., 415., 424., 425., 423., 422., 435., 439., 442., 448., 443., 454., 444., 52., 459., 458., 456., 460., 447., 470., 466., 484., 184., 485., 32., 487., 491., 494., 193., 516., 496., 499., 29., 78., 520., 507., 506., 512., 126., 64., 242., 518., 523., 539., 534., 436., 525., 541., 40., 455., 410., 45., 38., 49., 48., 67., 68., 65., 91., 37., 8., 179., 209., 219., 221., 227., 153., 186., 253., 202., 216., 275., 233., 280., 309., 321., 93., 316., 85., 107., 350., 279., 334., 348., 150., 73., 385., 418., 197., 450., 452., 115., 46., 76., 96., 100., 105., 101., 122., 11., 139., 142., 127., 143., 140., 149., 163., 160., 180., 238., 183., 222., 185., 217., 215., 213., 237., 230., 234., 35., 245., 158., 258., 259., 260., 411., 257., 271., 18., 106., 210., 273., 71., 284., 301., 305., 293., 264., 311., 304., 313., 288., 320., 314., 332., 341., 352., 243., 368., 393., 132., 220., 412., 420., 426., 417., 429., 433., 446., 357., 479., 483., 489., 229., 481., 497., 451., 492.]) . tipo de cliente . df.customer_type.unique() . array([&#39;Transient&#39;, &#39;Contract&#39;, &#39;Transient-Party&#39;, &#39;Group&#39;], dtype=object) . df.reservation_status.unique() . array([&#39;Check-Out&#39;, &#39;Canceled&#39;, &#39;No-Show&#39;], dtype=object) . df[[&quot;reservation_status&quot;, &quot;is_canceled&quot;]].groupby(&quot;reservation_status&quot;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . reservation_status is_canceled . 0 Canceled | 1.0 | . 2 No-Show | 1.0 | . 1 Check-Out | 0.0 | . Esta variable puede estar muy correlacionada con el target . df.describe(include=[&#39;O&#39;]) . hotel arrival_date_month meal country market_segment distribution_channel reserved_room_type assigned_room_type deposit_type customer_type reservation_status reservation_status_date name email phone-number credit_card . count 119390 | 119390 | 119390 | 118902 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | . unique 2 | 12 | 5 | 177 | 8 | 5 | 10 | 12 | 3 | 4 | 3 | 926 | 81503 | 115889 | 119390 | 9000 | . top City Hotel | August | BB | PRT | Online TA | TA/TO | A | A | No Deposit | Transient | Check-Out | 2015-10-21 | Michael Johnson | Michael.C@gmail.com | 669-792-1661 | ************4923 | . freq 79330 | 13877 | 92310 | 48590 | 56477 | 97870 | 85994 | 74053 | 104641 | 89613 | 75166 | 1461 | 48 | 6 | 1 | 28 | . Variables num&#233;ricas . Llegada y tiempo de la visita . df.lead_time.sample(10) . 62851 59 30587 0 15921 159 19127 11 3025 40 109693 4 63960 134 46124 125 4407 153 966 137 Name: lead_time, dtype: int64 . sns.FacetGrid(df, col=&#39;is_canceled&#39;).map(plt.hist, &#39;lead_time&#39;, bins=20) . &lt;seaborn.axisgrid.FacetGrid at 0x221bdf92520&gt; . Clientes frecuentres cancelan menos . df.arrival_date_week_number.unique() . array([27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], dtype=int64) . df.arrival_date_day_of_month.unique() . array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], dtype=int64) . N&#250;mero de visitantes . df.adults.unique() . array([ 2, 1, 3, 4, 40, 26, 50, 27, 55, 0, 20, 6, 5, 10], dtype=int64) . df.adults.describe() . count 119390.000000 mean 1.856403 std 0.579261 min 0.000000 25% 2.000000 50% 2.000000 75% 2.000000 max 55.000000 Name: adults, dtype: float64 . No puede haber reservaciones con cero adultos . df.children.unique() . array([ 0., 1., 2., 10., 3., nan]) . Debería ser un int . df.babies.unique() . array([ 0, 1, 2, 10, 9], dtype=int64) . Estos tres pueden ser una feature &quot;Guests&quot; . Cancelaciones previas . df.previous_cancellations.unique() . array([ 0, 1, 2, 3, 26, 25, 14, 4, 24, 19, 5, 21, 6, 13, 11], dtype=int64) . df.previous_cancellations.value_counts() . 0 112906 1 6051 2 116 3 65 24 48 11 35 4 31 26 26 25 25 6 22 19 19 5 19 14 14 13 12 21 1 Name: previous_cancellations, dtype: int64 . sns.barplot(data=df, x=df[&quot;is_canceled&quot;], y=df[&quot;previous_cancellations&quot;]) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;previous_cancellations&#39;&gt; . plt.figure(figsize=[20,10]) sns.barplot(data=df, hue=df[&quot;is_canceled&quot;], x=df[&quot;previous_cancellations&quot;], y=df[&quot;arrival_date_month&quot;]) . &lt;AxesSubplot:xlabel=&#39;previous_cancellations&#39;, ylabel=&#39;arrival_date_month&#39;&gt; . Septiembre y octubre son mesas con alta cancelación . df.previous_bookings_not_canceled.unique() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 19, 26, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72], dtype=int64) . sns.barplot(data=df, x=df[&quot;is_canceled&quot;], y=df[&quot;previous_bookings_not_canceled&quot;]) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;previous_bookings_not_canceled&#39;&gt; . (df[&quot;previous_bookings_not_canceled&quot;] &gt; 0).sum() / df.shape[0] . 0.030320797386715805 . df.booking_changes.unique() . array([ 3, 4, 0, 1, 2, 5, 17, 6, 8, 7, 10, 16, 9, 13, 12, 20, 14, 15, 11, 21, 18], dtype=int64) . N&#250;mero de noches . df.stays_in_weekend_nights.unique() . array([ 0, 1, 2, 4, 3, 6, 13, 8, 5, 7, 12, 9, 16, 18, 19, 10, 14], dtype=int64) . df.stays_in_week_nights.unique() . array([ 0, 1, 2, 3, 4, 5, 10, 11, 8, 6, 7, 15, 9, 12, 33, 20, 14, 16, 21, 13, 30, 19, 24, 40, 22, 42, 50, 25, 17, 32, 26, 18, 34, 35, 41], dtype=int64) . Estas dos podrían juntarse en una nueva columna . D&#237;as en lista de espera . df.days_in_waiting_list.unique() . array([ 0, 50, 47, 65, 122, 75, 101, 150, 125, 14, 60, 34, 100, 22, 121, 61, 39, 5, 1, 8, 107, 43, 52, 2, 11, 142, 116, 13, 44, 97, 83, 4, 113, 18, 20, 185, 93, 109, 6, 37, 105, 154, 64, 99, 38, 48, 33, 77, 21, 80, 59, 40, 58, 89, 53, 49, 69, 87, 91, 57, 111, 79, 98, 85, 63, 15, 3, 41, 224, 31, 56, 187, 176, 71, 55, 96, 236, 259, 207, 215, 160, 120, 30, 32, 27, 62, 24, 108, 147, 379, 70, 35, 178, 330, 223, 174, 162, 391, 68, 193, 10, 76, 16, 28, 9, 165, 17, 25, 46, 7, 84, 175, 183, 23, 117, 12, 54, 26, 73, 45, 19, 42, 72, 81, 92, 74, 167, 36], dtype=int64) . sns.barplot(data=df, y=&quot;days_in_waiting_list&quot;, x=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;days_in_waiting_list&#39;&gt; . A más tiempo de espera más cancelaciones . sns.boxplot(data=df, x=&quot;is_canceled&quot;, y=&quot;days_in_waiting_list&quot;) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;days_in_waiting_list&#39;&gt; . Algunas reservas se hacen hasta un año antes. . Adr . df.adr.unique() . array([ 0. , 75. , 98. , ..., 266.75, 209.25, 157.71]) . df.adr.describe() . count 119390.000000 mean 101.831122 std 50.535790 min -6.380000 25% 69.290000 50% 94.575000 75% 126.000000 max 5400.000000 Name: adr, dtype: float64 . Hay valores negativos, el minimo debería ser cero. . df.required_car_parking_spaces.unique() . array([0, 1, 2, 8, 3], dtype=int64) . df.total_of_special_requests.unique() . array([0, 1, 3, 2, 4, 5], dtype=int64) . Fecha de cambio de status . df.reservation_status_date.sample(3) . 25678 2016-07-07 43985 2015-09-22 99249 2016-10-11 Name: reservation_status_date, dtype: object . Esto debería ser formato int no object . df[[&quot;reservation_status_date&quot;, &quot;is_canceled&quot;]].groupby(&quot;reservation_status_date&quot;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . reservation_status_date is_canceled . 0 2014-10-17 | 1.0 | . 91 2015-05-28 | 1.0 | . 89 2015-05-26 | 1.0 | . 88 2015-05-25 | 1.0 | . 87 2015-05-23 | 1.0 | . ... ... | ... | . 237 2015-10-25 | 0.0 | . 251 2015-11-08 | 0.0 | . 272 2015-11-29 | 0.0 | . 258 2015-11-15 | 0.0 | . 925 2017-09-14 | 0.0 | . 926 rows × 2 columns . Buscamos valores faltantes . df.isna().sum() . hotel 0 is_canceled 0 lead_time 0 arrival_date_year 0 arrival_date_month 0 arrival_date_week_number 0 arrival_date_day_of_month 0 stays_in_weekend_nights 0 stays_in_week_nights 0 adults 0 children 4 babies 0 meal 0 country 488 market_segment 0 distribution_channel 0 is_repeated_guest 0 previous_cancellations 0 previous_bookings_not_canceled 0 reserved_room_type 0 assigned_room_type 0 booking_changes 0 deposit_type 0 agent 16340 company 112593 days_in_waiting_list 0 customer_type 0 adr 0 required_car_parking_spaces 0 total_of_special_requests 0 reservation_status 0 reservation_status_date 0 name 0 email 0 phone-number 0 credit_card 0 dtype: int64 . Las variables Country, agent y company tienen nulos. . Correlaci&#243;n . Medimos la correlación entre las variables . f, ax = plt.subplots(figsize = (20, 20)) sns.heatmap(df.corr(), annot=True, cmap=&quot;RdYlGn&quot;) . &lt;AxesSubplot:&gt; . plt.figure(figsize=(12, 8)) df_correlation = df.corr() target_correlation = df_correlation.loc[:, [&#39;is_canceled&#39;]].sort_values(&#39;is_canceled&#39;, ascending=False) sns.heatmap(target_correlation, annot=True, cmap=&quot;RdYlGn&quot;, vmin=-1, vmax=1) . &lt;AxesSubplot:&gt; . Conclusiones del EDA . 42% de la información corresponde a City hotel. | Este dataset cuenta con 119390 filas y 36 columnas. | Hay dos tipos de hotel: Resort y City hotel, este último cuenta con más registros. | El target se encuentra en la variable “is_canceled” que muestra con uno las reservas canceladas y con cero las que no. | Hay visitantes de 177 países, los mayores visitantes vienen de Portugal, seguido por Reino Unido, Francia y España. De algunos países solo se hizo una reserva. | Parece que aquellos que se han hospedado en el hotel con anterioridad tienden a cancelar menos que los nuevos visitantes. | Septiembre y octubre son mesas con alta cancelación | . Manejo de valores faltantes, borrado de columnas y cambio de tipos . Se borran columnas seg&#250;n el EDA . Según el EDA y la correlación de las variables se decidieron eliminar las siguientes variables para evitar contaminar los resultados de los modelos, aligerarlos y simplificar. . df.drop(columns=[&quot;name&quot;, &quot;email&quot;, &quot;phone-number&quot;, &quot;credit_card&quot;, &quot;agent&quot;, &quot;meal&quot;, &quot;reservation_status&quot;, &quot;company&quot;, &quot;reserved_room_type&quot;, &quot;total_of_special_requests&quot;, &quot;booking_changes&quot;, &quot;required_car_parking_spaces&quot;, &quot;arrival_date_week_number&quot;], inplace=True) . df.sample(5) . hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled assigned_room_type deposit_type days_in_waiting_list customer_type adr reservation_status_date . 94145 City Hotel | 0 | 73 | 2016 | July | 29 | 1 | 2 | 3 | 1.0 | 0 | FRA | Direct | Direct | 0 | 0 | 0 | E | No Deposit | 0 | Transient | 173.7 | 2016-08-01 | . 100662 City Hotel | 0 | 36 | 2016 | October | 28 | 1 | 2 | 2 | 0.0 | 0 | ITA | Online TA | TA/TO | 0 | 0 | 0 | A | No Deposit | 0 | Transient | 109.0 | 2016-10-31 | . 83687 City Hotel | 0 | 59 | 2016 | February | 6 | 2 | 2 | 2 | 0.0 | 0 | AUT | Groups | TA/TO | 0 | 0 | 0 | D | No Deposit | 0 | Transient-Party | 62.0 | 2016-02-10 | . 12676 Resort Hotel | 1 | 23 | 2017 | July | 11 | 0 | 5 | 2 | 0.0 | 0 | ESP | Direct | Direct | 0 | 0 | 0 | E | No Deposit | 0 | Transient | 275.0 | 2017-06-20 | . 80597 City Hotel | 0 | 5 | 2016 | February | 3 | 0 | 1 | 1 | 0.0 | 0 | PRT | Corporate | Corporate | 1 | 0 | 4 | A | No Deposit | 0 | Transient | 66.0 | 2016-02-04 | . Manejo de variables categ&#243;ricas . Para aprovechar mejor estas variables las convertiremos en binarias. . df[&quot;previous_cancellations&quot;] = df[&quot;previous_cancellations&quot;].apply(lambda x: 0 if x == 0 else 1) df[&quot;previous_cancellations&quot;].value_counts() . 0 112906 1 6484 Name: previous_cancellations, dtype: int64 . df[&quot;previous_bookings_not_canceled&quot;] = df[&quot;previous_bookings_not_canceled&quot;].apply(lambda x: 0 if x == 0 else 1) df[&quot;previous_bookings_not_canceled&quot;].value_counts() . 0 115770 1 3620 Name: previous_bookings_not_canceled, dtype: int64 . Manejo de variables num&#233;ricas . Reservation_status_date como tipo Date . df[&quot;reservation_status_date&quot;] = pd.to_datetime(df[&quot;reservation_status_date&quot;], format= &quot;%Y/%m/%d&quot;) . df[&quot;reservation_status_date&quot;].dt.month.value_counts() . 7 12106 8 11249 10 11143 1 10681 5 10304 3 10230 4 9999 2 9498 9 9403 6 9278 11 8099 12 7400 Name: reservation_status_date, dtype: int64 . Creamos una columna de estaciones . df[&quot;reservation_status_seasone&quot;] = (df[&quot;reservation_status_date&quot;].dt.month - 1) // 3 . df.drop(columns=&quot;reservation_status_date&quot;, inplace=True) . Manejo de outliers . El valor minimo del ADR debe ser cero . df.drop(df[df[&quot;adr&quot;] &lt; 0].index, inplace=True) . df[&quot;adr&quot;].describe() . count 119389.000000 mean 101.832028 std 50.535032 min 0.000000 25% 69.290000 50% 94.590000 75% 126.000000 max 5400.000000 Name: adr, dtype: float64 . El valor minimo de adultos debe ser 1 . df.drop(df[df[&quot;adults&quot;] == 0].index, inplace=True) . df[&quot;adults&quot;].describe() . count 118986.000000 mean 1.862690 std 0.570062 min 1.000000 25% 2.000000 50% 2.000000 75% 2.000000 max 55.000000 Name: adults, dtype: float64 . df[&quot;adults&quot;] = df[&quot;adults&quot;].astype(int) . Creaci&#243;n de variables . Creamos la variable &quot;guests&quot; usando los valores de adults, children y babies, . df[&quot;guests&quot;] = df[&quot;adults&quot;] + df[&quot;children&quot;] + df[&quot;babies&quot;] . Creamos la variable &quot;total nights&quot; usando los valores de weekend y week nights . df[&quot;total_nights&quot;] = df[&quot;stays_in_weekend_nights&quot;] + df[&quot;stays_in_week_nights&quot;] . Nos quedamos solo con &quot;total nights&quot; para evitar redundancia . df.drop(columns = [&quot;stays_in_weekend_nights&quot;, &quot;stays_in_week_nights&quot;], inplace=True) . df.sample(5) . hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_day_of_month adults children babies country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled assigned_room_type deposit_type days_in_waiting_list customer_type adr reservation_status_seasone guests total_nights . 79187 City Hotel | 0 | 34 | 2015 | October | 19 | 2 | 0.0 | 0 | ESP | Offline TA/TO | TA/TO | 0 | 0 | 0 | D | No Deposit | 0 | Transient | 105.12 | 3 | 2.0 | 4 | . 107205 City Hotel | 0 | 18 | 2017 | March | 6 | 3 | 0.0 | 0 | CHE | Online TA | TA/TO | 0 | 0 | 0 | D | No Deposit | 0 | Transient | 158.00 | 0 | 3.0 | 3 | . 23613 Resort Hotel | 0 | 12 | 2016 | April | 27 | 2 | 0.0 | 0 | PRT | Groups | Direct | 0 | 0 | 0 | A | No Deposit | 0 | Transient-Party | 85.33 | 1 | 2.0 | 3 | . 66633 City Hotel | 1 | 100 | 2017 | April | 22 | 2 | 0.0 | 0 | PRT | Offline TA/TO | TA/TO | 0 | 0 | 0 | A | Non Refund | 0 | Transient | 105.00 | 0 | 2.0 | 4 | . 87019 City Hotel | 0 | 86 | 2016 | April | 9 | 2 | 0.0 | 0 | NLD | Offline TA/TO | TA/TO | 0 | 0 | 0 | B | No Deposit | 0 | Transient-Party | 80.75 | 1 | 2.0 | 4 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 118986 entries, 0 to 119389 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 hotel 118986 non-null object 1 is_canceled 118986 non-null int64 2 lead_time 118986 non-null int64 3 arrival_date_year 118986 non-null int64 4 arrival_date_month 118986 non-null object 5 arrival_date_day_of_month 118986 non-null int64 6 adults 118986 non-null int32 7 children 118982 non-null float64 8 babies 118986 non-null int64 9 country 118508 non-null object 10 market_segment 118986 non-null object 11 distribution_channel 118986 non-null object 12 is_repeated_guest 118986 non-null int64 13 previous_cancellations 118986 non-null int64 14 previous_bookings_not_canceled 118986 non-null int64 15 assigned_room_type 118986 non-null object 16 deposit_type 118986 non-null object 17 days_in_waiting_list 118986 non-null int64 18 customer_type 118986 non-null object 19 adr 118986 non-null float64 20 reservation_status_seasone 118986 non-null int64 21 guests 118982 non-null float64 22 total_nights 118986 non-null int64 dtypes: float64(3), int32(1), int64(11), object(8) memory usage: 21.3+ MB . Los nulos que quedan, la estandarización, el encoding y demás procesos se harán en el pipeline. . Creaci&#243;n del pipeline . Agrupamos las columnas con valores categóricos. . categorical = [cname for cname in df.columns if df[cname].nunique() and df[cname].dtype == &quot;object&quot;] print(categorical) print(&quot;-&quot;*90) print(len(categorical)) . [&#39;hotel&#39;, &#39;arrival_date_month&#39;, &#39;country&#39;, &#39;market_segment&#39;, &#39;distribution_channel&#39;, &#39;assigned_room_type&#39;, &#39;deposit_type&#39;, &#39;customer_type&#39;] 8 . Agrupamos las columnas con valores númericos. . numerical = [cname for cname in df.columns if df[cname].dtype in [&#39;int64&#39;, &#39;float64&#39;]] numerical.remove(&quot;is_canceled&quot;) print(numerical) print(&quot;-&quot;*90) print(len(numerical)) . [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;previous_bookings_not_canceled&#39;, &#39;days_in_waiting_list&#39;, &#39;adr&#39;, &#39;reservation_status_seasone&#39;, &#39;guests&#39;, &#39;total_nights&#39;] 13 . Activamos el truncateSVD para hacer reducción de dimensionalidad. . tsvd = TruncatedSVD() . Creamos un transformer para las variables numéricas que agregue valores en los nulos, estandarice y reduzca dimensionalidad . numerical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;mean&#39;)), (&#39;scaler&#39;, StandardScaler()), (&quot;tsvd&quot;, tsvd) ]) . Creamos otro para las variables categóricas que agregue valores en los nulos, haga OHE y reduzca dimensionalidad . categorical_transformer = Pipeline(steps = [ (&quot;imputer&quot;, SimpleImputer(strategy= &quot;most_frequent&quot;)), (&quot;onehot&quot;, OneHotEncoder(handle_unknown= &quot;ignore&quot;)), (&quot;tsvd&quot;, tsvd) ]) . preprocessor = ColumnTransformer( transformers=[ (&quot;num&quot;, numerical_transformer, numerical), (&quot;cat&quot;, categorical_transformer, categorical), ], remainder = &quot;passthrough&quot;) . y = df[&quot;is_canceled&quot;] X = df.drop([&quot;is_canceled&quot;], axis=1) . Divisi&#243;n del dataset . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =.3, random_state=42, stratify=y) . Despliegue de modelos . tree = DecisionTreeClassifier(random_state=42, class_weight=&#39;balanced&#39;) . tree_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;model&quot;, tree) ]) . Buscamos los mejores hiperparmetros para regularizar el modelo . tree_pipeline.get_params().keys() . dict_keys([&#39;memory&#39;, &#39;steps&#39;, &#39;verbose&#39;, &#39;preprocessor&#39;, &#39;model&#39;, &#39;preprocessor__n_jobs&#39;, &#39;preprocessor__remainder&#39;, &#39;preprocessor__sparse_threshold&#39;, &#39;preprocessor__transformer_weights&#39;, &#39;preprocessor__transformers&#39;, &#39;preprocessor__verbose&#39;, &#39;preprocessor__verbose_feature_names_out&#39;, &#39;preprocessor__num&#39;, &#39;preprocessor__cat&#39;, &#39;preprocessor__num__memory&#39;, &#39;preprocessor__num__steps&#39;, &#39;preprocessor__num__verbose&#39;, &#39;preprocessor__num__imputer&#39;, &#39;preprocessor__num__scaler&#39;, &#39;preprocessor__num__tsvd&#39;, &#39;preprocessor__num__imputer__add_indicator&#39;, &#39;preprocessor__num__imputer__copy&#39;, &#39;preprocessor__num__imputer__fill_value&#39;, &#39;preprocessor__num__imputer__missing_values&#39;, &#39;preprocessor__num__imputer__strategy&#39;, &#39;preprocessor__num__imputer__verbose&#39;, &#39;preprocessor__num__scaler__copy&#39;, &#39;preprocessor__num__scaler__with_mean&#39;, &#39;preprocessor__num__scaler__with_std&#39;, &#39;preprocessor__num__tsvd__algorithm&#39;, &#39;preprocessor__num__tsvd__n_components&#39;, &#39;preprocessor__num__tsvd__n_iter&#39;, &#39;preprocessor__num__tsvd__random_state&#39;, &#39;preprocessor__num__tsvd__tol&#39;, &#39;preprocessor__cat__memory&#39;, &#39;preprocessor__cat__steps&#39;, &#39;preprocessor__cat__verbose&#39;, &#39;preprocessor__cat__imputer&#39;, &#39;preprocessor__cat__onehot&#39;, &#39;preprocessor__cat__tsvd&#39;, &#39;preprocessor__cat__imputer__add_indicator&#39;, &#39;preprocessor__cat__imputer__copy&#39;, &#39;preprocessor__cat__imputer__fill_value&#39;, &#39;preprocessor__cat__imputer__missing_values&#39;, &#39;preprocessor__cat__imputer__strategy&#39;, &#39;preprocessor__cat__imputer__verbose&#39;, &#39;preprocessor__cat__onehot__categories&#39;, &#39;preprocessor__cat__onehot__drop&#39;, &#39;preprocessor__cat__onehot__dtype&#39;, &#39;preprocessor__cat__onehot__handle_unknown&#39;, &#39;preprocessor__cat__onehot__sparse&#39;, &#39;preprocessor__cat__tsvd__algorithm&#39;, &#39;preprocessor__cat__tsvd__n_components&#39;, &#39;preprocessor__cat__tsvd__n_iter&#39;, &#39;preprocessor__cat__tsvd__random_state&#39;, &#39;preprocessor__cat__tsvd__tol&#39;, &#39;model__ccp_alpha&#39;, &#39;model__class_weight&#39;, &#39;model__criterion&#39;, &#39;model__max_depth&#39;, &#39;model__max_features&#39;, &#39;model__max_leaf_nodes&#39;, &#39;model__min_impurity_decrease&#39;, &#39;model__min_samples_leaf&#39;, &#39;model__min_samples_split&#39;, &#39;model__min_weight_fraction_leaf&#39;, &#39;model__random_state&#39;, &#39;model__splitter&#39;]) . Hacemos un random search . param_dist = { &quot;preprocessor__num__tsvd__n_components&quot;: randint(5, 13), &quot;preprocessor__cat__tsvd__n_components&quot;: randint(5, 8), &quot;model__min_samples_leaf&quot;: randint(10, 100), &#39;model__min_samples_split&#39; : randint(2, 100), &#39;model__ccp_alpha&#39;: list(np.arange(0.0, 1., step=0.05)) } . rs = RandomizedSearchCV(estimator = tree_pipeline, n_iter=100, param_distributions = param_dist, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=5, random_state=42, verbose= 1) . %%time rs.fit(X_train, y_train) . Fitting 5 folds for each of 100 candidates, totalling 500 fits Wall time: 4min 58s . RandomizedSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;model__min_samples_split&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE538918E0&gt;, &#39;preprocessor__cat__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE53891CA0&gt;, &#39;preprocessor__num__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5387EB20&gt;}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . rs.best_params_ . {&#39;model__ccp_alpha&#39;: 0.0, &#39;model__min_samples_leaf&#39;: 18, &#39;model__min_samples_split&#39;: 44, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 12} . rs.best_score_ . 0.7616077335487703 . search_space_tree = { &#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12], &#39;model__max_depth&#39;: [10, 17, 20], &#39;model__min_samples_split&#39;: [42, 44], &#39;model__min_samples_leaf&#39;: [17, 18], } . gs = GridSearchCV(estimator = tree_pipeline, param_grid = search_space_tree, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 5, verbose = 5) . %%time gs.fit(X_train, y_train) . Fitting 5 folds for each of 48 candidates, totalling 240 fits Wall time: 2min 58s . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... &#39;deposit_type&#39;, &#39;customer_type&#39;])])), (&#39;model&#39;, DecisionTreeClassifier(class_weight=&#39;balanced&#39;, random_state=42))]), n_jobs=-1, param_grid={&#39;model__max_depth&#39;: [10, 17, 20], &#39;model__min_samples_leaf&#39;: [17, 18], &#39;model__min_samples_split&#39;: [42, 44], &#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=5) . score = gs.best_score_ score . 0.7619156331548821 . bp = gs.best_params_ bp . {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_leaf&#39;: 18, &#39;model__min_samples_split&#39;: 42, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 12} . pred_train = gs.predict(X_train) print(classification_report(y_train, pred_train)) . precision recall f1-score support 0 0.91 0.88 0.90 52410 1 0.81 0.86 0.83 30880 accuracy 0.87 83290 macro avg 0.86 0.87 0.86 83290 weighted avg 0.87 0.87 0.87 83290 . pred_test = gs.predict(X_test) print(classification_report(y_test, pred_test)) . precision recall f1-score support 0 0.87 0.84 0.86 22461 1 0.75 0.79 0.77 13235 accuracy 0.83 35696 macro avg 0.81 0.82 0.81 35696 weighted avg 0.83 0.83 0.83 35696 . pd.DataFrame(gs.cv_results_).sort_values(by=&quot;rank_test_f1&quot;) . mean_fit_time std_fit_time mean_score_time std_score_time param_model__max_depth param_model__min_samples_leaf param_model__min_samples_split param_preprocessor__cat__tsvd__n_components param_preprocessor__num__tsvd__n_components params split0_test_f1 split1_test_f1 split2_test_f1 split3_test_f1 split4_test_f1 mean_test_f1 std_test_f1 rank_test_f1 . 43 5.909593 | 0.352149 | 0.152775 | 0.023438 | 20 | 18 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.764428 | 0.762749 | 0.766819 | 0.755563 | 0.760019 | 0.761916 | 0.003873 | 1 | . 39 5.752533 | 0.367464 | 0.137742 | 0.032969 | 20 | 17 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.766283 | 0.763309 | 0.764757 | 0.755963 | 0.759150 | 0.761892 | 0.003799 | 2 | . 47 4.120231 | 0.743095 | 0.074698 | 0.030800 | 20 | 18 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.763757 | 0.764424 | 0.767201 | 0.754657 | 0.757861 | 0.761580 | 0.004608 | 3 | . 35 5.727160 | 0.528002 | 0.132678 | 0.021309 | 20 | 17 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.765735 | 0.762226 | 0.765700 | 0.756449 | 0.756529 | 0.761328 | 0.004152 | 4 | . 46 5.756054 | 0.231365 | 0.133034 | 0.008300 | 20 | 18 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762425 | 0.766105 | 0.763892 | 0.750373 | 0.755390 | 0.759637 | 0.005858 | 5 | . 41 5.663632 | 0.329802 | 0.158780 | 0.025666 | 20 | 18 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762486 | 0.763880 | 0.765496 | 0.754287 | 0.750889 | 0.759408 | 0.005750 | 6 | . 37 5.593201 | 0.358663 | 0.135199 | 0.011575 | 20 | 17 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.764592 | 0.763674 | 0.764103 | 0.754797 | 0.749691 | 0.759371 | 0.006046 | 7 | . 42 5.671552 | 0.482549 | 0.134612 | 0.015834 | 20 | 18 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762150 | 0.766201 | 0.762195 | 0.753074 | 0.753226 | 0.759369 | 0.005287 | 8 | . 38 5.326279 | 0.401823 | 0.145958 | 0.014537 | 20 | 17 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.761957 | 0.766412 | 0.761332 | 0.749922 | 0.757015 | 0.759327 | 0.005567 | 9 | . 45 5.545277 | 0.403864 | 0.136949 | 0.035746 | 20 | 18 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762021 | 0.763899 | 0.765270 | 0.753521 | 0.751085 | 0.759159 | 0.005744 | 10 | . 33 5.452450 | 0.431252 | 0.147094 | 0.041165 | 20 | 17 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.764660 | 0.762265 | 0.761381 | 0.756090 | 0.751025 | 0.759084 | 0.004907 | 11 | . 34 5.631814 | 0.182973 | 0.136370 | 0.015566 | 20 | 17 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.760535 | 0.765312 | 0.760327 | 0.751518 | 0.757014 | 0.758941 | 0.004558 | 12 | . 32 5.200071 | 0.186772 | 0.150597 | 0.018287 | 20 | 17 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.760050 | 0.764669 | 0.758803 | 0.754770 | 0.754897 | 0.758638 | 0.003670 | 13 | . 36 5.205832 | 0.188727 | 0.136098 | 0.023822 | 20 | 17 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.759957 | 0.765523 | 0.761472 | 0.753748 | 0.749750 | 0.758090 | 0.005632 | 14 | . 44 6.017212 | 0.269164 | 0.147854 | 0.030257 | 20 | 18 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.760102 | 0.765679 | 0.759881 | 0.752082 | 0.749445 | 0.757438 | 0.005892 | 15 | . 40 5.521703 | 0.373788 | 0.145786 | 0.028774 | 20 | 18 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.759539 | 0.764462 | 0.760592 | 0.753281 | 0.748905 | 0.757356 | 0.005545 | 16 | . 26 5.775870 | 0.169365 | 0.124763 | 0.007315 | 17 | 18 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759635 | 0.762282 | 0.760687 | 0.753786 | 0.748934 | 0.757065 | 0.004976 | 17 | . 29 5.442686 | 0.424129 | 0.162982 | 0.031839 | 17 | 18 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759709 | 0.757191 | 0.759077 | 0.751491 | 0.757690 | 0.757032 | 0.002916 | 18 | . 23 5.573463 | 0.447110 | 0.143347 | 0.027625 | 17 | 17 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760422 | 0.760298 | 0.759224 | 0.754334 | 0.749557 | 0.756767 | 0.004237 | 19 | . 27 5.562768 | 0.301798 | 0.152453 | 0.026827 | 17 | 18 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760155 | 0.759820 | 0.758599 | 0.756722 | 0.748430 | 0.756745 | 0.004328 | 20 | . 19 5.609279 | 0.179823 | 0.139316 | 0.034424 | 17 | 17 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.761472 | 0.759902 | 0.757056 | 0.755834 | 0.749313 | 0.756715 | 0.004207 | 21 | . 22 5.353494 | 0.597877 | 0.140425 | 0.027570 | 17 | 17 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759429 | 0.762238 | 0.758808 | 0.752536 | 0.750101 | 0.756622 | 0.004548 | 22 | . 30 5.405706 | 0.523146 | 0.138758 | 0.024643 | 17 | 18 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.758724 | 0.762427 | 0.760786 | 0.752353 | 0.747971 | 0.756452 | 0.005447 | 23 | . 18 5.340284 | 0.329550 | 0.118122 | 0.010312 | 17 | 17 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759586 | 0.762116 | 0.758478 | 0.753319 | 0.748210 | 0.756342 | 0.004974 | 24 | . 16 5.366972 | 0.369566 | 0.124007 | 0.024215 | 17 | 17 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759838 | 0.761351 | 0.759101 | 0.752141 | 0.749154 | 0.756317 | 0.004780 | 25 | . 20 4.855590 | 0.299030 | 0.142960 | 0.020580 | 17 | 17 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760715 | 0.760539 | 0.759486 | 0.752509 | 0.746450 | 0.755940 | 0.005628 | 26 | . 31 6.347360 | 0.327202 | 0.120130 | 0.010828 | 17 | 18 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759632 | 0.760041 | 0.759046 | 0.754569 | 0.745993 | 0.755856 | 0.005308 | 27 | . 24 5.259175 | 0.565762 | 0.123623 | 0.016688 | 17 | 18 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760047 | 0.760213 | 0.758373 | 0.752628 | 0.745966 | 0.755446 | 0.005482 | 28 | . 17 5.774585 | 0.356408 | 0.132201 | 0.020936 | 17 | 17 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760727 | 0.758933 | 0.757967 | 0.751886 | 0.746811 | 0.755265 | 0.005167 | 29 | . 21 5.718334 | 0.614460 | 0.160697 | 0.040621 | 17 | 17 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760866 | 0.757819 | 0.757797 | 0.752102 | 0.747639 | 0.755245 | 0.004743 | 30 | . 28 5.322633 | 0.210339 | 0.139555 | 0.027456 | 17 | 18 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759452 | 0.760342 | 0.758840 | 0.751166 | 0.745976 | 0.755155 | 0.005641 | 31 | . 25 5.501385 | 0.263432 | 0.139574 | 0.013089 | 17 | 18 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759434 | 0.757225 | 0.758698 | 0.751745 | 0.748453 | 0.755111 | 0.004282 | 32 | . 9 5.172902 | 0.566289 | 0.134767 | 0.016438 | 10 | 18 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738211 | 0.734577 | 0.732961 | 0.726533 | 0.722196 | 0.730895 | 0.005763 | 33 | . 7 5.321097 | 0.341320 | 0.147221 | 0.022727 | 10 | 17 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.740003 | 0.734880 | 0.731690 | 0.727588 | 0.718374 | 0.730507 | 0.007300 | 34 | . 11 5.984165 | 0.308396 | 0.181516 | 0.024323 | 10 | 18 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.739367 | 0.734794 | 0.731912 | 0.727604 | 0.716532 | 0.730042 | 0.007765 | 35 | . 15 5.170612 | 0.244348 | 0.139610 | 0.024521 | 10 | 18 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.739474 | 0.734707 | 0.731882 | 0.727543 | 0.716544 | 0.730030 | 0.007779 | 36 | . 3 5.551793 | 0.557981 | 0.171418 | 0.056321 | 10 | 17 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.739441 | 0.734794 | 0.731865 | 0.727408 | 0.716355 | 0.729973 | 0.007855 | 37 | . 1 4.712395 | 0.380196 | 0.129655 | 0.024373 | 10 | 17 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738284 | 0.734827 | 0.732896 | 0.726337 | 0.716732 | 0.729815 | 0.007610 | 38 | . 5 5.143289 | 0.265635 | 0.156563 | 0.036970 | 10 | 17 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738846 | 0.734740 | 0.732343 | 0.726729 | 0.715682 | 0.729668 | 0.008017 | 39 | . 13 5.684822 | 0.246634 | 0.127606 | 0.014110 | 10 | 18 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738318 | 0.734551 | 0.732867 | 0.726562 | 0.715253 | 0.729510 | 0.008077 | 40 | . 14 5.170579 | 0.331523 | 0.142670 | 0.029044 | 10 | 18 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.736111 | 0.731863 | 0.729397 | 0.728114 | 0.717235 | 0.728544 | 0.006278 | 41 | . 6 5.063322 | 0.442805 | 0.131332 | 0.021350 | 10 | 17 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.736565 | 0.731927 | 0.728715 | 0.727982 | 0.717327 | 0.728503 | 0.006355 | 42 | . 8 5.362791 | 0.333177 | 0.140469 | 0.042554 | 10 | 18 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.735071 | 0.732265 | 0.730214 | 0.728389 | 0.716082 | 0.728404 | 0.006549 | 43 | . 0 4.590914 | 0.331202 | 0.123669 | 0.027797 | 10 | 17 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.735020 | 0.732581 | 0.729609 | 0.728192 | 0.716378 | 0.728356 | 0.006439 | 44 | . 2 4.822303 | 0.391141 | 0.150000 | 0.035955 | 10 | 17 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.736060 | 0.731927 | 0.728809 | 0.728058 | 0.716913 | 0.728354 | 0.006377 | 45 | . 4 5.316231 | 0.158647 | 0.153395 | 0.018886 | 10 | 17 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.735550 | 0.732493 | 0.729515 | 0.728177 | 0.715972 | 0.728341 | 0.006687 | 46 | . 10 5.595861 | 0.525995 | 0.222177 | 0.086424 | 10 | 18 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.734943 | 0.731863 | 0.729032 | 0.728194 | 0.717235 | 0.728253 | 0.005997 | 47 | . 12 5.678165 | 0.310659 | 0.175668 | 0.040308 | 10 | 18 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.734970 | 0.732413 | 0.729659 | 0.728192 | 0.715982 | 0.728243 | 0.006557 | 48 | . Entrenamos nuevamente el modelo usando los nuevos hiperparametros . final_tree_cv = cross_val_score(gs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) final_tree_cv . Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits . array([0.80378292, 0.80038238, 0.80181624, 0.79704733, 0.79958445]) . cm = confusion_matrix(y_test, pred_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = gs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.9032494101726963 . Regresi&#243;n log&#237;stica . regression = LogisticRegression(random_state=0, class_weight=&#39;balanced&#39;) . regression_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;regression&quot;, regression) ]) . regression_pipeline.get_params().keys() . dict_keys([&#39;memory&#39;, &#39;steps&#39;, &#39;verbose&#39;, &#39;preprocessor&#39;, &#39;regression&#39;, &#39;preprocessor__n_jobs&#39;, &#39;preprocessor__remainder&#39;, &#39;preprocessor__sparse_threshold&#39;, &#39;preprocessor__transformer_weights&#39;, &#39;preprocessor__transformers&#39;, &#39;preprocessor__verbose&#39;, &#39;preprocessor__verbose_feature_names_out&#39;, &#39;preprocessor__num&#39;, &#39;preprocessor__cat&#39;, &#39;preprocessor__num__memory&#39;, &#39;preprocessor__num__steps&#39;, &#39;preprocessor__num__verbose&#39;, &#39;preprocessor__num__imputer&#39;, &#39;preprocessor__num__scaler&#39;, &#39;preprocessor__num__tsvd&#39;, &#39;preprocessor__num__imputer__add_indicator&#39;, &#39;preprocessor__num__imputer__copy&#39;, &#39;preprocessor__num__imputer__fill_value&#39;, &#39;preprocessor__num__imputer__missing_values&#39;, &#39;preprocessor__num__imputer__strategy&#39;, &#39;preprocessor__num__imputer__verbose&#39;, &#39;preprocessor__num__scaler__copy&#39;, &#39;preprocessor__num__scaler__with_mean&#39;, &#39;preprocessor__num__scaler__with_std&#39;, &#39;preprocessor__num__tsvd__algorithm&#39;, &#39;preprocessor__num__tsvd__n_components&#39;, &#39;preprocessor__num__tsvd__n_iter&#39;, &#39;preprocessor__num__tsvd__random_state&#39;, &#39;preprocessor__num__tsvd__tol&#39;, &#39;preprocessor__cat__memory&#39;, &#39;preprocessor__cat__steps&#39;, &#39;preprocessor__cat__verbose&#39;, &#39;preprocessor__cat__imputer&#39;, &#39;preprocessor__cat__onehot&#39;, &#39;preprocessor__cat__tsvd&#39;, &#39;preprocessor__cat__imputer__add_indicator&#39;, &#39;preprocessor__cat__imputer__copy&#39;, &#39;preprocessor__cat__imputer__fill_value&#39;, &#39;preprocessor__cat__imputer__missing_values&#39;, &#39;preprocessor__cat__imputer__strategy&#39;, &#39;preprocessor__cat__imputer__verbose&#39;, &#39;preprocessor__cat__onehot__categories&#39;, &#39;preprocessor__cat__onehot__drop&#39;, &#39;preprocessor__cat__onehot__dtype&#39;, &#39;preprocessor__cat__onehot__handle_unknown&#39;, &#39;preprocessor__cat__onehot__sparse&#39;, &#39;preprocessor__cat__tsvd__algorithm&#39;, &#39;preprocessor__cat__tsvd__n_components&#39;, &#39;preprocessor__cat__tsvd__n_iter&#39;, &#39;preprocessor__cat__tsvd__random_state&#39;, &#39;preprocessor__cat__tsvd__tol&#39;, &#39;regression__C&#39;, &#39;regression__class_weight&#39;, &#39;regression__dual&#39;, &#39;regression__fit_intercept&#39;, &#39;regression__intercept_scaling&#39;, &#39;regression__l1_ratio&#39;, &#39;regression__max_iter&#39;, &#39;regression__multi_class&#39;, &#39;regression__n_jobs&#39;, &#39;regression__penalty&#39;, &#39;regression__random_state&#39;, &#39;regression__solver&#39;, &#39;regression__tol&#39;, &#39;regression__verbose&#39;, &#39;regression__warm_start&#39;]) . Hacemos un random search . parameters ={ &#39;preprocessor__num__tsvd__n_components&#39;: randint(5, 13), &#39;preprocessor__cat__tsvd__n_components&#39;: randint(5, 8), &#39;regression__multi_class&#39;:[&quot;auto&quot;, &quot;ovr&quot;], &#39;regression__fit_intercept&#39;: [True,False], &#39;regression__intercept_scaling&#39;: randint(1, 10), &#39;regression__max_iter&#39;: randint(100, 500) } . randomregression = RandomizedSearchCV(estimator = regression_pipeline, n_iter=50, param_distributions = parameters, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=3, random_state=42, verbose= 1) . %%time randomregression.fit(X_train, y_train) . Fitting 3 folds for each of 50 candidates, totalling 150 fits Wall time: 1min 6s . RandomizedSearchCV(cv=3, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;preprocessor__num__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55F2F9A0&gt;, &#39;regression__fit_intercept&#39;: [True, False], &#39;regression__intercept_scaling&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5BF26310&gt;, &#39;regression__max_iter&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55F2FDF0&gt;, &#39;regression__multi_class&#39;: [&#39;auto&#39;, &#39;ovr&#39;]}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . randomregression.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 7, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;regression__fit_intercept&#39;: False, &#39;regression__intercept_scaling&#39;: 7, &#39;regression__max_iter&#39;: 351, &#39;regression__multi_class&#39;: &#39;auto&#39;} . randomregression.best_score_ . 0.6929315835247353 . search_logistic_regression = { &#39;preprocessor__cat__tsvd__n_components&#39;: [7, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12], &#39;regression__fit_intercept&#39;: [False], &#39;regression__intercept_scaling&#39;: [7, 5, 6], &#39;regression__max_iter&#39;: [351, 350], &#39;regression__multi_class&#39;: [&#39;auto&#39;] } . logisticgs = GridSearchCV(estimator = regression_pipeline, param_grid = search_logistic_regression, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 4, verbose = 4) . %%time logisticgs.fit(X_train, y_train) . Fitting 4 folds for each of 24 candidates, totalling 96 fits Wall time: 51.6 s . GridSearchCV(cv=4, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... LogisticRegression(class_weight=&#39;balanced&#39;, random_state=0))]), n_jobs=-1, param_grid={&#39;preprocessor__cat__tsvd__n_components&#39;: [7, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12], &#39;regression__fit_intercept&#39;: [False], &#39;regression__intercept_scaling&#39;: [7, 5, 6], &#39;regression__max_iter&#39;: [351, 350], &#39;regression__multi_class&#39;: [&#39;auto&#39;]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=4) . logisticgs.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 7, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;regression__fit_intercept&#39;: False, &#39;regression__intercept_scaling&#39;: 5, &#39;regression__max_iter&#39;: 350, &#39;regression__multi_class&#39;: &#39;auto&#39;} . logisticgs.best_score_ . 0.6928219124999713 . logisticgs_test = logisticgs.predict(X_test) print(classification_report(y_test, logisticgs_test)) . precision recall f1-score support 0 0.82 0.81 0.82 22461 1 0.69 0.70 0.70 13235 accuracy 0.77 35696 macro avg 0.76 0.76 0.76 35696 weighted avg 0.77 0.77 0.77 35696 . cross_val_score(logisticgs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) . Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits . array([0.69400681, 0.69014763, 0.69361397, 0.69150212, 0.69391983]) . cm = confusion_matrix(y_test, logisticgs_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = logisticgs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.8511245004500685 . Random forest . randomf = RandomForestClassifier(random_state=42, class_weight=&#39;balanced&#39;) . randomf_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;randomf&quot;, randomf) ]) . randomf_pipeline.get_params().keys() . dict_keys([&#39;memory&#39;, &#39;steps&#39;, &#39;verbose&#39;, &#39;preprocessor&#39;, &#39;randomf&#39;, &#39;preprocessor__n_jobs&#39;, &#39;preprocessor__remainder&#39;, &#39;preprocessor__sparse_threshold&#39;, &#39;preprocessor__transformer_weights&#39;, &#39;preprocessor__transformers&#39;, &#39;preprocessor__verbose&#39;, &#39;preprocessor__verbose_feature_names_out&#39;, &#39;preprocessor__num&#39;, &#39;preprocessor__cat&#39;, &#39;preprocessor__num__memory&#39;, &#39;preprocessor__num__steps&#39;, &#39;preprocessor__num__verbose&#39;, &#39;preprocessor__num__imputer&#39;, &#39;preprocessor__num__scaler&#39;, &#39;preprocessor__num__tsvd&#39;, &#39;preprocessor__num__imputer__add_indicator&#39;, &#39;preprocessor__num__imputer__copy&#39;, &#39;preprocessor__num__imputer__fill_value&#39;, &#39;preprocessor__num__imputer__missing_values&#39;, &#39;preprocessor__num__imputer__strategy&#39;, &#39;preprocessor__num__imputer__verbose&#39;, &#39;preprocessor__num__scaler__copy&#39;, &#39;preprocessor__num__scaler__with_mean&#39;, &#39;preprocessor__num__scaler__with_std&#39;, &#39;preprocessor__num__tsvd__algorithm&#39;, &#39;preprocessor__num__tsvd__n_components&#39;, &#39;preprocessor__num__tsvd__n_iter&#39;, &#39;preprocessor__num__tsvd__random_state&#39;, &#39;preprocessor__num__tsvd__tol&#39;, &#39;preprocessor__cat__memory&#39;, &#39;preprocessor__cat__steps&#39;, &#39;preprocessor__cat__verbose&#39;, &#39;preprocessor__cat__imputer&#39;, &#39;preprocessor__cat__onehot&#39;, &#39;preprocessor__cat__tsvd&#39;, &#39;preprocessor__cat__imputer__add_indicator&#39;, &#39;preprocessor__cat__imputer__copy&#39;, &#39;preprocessor__cat__imputer__fill_value&#39;, &#39;preprocessor__cat__imputer__missing_values&#39;, &#39;preprocessor__cat__imputer__strategy&#39;, &#39;preprocessor__cat__imputer__verbose&#39;, &#39;preprocessor__cat__onehot__categories&#39;, &#39;preprocessor__cat__onehot__drop&#39;, &#39;preprocessor__cat__onehot__dtype&#39;, &#39;preprocessor__cat__onehot__handle_unknown&#39;, &#39;preprocessor__cat__onehot__sparse&#39;, &#39;preprocessor__cat__tsvd__algorithm&#39;, &#39;preprocessor__cat__tsvd__n_components&#39;, &#39;preprocessor__cat__tsvd__n_iter&#39;, &#39;preprocessor__cat__tsvd__random_state&#39;, &#39;preprocessor__cat__tsvd__tol&#39;, &#39;randomf__bootstrap&#39;, &#39;randomf__ccp_alpha&#39;, &#39;randomf__class_weight&#39;, &#39;randomf__criterion&#39;, &#39;randomf__max_depth&#39;, &#39;randomf__max_features&#39;, &#39;randomf__max_leaf_nodes&#39;, &#39;randomf__max_samples&#39;, &#39;randomf__min_impurity_decrease&#39;, &#39;randomf__min_samples_leaf&#39;, &#39;randomf__min_samples_split&#39;, &#39;randomf__min_weight_fraction_leaf&#39;, &#39;randomf__n_estimators&#39;, &#39;randomf__n_jobs&#39;, &#39;randomf__oob_score&#39;, &#39;randomf__random_state&#39;, &#39;randomf__verbose&#39;, &#39;randomf__warm_start&#39;]) . Hacemos un random search . param_dist = { &quot;preprocessor__num__tsvd__n_components&quot;: randint(5, 13), &quot;preprocessor__cat__tsvd__n_components&quot;: randint(5, 8), &#39;randomf__n_estimators&#39;: randint(100, 2000), &#39;randomf__max_depth&#39;: randint(10, 100), &#39;randomf__max_leaf_nodes&#39;: randint(3, 100), &#39;randomf__max_samples&#39;: randint(1, 100), &#39;randomf__min_samples_split&#39;: randint(4, 100) } . rsrandomf = RandomizedSearchCV(estimator = randomf_pipeline, n_iter=100, param_distributions = param_dist, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=4, random_state=42, verbose= 1) . %%time rsrandomf.fit(X_train, y_train) . Fitting 4 folds for each of 100 candidates, totalling 400 fits Wall time: 17min 23s . RandomizedSearchCV(cv=4, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;randomf__max_leaf_nodes&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55F247C0&gt;, &#39;randomf__max_samples&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55CBF520&gt;, &#39;randomf__min_samples_split&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55B69CA0&gt;, &#39;randomf__n_estimators&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE537BAA30&gt;}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . rsrandomf.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 5, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;randomf__max_depth&#39;: 26, &#39;randomf__max_leaf_nodes&#39;: 46, &#39;randomf__max_samples&#39;: 92, &#39;randomf__min_samples_split&#39;: 33, &#39;randomf__n_estimators&#39;: 576} . rsrandomf.best_score_ . 0.6644808092260801 . search_random_forest = { &#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [12, 11], &#39;randomf__max_depth&#39;: [23, 25, 26], &#39;randomf__max_samples&#39;: [93, 92], &#39;randomf__n_estimators&#39;: [576] } . randomgs = GridSearchCV(estimator = randomf_pipeline, param_grid = search_random_forest, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 5, verbose = 4) . %%time randomgs.fit(X_train, y_train) . Fitting 5 folds for each of 24 candidates, totalling 120 fits Wall time: 3min 24s . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... &#39;deposit_type&#39;, &#39;customer_type&#39;])])), (&#39;randomf&#39;, RandomForestClassifier(class_weight=&#39;balanced&#39;, random_state=42))]), n_jobs=-1, param_grid={&#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [12, 11], &#39;randomf__max_depth&#39;: [23, 25, 26], &#39;randomf__max_samples&#39;: [93, 92], &#39;randomf__n_estimators&#39;: [576]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=4) . randomgs.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 5, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;randomf__max_depth&#39;: 25, &#39;randomf__max_samples&#39;: 92, &#39;randomf__n_estimators&#39;: 576} . randomgs.best_score_ . 0.5972750870383681 . randomgs_test = randomgs.predict(X_test) print(classification_report(y_test, randomgs_test)) . precision recall f1-score support 0 0.75 0.98 0.85 22461 1 0.94 0.43 0.59 13235 accuracy 0.78 35696 macro avg 0.84 0.71 0.72 35696 weighted avg 0.82 0.78 0.76 35696 . cross_val_score(randomgs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) . Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits . array([0.59878654, 0.59763248, 0.60024168, 0.58686229, 0.59151488]) . cm = confusion_matrix(y_test, randomgs_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = randomgs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.8616904872445909 . LightGBM . light = LGBMClassifier(random_state=0, class_weight=&#39;balanced&#39;) . light_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;light&quot;, light) ]) . lgbm_parameters = { &#39;preprocessor__num__tsvd__n_components&#39;: randint(5, 12), &#39;preprocessor__cat__tsvd__n_components&#39;: randint(5, 8), &#39;light__n_estimators&#39;: randint(2000, 5000), &#39;light__max_depth&#39;: randint(1, 20), &#39;light__num_leaves&#39;: randint(10, 50), } . random_light = RandomizedSearchCV(estimator = light_pipeline, n_iter=100, param_distributions = lgbm_parameters, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=4, random_state=42, verbose= 1) . %%time random_light.fit(X_train, y_train) . Fitting 4 folds for each of 100 candidates, totalling 400 fits Wall time: 1h 1min 58s . RandomizedSearchCV(cv=4, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;light__num_leaves&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55B8A1C0&gt;, &#39;preprocessor__cat__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5387EFD0&gt;, &#39;preprocessor__num__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5BD06EB0&gt;}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . random_light.best_params_ . {&#39;light__max_depth&#39;: 15, &#39;light__n_estimators&#39;: 3115, &#39;light__num_leaves&#39;: 43, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 11} . random_light.best_score_ . 0.8377610877611602 . search_light = { &#39;light__max_depth&#39;: [15, 16, 17], &#39;light__n_estimators&#39;: [3115], &#39;light__num_leaves&#39;: [43, 44, 45], &#39;preprocessor__cat__tsvd__n_components&#39;: [6], &#39;preprocessor__num__tsvd__n_components&#39;: [11] } . lightgs = GridSearchCV(estimator = light_pipeline, param_grid = search_light, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 5, verbose = 4) . %%time lightgs.fit(X_train, y_train) . Fitting 5 folds for each of 9 candidates, totalling 45 fits Wall time: 7min 33s . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... &#39;assigned_room_type&#39;, &#39;deposit_type&#39;, &#39;customer_type&#39;])])), (&#39;light&#39;, LGBMClassifier(class_weight=&#39;balanced&#39;, random_state=0))]), n_jobs=-1, param_grid={&#39;light__max_depth&#39;: [15, 16, 17], &#39;light__n_estimators&#39;: [3115], &#39;light__num_leaves&#39;: [43, 44, 45], &#39;preprocessor__cat__tsvd__n_components&#39;: [6], &#39;preprocessor__num__tsvd__n_components&#39;: [11]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=4) . lightgs.best_params_ . {&#39;light__max_depth&#39;: 16, &#39;light__n_estimators&#39;: 3115, &#39;light__num_leaves&#39;: 45, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 11} . lightgs.best_score_ . 0.8406825152421916 . lightgs_test = lightgs.predict(X_test) print(classification_report(y_test, lightgs_test)) . precision recall f1-score support 0 0.90 0.94 0.92 22461 1 0.88 0.82 0.85 13235 accuracy 0.89 35696 macro avg 0.89 0.88 0.88 35696 weighted avg 0.89 0.89 0.89 35696 . cross_val_score(lightgs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) . Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits . array([0.84282116, 0.84072666, 0.83988882, 0.8364557 , 0.8356315 ]) . cm = confusion_matrix(y_test, lightgs_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = lightgs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.9515071138628284 .",
            "url": "https://pabloja4.github.io/portfolio/2021/11/02/Booking.html",
            "relUrl": "/2021/11/02/Booking.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://pabloja4.github.io/portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pabloja4.github.io/portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Comunicador Social y Periodista egresado de La Universidad de Manizales con formación en Inteligencia Artificial, Machine Learning (ML) y Ciencia de Datos. Experiencia desarrollando modelos supervisados de clasificación y regresión en ML. Conocimientos e interés en modelos no supervisados de clustering y reportería de datos. . Orientado al logro, con habilidades como la tolerancia al error, atención al detalle, resolución de problemas, trabajo en equipo y análisis de datos. Manejo de herramientas como Power BI, SQL server. Lenguajes como SQL y Python, con librerías tales como Pandas, Numpy, Seaborn, Matplotlib y Sklearn. . https://www.linkedin.com/in/pablojaramillojaramillo/ .",
          "url": "https://pabloja4.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pabloja4.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}