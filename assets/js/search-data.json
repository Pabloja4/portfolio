{
  
    
        "post0": {
            "title": "Customer segmentation - Kmeans clustering",
            "content": "The objective of the team is to develop a model that predicts customer behavior and to apply it to the rest of the customer base. Hopefully, the model will allow the company to cherry pick the customers that are most likely to purchase the offer while leaving out the non-respondents, making the next campaign highly profitable. . import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, None) import datetime from sklearn.decomposition import PCA from sklearn.preprocessing import MinMaxScaler, StandardScaler from sklearn.cluster import KMeans from sklearn.preprocessing import OneHotEncoder from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV from lightgbm import LGBMClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import RandomizedSearchCV from sklearn.metrics import classification_report, accuracy_score from sklearn.metrics import confusion_matrix from scipy.stats import randint import random sns.set(style=&#39;white&#39;, context=&#39;notebook&#39;, palette=&quot;RdBu&quot;, rc={&#39;figure.figsize&#39;:(9,7)}) %matplotlib inline . df = pd.read_csv(r&quot;C: Users pablo Documents GitHub iFood_project data ml_project1_data.csv&quot;) df.head() . ID Year_Birth Education Marital_Status Income Kidhome Teenhome Dt_Customer Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Z_CostContact Z_Revenue Response . 0 5524 | 1957 | Graduation | Single | 58138.0 | 0 | 0 | 2012-09-04 | 58 | 635 | 88 | 546 | 172 | 88 | 88 | 3 | 8 | 10 | 4 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 1 | . 1 2174 | 1954 | Graduation | Single | 46344.0 | 1 | 1 | 2014-03-08 | 38 | 11 | 1 | 6 | 2 | 1 | 6 | 2 | 1 | 1 | 2 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . 2 4141 | 1965 | Graduation | Together | 71613.0 | 0 | 0 | 2013-08-21 | 26 | 426 | 49 | 127 | 111 | 21 | 42 | 1 | 8 | 2 | 10 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . 3 6182 | 1984 | Graduation | Together | 26646.0 | 1 | 0 | 2014-02-10 | 26 | 11 | 4 | 20 | 10 | 3 | 5 | 2 | 2 | 0 | 4 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . 4 5324 | 1981 | PhD | Married | 58293.0 | 1 | 0 | 2014-01-19 | 94 | 173 | 43 | 118 | 46 | 27 | 15 | 5 | 5 | 3 | 6 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . The data set contains socio-demographic and firmographic features about 2.240 customers who were contacted. Additionally, it contains a flag for those customers who responded the campaign, by buying the product. . df.shape . (2240, 29) . Exploratory Data Analysis . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2240 entries, 0 to 2239 Data columns (total 29 columns): # Column Non-Null Count Dtype -- -- 0 ID 2240 non-null int64 1 Year_Birth 2240 non-null int64 2 Education 2240 non-null object 3 Marital_Status 2240 non-null object 4 Income 2216 non-null float64 5 Kidhome 2240 non-null int64 6 Teenhome 2240 non-null int64 7 Dt_Customer 2240 non-null object 8 Recency 2240 non-null int64 9 MntWines 2240 non-null int64 10 MntFruits 2240 non-null int64 11 MntMeatProducts 2240 non-null int64 12 MntFishProducts 2240 non-null int64 13 MntSweetProducts 2240 non-null int64 14 MntGoldProds 2240 non-null int64 15 NumDealsPurchases 2240 non-null int64 16 NumWebPurchases 2240 non-null int64 17 NumCatalogPurchases 2240 non-null int64 18 NumStorePurchases 2240 non-null int64 19 NumWebVisitsMonth 2240 non-null int64 20 AcceptedCmp3 2240 non-null int64 21 AcceptedCmp4 2240 non-null int64 22 AcceptedCmp5 2240 non-null int64 23 AcceptedCmp1 2240 non-null int64 24 AcceptedCmp2 2240 non-null int64 25 Complain 2240 non-null int64 26 Z_CostContact 2240 non-null int64 27 Z_Revenue 2240 non-null int64 28 Response 2240 non-null int64 dtypes: float64(1), int64(25), object(3) memory usage: 507.6+ KB . df.isna().sum() . ID 0 Year_Birth 0 Education 0 Marital_Status 0 Income 24 Kidhome 0 Teenhome 0 Dt_Customer 0 Recency 0 MntWines 0 MntFruits 0 MntMeatProducts 0 MntFishProducts 0 MntSweetProducts 0 MntGoldProds 0 NumDealsPurchases 0 NumWebPurchases 0 NumCatalogPurchases 0 NumStorePurchases 0 NumWebVisitsMonth 0 AcceptedCmp3 0 AcceptedCmp4 0 AcceptedCmp5 0 AcceptedCmp1 0 AcceptedCmp2 0 Complain 0 Z_CostContact 0 Z_Revenue 0 Response 0 dtype: int64 . The Income feature has some missing values. | . df.hist(bins=20, figsize=(20,20), color=&quot;red&quot;) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;ID&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Year_Birth&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Income&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Kidhome&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Teenhome&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Recency&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;MntWines&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;MntFruits&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;MntMeatProducts&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;MntFishProducts&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;MntSweetProducts&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;MntGoldProds&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;NumDealsPurchases&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;NumWebPurchases&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;NumCatalogPurchases&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;NumStorePurchases&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;NumWebVisitsMonth&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;AcceptedCmp3&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;AcceptedCmp4&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;AcceptedCmp5&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;AcceptedCmp1&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;AcceptedCmp2&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Complain&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Z_CostContact&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Z_Revenue&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Response&#39;}&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . df.describe() . ID Year_Birth Income Kidhome Teenhome Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Z_CostContact Z_Revenue Response . count 2240.000000 | 2240.000000 | 2216.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.000000 | 2240.0 | 2240.0 | 2240.000000 | . mean 5592.159821 | 1968.805804 | 52247.251354 | 0.444196 | 0.506250 | 49.109375 | 303.935714 | 26.302232 | 166.950000 | 37.525446 | 27.062946 | 44.021875 | 2.325000 | 4.084821 | 2.662054 | 5.790179 | 5.316518 | 0.072768 | 0.074554 | 0.072768 | 0.064286 | 0.013393 | 0.009375 | 3.0 | 11.0 | 0.149107 | . std 3246.662198 | 11.984069 | 25173.076661 | 0.538398 | 0.544538 | 28.962453 | 336.597393 | 39.773434 | 225.715373 | 54.628979 | 41.280498 | 52.167439 | 1.932238 | 2.778714 | 2.923101 | 3.250958 | 2.426645 | 0.259813 | 0.262728 | 0.259813 | 0.245316 | 0.114976 | 0.096391 | 0.0 | 0.0 | 0.356274 | . min 0.000000 | 1893.000000 | 1730.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 3.0 | 11.0 | 0.000000 | . 25% 2828.250000 | 1959.000000 | 35303.000000 | 0.000000 | 0.000000 | 24.000000 | 23.750000 | 1.000000 | 16.000000 | 3.000000 | 1.000000 | 9.000000 | 1.000000 | 2.000000 | 0.000000 | 3.000000 | 3.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 3.0 | 11.0 | 0.000000 | . 50% 5458.500000 | 1970.000000 | 51381.500000 | 0.000000 | 0.000000 | 49.000000 | 173.500000 | 8.000000 | 67.000000 | 12.000000 | 8.000000 | 24.000000 | 2.000000 | 4.000000 | 2.000000 | 5.000000 | 6.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 3.0 | 11.0 | 0.000000 | . 75% 8427.750000 | 1977.000000 | 68522.000000 | 1.000000 | 1.000000 | 74.000000 | 504.250000 | 33.000000 | 232.000000 | 50.000000 | 33.000000 | 56.000000 | 3.000000 | 6.000000 | 4.000000 | 8.000000 | 7.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 3.0 | 11.0 | 0.000000 | . max 11191.000000 | 1996.000000 | 666666.000000 | 2.000000 | 2.000000 | 99.000000 | 1493.000000 | 199.000000 | 1725.000000 | 259.000000 | 263.000000 | 362.000000 | 15.000000 | 27.000000 | 28.000000 | 13.000000 | 20.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 3.0 | 11.0 | 1.000000 | . The Year_birth feature seams to has outliers. | . df[df[&quot;Year_Birth&quot;] &lt; 1940] . ID Year_Birth Education Marital_Status Income Kidhome Teenhome Dt_Customer Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Z_CostContact Z_Revenue Response . 192 7829 | 1900 | 2n Cycle | Divorced | 36640.0 | 1 | 0 | 2013-09-26 | 99 | 15 | 6 | 8 | 7 | 4 | 25 | 1 | 2 | 1 | 2 | 5 | 0 | 0 | 0 | 0 | 0 | 1 | 3 | 11 | 0 | . 239 11004 | 1893 | 2n Cycle | Single | 60182.0 | 0 | 1 | 2014-05-17 | 23 | 8 | 0 | 5 | 7 | 0 | 2 | 1 | 1 | 0 | 2 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . 339 1150 | 1899 | PhD | Together | 83532.0 | 0 | 0 | 2013-09-26 | 36 | 755 | 144 | 562 | 104 | 64 | 224 | 1 | 4 | 6 | 4 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 3 | 11 | 0 | . We need to clean those entries before training the model. | . df[df[&quot;Income&quot;] &gt; 160000] . ID Year_Birth Education Marital_Status Income Kidhome Teenhome Dt_Customer Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Z_CostContact Z_Revenue Response . 617 1503 | 1976 | PhD | Together | 162397.0 | 1 | 1 | 2013-06-03 | 31 | 85 | 1 | 16 | 2 | 1 | 2 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . 687 1501 | 1982 | PhD | Married | 160803.0 | 0 | 0 | 2012-08-04 | 21 | 55 | 16 | 1622 | 17 | 3 | 4 | 15 | 0 | 28 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . 2233 9432 | 1977 | Graduation | Together | 666666.0 | 1 | 0 | 2013-06-02 | 23 | 9 | 14 | 18 | 8 | 1 | 12 | 4 | 3 | 1 | 3 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 11 | 0 | . 666666.0 doesn&#39;t look like a real income, we have to clean it as well. | . Target distribution . df.groupby([&#39;Response&#39;]).Year_Birth.agg([len, min, max]) . len min max . Response . 0 1906 | 1893 | 1996 | . 1 334 | 1943 | 1996 | . df.groupby(&quot;Response&quot;)[&quot;Education&quot;].count() . Response 0 1906 1 334 Name: Education, dtype: int64 . sns.histplot(data=df, x= &quot;Response&quot;, kde=False, bins=5) . &lt;AxesSubplot:xlabel=&#39;Response&#39;, ylabel=&#39;Count&#39;&gt; . Target distribution is inbalanced . Categorical unique values . df[&quot;Education&quot;].unique() . array([&#39;Graduation&#39;, &#39;PhD&#39;, &#39;Master&#39;, &#39;Basic&#39;, &#39;2n Cycle&#39;], dtype=object) . df[&quot;Marital_Status&quot;].unique() . array([&#39;Single&#39;, &#39;Together&#39;, &#39;Married&#39;, &#39;Divorced&#39;, &#39;Widow&#39;, &#39;Alone&#39;, &#39;Absurd&#39;, &#39;YOLO&#39;], dtype=object) . Marital status has odd category names such as &quot;YOLO&quot; | . EDA findings . The Income feature has some missing values. | The Year_birth feature seams to has outliers, we need to clean those entries before training the model. | 666666.0 doesn&#39;t look like a real income, we have to clean it as well. | Marital status has odd category names such as &quot;YOLO&quot; | DtCostumer should be change to days enrolled. | . Cleaning data . I Will clean the data due to the findings of the EDA. . This line will fill the missing values with the mean income | . df[&quot;Income&quot;].fillna(value=df[&quot;Income&quot;].mean(), inplace=True) . df[&quot;Income&quot;].isna().sum() . 0 . Costumeres with an age greater than 80 will be droped . df.drop(df[df[&quot;Year_Birth&quot;] &lt; 1942].index, inplace=True) . Income overlays will be drop . df.drop(df[df[&quot;Income&quot;] &gt; 300000].index, inplace=True) . Odd Marital status will be eliminate. . df[&quot;Marital_Status&quot;].replace(&quot;Alone&quot;, &quot;Single&quot;, inplace=True) df.drop(df[df[&quot;Marital_Status&quot;] == &#39;Absurd&#39;].index, inplace=True) df.drop(df[df[&quot;Marital_Status&quot;] == &#39;YOLO&#39;].index, inplace=True) . Date Customer needs a type change: . df[&quot;Dt_Customer&quot;] = pd.to_datetime(df[&quot;Dt_Customer&quot;]) df[&quot;Dt_Customer&quot;].sample(5) . 1074 2013-07-18 361 2014-03-22 948 2013-12-06 1169 2014-01-22 1087 2013-01-04 Name: Dt_Customer, dtype: datetime64[ns] . &quot;Z_CostContact&quot; and &quot;Z_Revenue&quot; don&#39;t have relevant data or correlation with the targer . df.drop(columns=[&quot;Z_CostContact&quot;, &quot;Z_Revenue&quot;], inplace=True, axis=1) . df.describe(include=[&#39;O&#39;]) . Education Marital_Status . count 2230 | 2230 | . unique 5 | 5 | . top Graduation | Married | . freq 1125 | 863 | . Categorical data looks clean now . df.describe() . ID Year_Birth Income Kidhome Teenhome Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Response . count 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | 2230.000000 | . mean 5587.309417 | 1968.914350 | 51928.726472 | 0.445291 | 0.507175 | 49.120628 | 303.697309 | 26.249327 | 166.783408 | 37.424664 | 27.123318 | 43.868610 | 2.325561 | 4.084753 | 2.657848 | 5.795964 | 5.321525 | 0.073094 | 0.074888 | 0.071749 | 0.064126 | 0.013453 | 0.008969 | 0.148879 | . std 3244.647498 | 11.673567 | 21412.547317 | 0.538690 | 0.544719 | 28.939631 | 336.364482 | 39.724304 | 225.529050 | 54.473502 | 41.330557 | 51.887559 | 1.932885 | 2.781483 | 2.918509 | 3.255128 | 2.425051 | 0.260349 | 0.263269 | 0.258130 | 0.245031 | 0.115230 | 0.094298 | 0.356049 | . min 0.000000 | 1943.000000 | 1730.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 2826.750000 | 1959.000000 | 35422.250000 | 0.000000 | 0.000000 | 24.000000 | 24.000000 | 1.000000 | 16.000000 | 3.000000 | 1.000000 | 9.000000 | 1.000000 | 2.000000 | 0.000000 | 3.000000 | 3.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 5453.500000 | 1970.000000 | 51684.000000 | 0.000000 | 0.000000 | 49.000000 | 173.000000 | 8.000000 | 67.000000 | 12.000000 | 8.000000 | 24.000000 | 2.000000 | 4.000000 | 2.000000 | 5.000000 | 6.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 75% 8425.250000 | 1977.000000 | 68242.500000 | 1.000000 | 1.000000 | 74.000000 | 504.750000 | 33.000000 | 231.750000 | 50.000000 | 33.000000 | 56.000000 | 3.000000 | 6.000000 | 4.000000 | 8.000000 | 7.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . max 11191.000000 | 1996.000000 | 162397.000000 | 2.000000 | 2.000000 | 99.000000 | 1493.000000 | 199.000000 | 1725.000000 | 259.000000 | 263.000000 | 362.000000 | 15.000000 | 27.000000 | 28.000000 | 13.000000 | 20.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . Feature engineering . Customer&#39;s age . An &quot;age&quot; column may come in handy for analytic purposes . df[&quot;age&quot;] = pd.to_datetime(df[&quot;Year_Birth&quot;], yearfirst=True, format=&quot;%Y&quot;) . df[&quot;age&quot;].head() . 0 1957-01-01 1 1954-01-01 2 1965-01-01 3 1984-01-01 4 1981-01-01 Name: age, dtype: datetime64[ns] . def year_age(born): today = datetime.date.today() return today.year - born.year - ((today.month, today.day) &lt; (born.month, born.day)) . datetime.date.today() . datetime.date(2022, 2, 28) . df[&quot;age&quot;] = df[&quot;age&quot;].apply(lambda x: year_age(x)) . df[&quot;age&quot;].sample(5) . 1157 48 1963 70 1825 53 1803 56 2079 68 Name: age, dtype: int64 . Total standard products purchased . df[&quot;MntStandardProds&quot;] = df[&quot;MntWines&quot;] + df[&quot;MntFruits&quot;]+df[&quot;MntMeatProducts&quot;]+df[&quot;MntFishProducts&quot;]+df[&quot;MntSweetProducts&quot;] df[&quot;MntStandardProds&quot;].head() . 0 1529 1 21 2 734 3 48 4 407 Name: MntStandardProds, dtype: int64 . Days since enrollment . def day_enrollment(date): today = pd.to_datetime(datetime.date.today()) days = today - date return days.days . datetime.date.today() . datetime.date(2022, 2, 28) . df[&quot;days_enroll&quot;] = df[&quot;Dt_Customer&quot;].apply(lambda x: day_enrollment(x)) . df[&quot;days_enroll&quot;].sample(5).reset_index() . index days_enroll . 0 1251 | 3149 | . 1 2118 | 3246 | . 2 740 | 3264 | . 3 1110 | 3290 | . 4 1880 | 3423 | . Data overview . After some transformations, our data looks like this: . df.reset_index(inplace=True) . df.shape . (2230, 31) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2230 entries, 0 to 2229 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 index 2230 non-null int64 1 ID 2230 non-null int64 2 Year_Birth 2230 non-null int64 3 Education 2230 non-null object 4 Marital_Status 2230 non-null object 5 Income 2230 non-null float64 6 Kidhome 2230 non-null int64 7 Teenhome 2230 non-null int64 8 Dt_Customer 2230 non-null datetime64[ns] 9 Recency 2230 non-null int64 10 MntWines 2230 non-null int64 11 MntFruits 2230 non-null int64 12 MntMeatProducts 2230 non-null int64 13 MntFishProducts 2230 non-null int64 14 MntSweetProducts 2230 non-null int64 15 MntGoldProds 2230 non-null int64 16 NumDealsPurchases 2230 non-null int64 17 NumWebPurchases 2230 non-null int64 18 NumCatalogPurchases 2230 non-null int64 19 NumStorePurchases 2230 non-null int64 20 NumWebVisitsMonth 2230 non-null int64 21 AcceptedCmp3 2230 non-null int64 22 AcceptedCmp4 2230 non-null int64 23 AcceptedCmp5 2230 non-null int64 24 AcceptedCmp1 2230 non-null int64 25 AcceptedCmp2 2230 non-null int64 26 Complain 2230 non-null int64 27 Response 2230 non-null int64 28 age 2230 non-null int64 29 MntStandardProds 2230 non-null int64 30 days_enroll 2230 non-null int64 dtypes: datetime64[ns](1), float64(1), int64(27), object(2) memory usage: 540.2+ KB . 10 entries and 2 columns were deleted and 3 new features were added . Descriptive Analytics . df[&quot;age&quot;].hist(bins=5) . &lt;AxesSubplot:&gt; . Customer between 40 and 60 have better reseption to the product | . plt.figure(figsize=[15,6]) sns.countplot(x=&quot;age&quot;, hue = &quot;Response&quot;, data= df) plt.xlabel(&quot;Response by age&quot;) . Text(0.5, 0, &#39;Response by age&#39;) . plt.figure(figsize=[10,5]) sns.barplot(data=df, y=&quot;MntGoldProds&quot;, x=&quot;Response&quot;) . &lt;AxesSubplot:xlabel=&#39;Response&#39;, ylabel=&#39;MntGoldProds&#39;&gt; . plt.figure(figsize=[10,5]) sns.barplot(data=df, y=&quot;MntStandardProds&quot;, x=&quot;Response&quot;) . &lt;AxesSubplot:xlabel=&#39;Response&#39;, ylabel=&#39;MntStandardProds&#39;&gt; . Customers who accepted the gadget tend to purchase more products | . plt.figure(figsize=[20,15]) plt.subplot(411) sns.countplot(x=&quot;Education&quot;, hue = &quot;Response&quot;, data= df) plt.subplot(412) sns.countplot(x=&quot;Marital_Status&quot;, hue = &quot;Response&quot;, data= df) . &lt;AxesSubplot:xlabel=&#39;Marital_Status&#39;, ylabel=&#39;count&#39;&gt; . Graduate and single clients are those who accept the most, Maried customers have a great acceptation as well and they are the ones who buy the most. | . plt.figure(figsize=[20,15]) plt.subplot(411) sns.countplot(x=&quot;Kidhome&quot;, hue = &quot;Response&quot;, data= df) plt.subplot(412) sns.countplot(x=&quot;Teenhome&quot;, hue = &quot;Response&quot;, data= df) . &lt;AxesSubplot:xlabel=&#39;Teenhome&#39;, ylabel=&#39;count&#39;&gt; . Customers without children have a bigger response. | . plt.figure(figsize=[20,15]) plt.subplot(411) sns.histplot(x=&quot;Income&quot;, hue = &quot;Response&quot;, data= df) plt.xlabel(&quot;Response by Income&quot;) plt.subplot(412) sns.histplot(y=&quot;Complain&quot;, hue = &quot;Response&quot;, data= df) plt.xlabel(&quot;Response by Complain&quot;) plt.subplot(413) sns.histplot(x=&quot;NumWebPurchases&quot;, hue = &quot;Response&quot;, data= df) plt.xlabel(&quot;Response by Web purchases&quot;) plt.subplot(414) sns.histplot(x=&quot;NumStorePurchases&quot;, hue = &quot;Response&quot;, data= df) plt.xlabel(&quot;Response by Store purchases&quot;) . Text(0.5, 0, &#39;Response by Store purchases&#39;) . df.sample() . index ID Year_Birth Education Marital_Status Income Kidhome Teenhome Dt_Customer Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Response age MntStandardProds days_enroll . 757 761 | 6887 | 1967 | Graduation | Single | 79146.0 | 1 | 1 | 2014-04-24 | 33 | 245 | 16 | 223 | 21 | 43 | 16 | 2 | 8 | 1 | 8 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 55 | 548 | 2867 | . plt.figure(figsize=[10,7]) sns.lineplot(data=df, x= &quot;age&quot;, y=&quot;Recency&quot;) . &lt;AxesSubplot:xlabel=&#39;age&#39;, ylabel=&#39;Recency&#39;&gt; . plt.figure(figsize=[10,7]) df[&quot;Recency&quot;].hist(bins=5) . &lt;AxesSubplot:&gt; . df[df[&quot;Recency&quot;]&gt;90] . index ID Year_Birth Education Marital_Status Income Kidhome Teenhome Dt_Customer Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Response age MntStandardProds days_enroll . 4 4 | 5324 | 1981 | PhD | Married | 58293.0 | 1 | 0 | 2014-01-19 | 94 | 173 | 43 | 118 | 46 | 27 | 15 | 5 | 5 | 3 | 6 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 41 | 407 | 2962 | . 18 18 | 6565 | 1949 | Master | Married | 76995.0 | 0 | 1 | 2013-03-28 | 91 | 1012 | 80 | 498 | 0 | 16 | 176 | 2 | 11 | 4 | 9 | 5 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 73 | 1606 | 3259 | . 29 29 | 1966 | 1965 | PhD | Married | 84618.0 | 0 | 0 | 2013-11-22 | 96 | 684 | 100 | 801 | 21 | 66 | 0 | 1 | 6 | 9 | 10 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 57 | 1672 | 3020 | . 38 38 | 8595 | 1973 | Graduation | Widow | 42429.0 | 0 | 1 | 2014-02-11 | 99 | 55 | 0 | 6 | 2 | 0 | 4 | 2 | 1 | 1 | 3 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 49 | 63 | 2939 | . 47 47 | 7286 | 1968 | Graduation | Together | 41728.0 | 1 | 0 | 2013-05-24 | 92 | 13 | 6 | 15 | 3 | 5 | 13 | 1 | 2 | 0 | 2 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 54 | 42 | 3202 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2158 2165 | 9706 | 1974 | PhD | Single | 31560.0 | 1 | 0 | 2013-06-24 | 98 | 62 | 1 | 20 | 4 | 0 | 7 | 2 | 2 | 1 | 3 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 87 | 3171 | . 2160 2167 | 3520 | 1990 | Master | Single | 91172.0 | 0 | 0 | 2013-03-27 | 94 | 162 | 28 | 818 | 0 | 28 | 56 | 0 | 4 | 3 | 7 | 3 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 32 | 1036 | 3260 | . 2161 2168 | 10394 | 1984 | Graduation | Married | 90000.0 | 0 | 0 | 2013-12-23 | 91 | 675 | 144 | 133 | 94 | 192 | 241 | 1 | 4 | 8 | 5 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 38 | 1238 | 2989 | . 2174 2182 | 3266 | 1964 | Graduation | Married | 42523.0 | 0 | 0 | 2014-04-23 | 96 | 14 | 36 | 11 | 3 | 26 | 35 | 1 | 1 | 1 | 4 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 58 | 90 | 2868 | . 2227 2237 | 7270 | 1981 | Graduation | Divorced | 56981.0 | 0 | 0 | 2014-01-25 | 91 | 908 | 48 | 217 | 32 | 12 | 24 | 1 | 2 | 3 | 13 | 6 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 41 | 1217 | 2956 | . 196 rows × 31 columns . plt.figure(figsize=[10,7]) recency_age = df.groupby(&quot;age&quot;)[&quot;Recency&quot;].mean().hist(bins=100) . Between 40 and 55 years old there is a higher average of recency | . Correlations . plt.figure(figsize=(12, 8)) df_correlation = df.corr() target_correlation = df_correlation.loc[:, [&#39;Response&#39;]].sort_values(&#39;Response&#39;, ascending=False) sns.heatmap(target_correlation, annot=True, cmap=&quot;RdYlGn&quot;, vmin=-1, vmax=1) . &lt;AxesSubplot:&gt; . f, ax = plt.subplots(figsize= (20,10)) sns.heatmap(df.corr(), cmap=&quot;RdYlGn&quot;, annot=True) . &lt;AxesSubplot:&gt; . Customer segmentation . We need to encode categorical features to binary values . cat_ohe = df[[&quot;Education&quot;, &quot;Marital_Status&quot;]] . cat_ohe.head(3) . Education Marital_Status . 0 Graduation | Single | . 1 Graduation | Single | . 2 Graduation | Together | . ohe = OneHotEncoder(dtype=&#39;int32&#39;) val_cat = ohe.fit_transform(cat_ohe).toarray() val_cat = pd.DataFrame(val_cat) ohe_features = ohe.get_feature_names_out([&quot;Education&quot;, &quot;Marital_Status&quot;]) val_cat.columns = ohe_features val_cat.head(3) . Education_2n Cycle Education_Basic Education_Graduation Education_Master Education_PhD Marital_Status_Divorced Marital_Status_Married Marital_Status_Single Marital_Status_Together Marital_Status_Widow . 0 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 2 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . val_cat.shape . (2230, 10) . val_cat.isna().sum() . Education_2n Cycle 0 Education_Basic 0 Education_Graduation 0 Education_Master 0 Education_PhD 0 Marital_Status_Divorced 0 Marital_Status_Married 0 Marital_Status_Single 0 Marital_Status_Together 0 Marital_Status_Widow 0 dtype: int64 . Marital Status and Education are now numerical features . val_num = df.drop(columns=[&quot;Response&quot;, &quot;Year_Birth&quot;,&quot;Education&quot;, &quot;Marital_Status&quot;, &quot;Dt_Customer&quot;], axis=1) val_num.shape . (2230, 26) . df_ohe = pd.concat([val_cat, val_num], axis=1, join=&quot;inner&quot;) . df_ohe.head(5) . Education_2n Cycle Education_Basic Education_Graduation Education_Master Education_PhD Marital_Status_Divorced Marital_Status_Married Marital_Status_Single Marital_Status_Together Marital_Status_Widow index ID Income Kidhome Teenhome Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain age MntStandardProds days_enroll . 0 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0 | 5524 | 58138.0 | 0 | 0 | 58 | 635 | 88 | 546 | 172 | 88 | 88 | 3 | 8 | 10 | 4 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 65 | 1529 | 3464 | . 1 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1 | 2174 | 46344.0 | 1 | 1 | 38 | 11 | 1 | 6 | 2 | 1 | 6 | 2 | 1 | 1 | 2 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 68 | 21 | 2914 | . 2 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 2 | 4141 | 71613.0 | 0 | 0 | 26 | 426 | 49 | 127 | 111 | 21 | 42 | 1 | 8 | 2 | 10 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 57 | 734 | 3113 | . 3 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 3 | 6182 | 26646.0 | 1 | 0 | 26 | 11 | 4 | 20 | 10 | 3 | 5 | 2 | 2 | 0 | 4 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 38 | 48 | 2940 | . 4 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 4 | 5324 | 58293.0 | 1 | 0 | 94 | 173 | 43 | 118 | 46 | 27 | 15 | 5 | 5 | 3 | 6 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 41 | 407 | 2962 | . df_ohe.shape . (2230, 36) . df_ohe.isna().sum() . Education_2n Cycle 0 Education_Basic 0 Education_Graduation 0 Education_Master 0 Education_PhD 0 Marital_Status_Divorced 0 Marital_Status_Married 0 Marital_Status_Single 0 Marital_Status_Together 0 Marital_Status_Widow 0 index 0 ID 0 Income 0 Kidhome 0 Teenhome 0 Recency 0 MntWines 0 MntFruits 0 MntMeatProducts 0 MntFishProducts 0 MntSweetProducts 0 MntGoldProds 0 NumDealsPurchases 0 NumWebPurchases 0 NumCatalogPurchases 0 NumStorePurchases 0 NumWebVisitsMonth 0 AcceptedCmp3 0 AcceptedCmp4 0 AcceptedCmp5 0 AcceptedCmp1 0 AcceptedCmp2 0 Complain 0 age 0 MntStandardProds 0 days_enroll 0 dtype: int64 . scaler = StandardScaler() df_scaled = scaler.fit_transform(df_ohe) . score_1 = [] range_values = range(1, 11) for i in range_values: kmeans = KMeans(n_clusters = i) kmeans.fit(df_scaled) score_1.append(kmeans.inertia_) plt.plot(range_values, score_1, &quot;rx-&quot; ) plt.title(&quot;Optimal cluster&quot;) plt.xlabel(&quot;Cluster&quot;) plt.ylabel(&quot;wcss[k]&quot;) plt.show() . 4 or 5 would be an optimal number of clusters . kmeans = KMeans(5) kmeans.fit(df_scaled) labels = kmeans.labels_ print(&quot;Customer types: &quot; + str(np.unique(labels))) . Customer types: [0 1 2 3 4] . All customers will be segmented into 5 types . cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns=[df_ohe.columns]) cluster_centers . Education_2n Cycle Education_Basic Education_Graduation Education_Master Education_PhD Marital_Status_Divorced Marital_Status_Married Marital_Status_Single Marital_Status_Together Marital_Status_Widow index ID Income Kidhome Teenhome Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain age MntStandardProds days_enroll . 0 -0.125164 | -0.157532 | -0.105295 | 0.115541 | 0.169629 | 0.091102 | 0.014475 | -0.136944 | 0.027980 | 0.050682 | -0.015894 | -0.039013 | 0.234877 | -0.384562 | 0.792383 | -0.027776 | 0.423401 | -0.158147 | -0.177162 | -0.198746 | -0.164124 | 0.293372 | 0.790358 | 0.800825 | 0.107058 | 0.584151 | 0.214830 | -0.045327 | 0.241122 | -0.257661 | -0.140231 | -0.025564 | -0.020825 | 0.413422 | 0.136408 | 0.268280 | . 1 0.083522 | -0.157532 | 0.029729 | 0.012352 | -0.046577 | -0.015783 | 0.016664 | 0.032650 | -0.012845 | -0.060835 | -0.006417 | 0.006318 | -0.750074 | 0.665042 | -0.082360 | 0.005650 | -0.774752 | -0.538245 | -0.631562 | -0.556909 | -0.535989 | -0.553235 | -0.136325 | -0.702075 | -0.718243 | -0.787791 | 0.455827 | -0.006958 | -0.257435 | -0.278019 | -0.257605 | -0.099096 | 0.034487 | -0.228177 | -0.827974 | -0.192561 | . 2 -0.066796 | -0.157532 | 0.067957 | -0.015359 | 0.036650 | -0.068085 | 0.092303 | -0.078441 | 0.002652 | 0.037741 | 0.052089 | -0.026993 | 1.381510 | -0.727924 | -0.681386 | 0.018123 | 1.714011 | 0.763102 | 1.324668 | 0.744922 | 0.954157 | 0.644825 | -0.649204 | 0.488710 | 1.137303 | 0.764522 | -0.942881 | 0.264772 | 1.266653 | 3.321733 | 1.573941 | 1.013200 | -0.032367 | -0.066658 | 1.711159 | -0.003692 | . 3 0.039064 | -0.157532 | 0.162844 | -0.113354 | -0.063865 | -0.021892 | -0.084549 | 0.096794 | -0.008412 | 0.064224 | -0.014911 | 0.052450 | 0.988433 | -0.724560 | -0.474252 | 0.018746 | 0.610089 | 1.124399 | 1.177724 | 1.221747 | 1.053793 | 0.635864 | -0.424929 | 0.423380 | 1.084269 | 0.792308 | -0.981145 | -0.043875 | -0.184097 | -0.278019 | 0.178809 | -0.116775 | -0.025040 | 0.085516 | 1.086121 | 0.036503 | . 4 -0.314744 | 6.347936 | -1.009009 | -0.445287 | -0.524418 | -0.279167 | -0.034133 | 0.286013 | 0.000152 | -0.087689 | 0.247107 | -0.058849 | -1.477151 | 0.342274 | -0.761265 | -0.023371 | -0.881553 | -0.381167 | -0.688930 | -0.374011 | -0.363304 | -0.405492 | -0.273882 | -0.789635 | -0.745879 | -0.904657 | 0.638829 | 0.146056 | -0.284517 | -0.278019 | -0.261762 | -0.116775 | -0.095130 | -0.732469 | -0.872284 | 0.369596 | . cluster = scaler.inverse_transform(cluster_centers) cluster_centers = pd.DataFrame(data = cluster, columns=[df_ohe.columns]) cluster_centers . Education_2n Cycle Education_Basic Education_Graduation Education_Master Education_PhD Marital_Status_Divorced Marital_Status_Married Marital_Status_Single Marital_Status_Together Marital_Status_Widow index ID Income Kidhome Teenhome Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain age MntStandardProds days_enroll . 0 5.429072e-02 | 1.040834e-17 | 4.518389e-01 | 2.084063e-01 | 2.854641e-01 | 0.131349 | 0.394046 | 0.159370 | 0.271454 | 0.043783 | 1107.998249 | 5460.753065 | 56956.915516 | 0.238179 | 0.938704 | 48.316988 | 446.082312 | 19.968476 | 126.837128 | 26.600701 | 20.341506 | 59.087566 | 3.852890 | 6.311734 | 2.970228 | 7.697023 | 5.842382 | 0.061296 | 1.383538e-01 | 5.253940e-03 | 0.029772 | 1.050788e-02 | 7.005254e-03 | 57.910683 | 639.830123 | 3208.721541 | . 1 1.140530e-01 | 2.428613e-17 | 5.193483e-01 | 1.700611e-01 | 1.965377e-01 | 0.098778 | 0.395112 | 0.229124 | 0.253564 | 0.023422 | 1114.110998 | 5607.804481 | 35871.335809 | 0.803462 | 0.462322 | 49.284114 | 43.156823 | 4.872709 | 24.379837 | 7.094705 | 4.975560 | 15.169043 | 2.062118 | 2.132383 | 0.562118 | 3.232179 | 6.426680 | 0.071283 | 7.128310e-03 | 8.326673e-17 | 0.001018 | 2.036660e-03 | 1.221996e-02 | 50.422607 | 84.479633 | 3115.558045 | . 2 7.100592e-02 | 0.000000e+00 | 5.384615e-01 | 1.597633e-01 | 2.307692e-01 | 0.082840 | 0.431953 | 0.183432 | 0.260355 | 0.041420 | 1151.846154 | 5499.745562 | 81503.742620 | 0.053254 | 0.136095 | 49.644970 | 880.100592 | 56.556213 | 465.467456 | 77.994083 | 66.550296 | 77.319527 | 1.071006 | 5.443787 | 5.976331 | 8.284024 | 3.035503 | 0.142012 | 4.082840e-01 | 9.289941e-01 | 0.449704 | 1.301775e-01 | 5.917160e-03 | 52.307692 | 1546.668639 | 3153.739645 | . 3 1.013216e-01 | -3.469447e-18 | 5.859031e-01 | 1.233480e-01 | 1.894273e-01 | 0.096916 | 0.345815 | 0.255507 | 0.255507 | 0.046256 | 1108.632159 | 5757.451542 | 73088.852435 | 0.055066 | 0.248899 | 49.662996 | 508.863436 | 70.905286 | 432.334802 | 103.962555 | 70.667401 | 76.854626 | 1.504405 | 5.262115 | 5.821586 | 8.374449 | 2.942731 | 0.061674 | 2.643172e-02 | -2.775558e-17 | 0.107930 | -1.214306e-17 | 6.607930e-03 | 54.083700 | 1186.733480 | 3161.865639 | . 4 1.387779e-17 | 1.000000e+00 | -1.110223e-16 | 2.775558e-17 | 5.551115e-17 | 0.018519 | 0.370370 | 0.333333 | 0.259259 | 0.018519 | 1277.629630 | 5396.407407 | 20306.259259 | 0.629630 | 0.092593 | 48.444444 | 7.240741 | 11.111111 | 11.444444 | 17.055556 | 12.111111 | 22.833333 | 1.796296 | 1.888889 | 0.481481 | 2.851852 | 6.870370 | 0.111111 | 1.387779e-17 | -1.387779e-17 | 0.000000 | 1.734723e-18 | 1.734723e-18 | 44.537037 | 58.962963 | 3229.203704 | . y_kmeans = kmeans.fit_predict(df_scaled) y_kmeans . array([4, 1, 4, ..., 2, 2, 1]) . df_cluster = pd.concat([df, pd.DataFrame({&quot;Segment&quot;: labels})], axis = 1) df_cluster.sample(10) . index ID Year_Birth Education Marital_Status Income Kidhome Teenhome Dt_Customer Recency MntWines MntFruits MntMeatProducts MntFishProducts MntSweetProducts MntGoldProds NumDealsPurchases NumWebPurchases NumCatalogPurchases NumStorePurchases NumWebVisitsMonth AcceptedCmp3 AcceptedCmp4 AcceptedCmp5 AcceptedCmp1 AcceptedCmp2 Complain Response age MntStandardProds days_enroll Segment . 1148 1152 | 10710 | 1979 | Graduation | Married | 7500.0 | 0 | 1 | 2012-08-29 | 61 | 5 | 2 | 3 | 3 | 0 | 5 | 1 | 1 | 0 | 2 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 13 | 3470 | 1 | . 1268 1272 | 195 | 1972 | Graduation | Single | 38808.0 | 1 | 0 | 2012-08-26 | 21 | 125 | 17 | 52 | 3 | 19 | 30 | 4 | 5 | 1 | 4 | 8 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 50 | 216 | 3473 | 1 | . 1636 1640 | 6866 | 1969 | Master | Together | 35924.0 | 1 | 1 | 2014-03-23 | 56 | 8 | 0 | 14 | 2 | 3 | 7 | 1 | 1 | 0 | 3 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 53 | 27 | 2899 | 1 | . 1947 1952 | 3434 | 1951 | Graduation | Single | 80872.0 | 0 | 0 | 2014-05-12 | 60 | 483 | 72 | 567 | 94 | 12 | 108 | 1 | 4 | 4 | 10 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 71 | 1228 | 2849 | 3 | . 94 94 | 2230 | 1970 | PhD | Married | 23626.0 | 1 | 0 | 2014-05-24 | 84 | 27 | 2 | 14 | 0 | 0 | 0 | 3 | 3 | 1 | 3 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 52 | 43 | 2837 | 1 | . 1799 1803 | 4168 | 1966 | Graduation | Single | 37070.0 | 1 | 1 | 2013-03-20 | 30 | 231 | 7 | 137 | 4 | 15 | 39 | 9 | 5 | 1 | 8 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 56 | 394 | 3267 | 0 | . 1135 1139 | 11166 | 1961 | Graduation | Married | 49678.0 | 0 | 1 | 2013-02-02 | 81 | 229 | 5 | 56 | 3 | 2 | 20 | 2 | 6 | 2 | 4 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 61 | 295 | 3313 | 0 | . 66 66 | 1386 | 1967 | Graduation | Together | 32474.0 | 1 | 1 | 2014-05-11 | 0 | 10 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 2 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 55 | 11 | 2850 | 1 | . 692 696 | 8315 | 1995 | Graduation | Single | 34824.0 | 0 | 0 | 2014-03-26 | 65 | 4 | 2 | 11 | 2 | 0 | 4 | 1 | 1 | 0 | 2 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 27 | 19 | 2896 | 1 | . 1742 1746 | 10250 | 1947 | Basic | Together | 28389.0 | 0 | 0 | 2012-11-20 | 49 | 1 | 5 | 3 | 7 | 4 | 8 | 1 | 1 | 0 | 2 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 75 | 20 | 3387 | 4 | . df_cluster_num = df_cluster.drop(columns=[&quot;Education&quot;, &quot;Marital_Status&quot;, &quot;Dt_Customer&quot;], axis=1) . This code will plot every categorical feature into segments and easy the analysis . for i in df_cluster_num.columns: plt.figure(figsize=(40, 5)) for j in range(5): plt.subplot(1, 5, j+1) cluster = df_cluster_num[df_cluster_num[&quot;Segment&quot;]==j] cluster[i].hist(bins=10) plt.axvline(cluster[i].mean(), color=&#39;k&#39;, linestyle=&#39;dashed&#39;, linewidth=1) #min_ylim, max_ylim = plt.ylim() #plt.text(cluster[i].mean()*1.1, max_ylim*0.9, &#39;Mean: {:.2f}&#39;.format(cluster[i].mean())) plt.title(&quot;{} nCluster {}&quot;.format(i, j)) plt.show() . plt.savefig(&quot;cluster.png&quot;) . df_cluster[&quot;Segment&quot;].value_counts() . 1 983 0 570 3 454 2 169 4 54 Name: Segment, dtype: int64 . df_cluster[&quot;Segment&quot;].hist(bins=5) . &lt;AxesSubplot:&gt; . plt.figure(figsize=[20,5]) sns.barplot(data=df_cluster, x=&quot;Segment&quot;, y=&quot;Education&quot;) . &lt;AxesSubplot:xlabel=&#39;Segment&#39;, ylabel=&#39;Education&#39;&gt; . plt.figure(figsize=[20,5]) sns.histplot(data=df_cluster, x=&quot;Segment&quot;, y=&quot;Marital_Status&quot;) . &lt;AxesSubplot:xlabel=&#39;Segment&#39;, ylabel=&#39;Marital_Status&#39;&gt; . Segment 0: . Those customers in segment 0 are Graduated and postgraduated, they are married or live as a couple, with an income between 50000 and 70000 and have teenagers at home. They have spent on average the last two years: 400 plus on wine, 20 in fruits, 150 in meat, 25 in fish, 20 in sweets, and more than 50 on Gold products. Segment 0 is the biggest deal buyer with an average of 4 deals purchased. They have a low level of response, an average age near to 60 years old, 600 amount spent in total and 3200 days enroll in the platform. . Segment 1: . People in segment 1 have an income of 40k, kids and teenagers at home, they are the cluster with more children, They have spent on average the last two years: 50 on wine, they are low meat, fish, gold and sweet products buyers, and the lowest fruit customers. They perform Few web and store purchases, their average age is 50, the second smallest buyers only surpassed by segment 4. Poor response. . Segment 2: . The segment with higher income, almost any children, the biggest wine and gold consumers, high meat purchase, and the second bigger fish buyers. lowest deal buyers, high web and store acquisitions. they have the best response, with an average age of 55. . Segment 3: . The second biggest budget, almost any kids, some teens, second biggest wine buyers, the best fruits and fish consumers, age 50 to 60. The second best response. . Segment 4: . Lowest income, kids at home, small purchases, bad response, ages 40 to 50, undergraduate. . Dimensionality reduction . We can see the distribution of the cluster with a PCA reduction. . pca = PCA(n_components=2) principal_comp = pca.fit_transform(df_scaled) principal_comp . array([[ 3.95570351, -0.63921269], [-2.51012731, -0.51509201], [ 1.75141287, -0.42771334], ..., [ 1.86393747, -0.02048655], [ 1.83012942, 1.21129792], [-1.97141246, 1.90721925]]) . pca_df = pd.DataFrame(data= principal_comp, columns=[&quot;ACP_1&quot;, &quot;ACP_2&quot;]) pca_df.head() . ACP_1 ACP_2 . 0 3.955704 | -0.639213 | . 1 -2.510127 | -0.515092 | . 2 1.751413 | -0.427713 | . 3 -2.713155 | -1.610820 | . 4 -0.474270 | 0.141759 | . pca_df = pd.concat([pca_df, pd.DataFrame({&quot;Segment&quot; : labels})], axis=1) pca_df.head() . ACP_1 ACP_2 Segment . 0 3.955704 | -0.639213 | 3 | . 1 -2.510127 | -0.515092 | 1 | . 2 1.751413 | -0.427713 | 3 | . 3 -2.713155 | -1.610820 | 1 | . 4 -0.474270 | 0.141759 | 1 | . cluster_fig = plt.figure(figsize=(10,10)) ax = sns.scatterplot(x=&quot;ACP_1&quot;, y=&quot;ACP_2&quot;, data= pca_df, hue=&quot;Segment&quot;, palette=[&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;purple&quot;, &quot;orange&quot; ]) . Classification model . Also we can train a classification model to predict which clients are more likely to buy the product. . plt.figure(figsize=(12, 8)) df_correlation = df_cluster.corr() target_correlation = df_correlation.loc[:, [&#39;Response&#39;]].sort_values(&#39;Response&#39;, ascending=False) sns.heatmap(target_correlation, annot=True, cmap=&quot;RdYlGn&quot;, vmin=-1, vmax=1) . &lt;AxesSubplot:&gt; . We need to prepare the data for the training . X = df_ohe y = df[&quot;Response&quot;] . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2230 entries, 0 to 2229 Data columns (total 32 columns): # Column Non-Null Count Dtype -- -- 0 Education_2n Cycle 2230 non-null float64 1 Education_Basic 2230 non-null float64 2 Education_Graduation 2230 non-null float64 3 Education_Master 2230 non-null float64 4 Education_PhD 2230 non-null float64 5 Marital_Status_Divorced 2230 non-null float64 6 Marital_Status_Married 2230 non-null float64 7 Marital_Status_Single 2230 non-null float64 8 Marital_Status_Together 2230 non-null float64 9 Marital_Status_Widow 2230 non-null float64 10 index 2230 non-null int64 11 Income 2230 non-null float64 12 MntWines 2230 non-null int64 13 MntFruits 2230 non-null int64 14 MntMeatProducts 2230 non-null int64 15 MntFishProducts 2230 non-null int64 16 MntSweetProducts 2230 non-null int64 17 MntGoldProds 2230 non-null int64 18 NumDealsPurchases 2230 non-null int64 19 NumWebPurchases 2230 non-null int64 20 NumCatalogPurchases 2230 non-null int64 21 NumStorePurchases 2230 non-null int64 22 NumWebVisitsMonth 2230 non-null int64 23 AcceptedCmp3 2230 non-null int64 24 AcceptedCmp4 2230 non-null int64 25 AcceptedCmp5 2230 non-null int64 26 AcceptedCmp1 2230 non-null int64 27 AcceptedCmp2 2230 non-null int64 28 Complain 2230 non-null int64 29 age 2230 non-null int64 30 MntStandardProds 2230 non-null int64 31 days_enroll 2230 non-null int64 dtypes: float64(11), int64(21) memory usage: 557.6 KB . y.info() . &lt;class &#39;pandas.core.series.Series&#39;&gt; RangeIndex: 2230 entries, 0 to 2229 Series name: Response Non-Null Count Dtype -- -- 2230 non-null int64 dtypes: int64(1) memory usage: 17.5 KB . X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.2, stratify=y) . rforest = RandomForestClassifier(random_state=42) . param_dist = { &#39;n_estimators&#39;: randint(100, 2000), &#39;max_depth&#39;: randint(10, 100), &#39;max_leaf_nodes&#39;: randint(3, 100), &#39;max_samples&#39;: randint(1, 100), &#39;min_samples_split&#39;: randint(4, 100) } . random_forest = RandomizedSearchCV(estimator = rforest, n_iter=100, param_distributions = param_dist, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=5, random_state=42, verbose= 1) . random_forest.fit(X_train, y_train) . Fitting 5 folds for each of 100 candidates, totalling 500 fits . RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_iter=100, n_jobs=-1, param_distributions={&#39;max_depth&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x0000025D21908FD0&gt;, &#39;max_leaf_nodes&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x0000025D26A4D580&gt;, &#39;max_samples&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x0000025D241E9A30&gt;, &#39;min_samples_split&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x0000025D26A4DA30&gt;, &#39;n_estimators&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x0000025D26A4D820&gt;}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . random_forest.best_params_ . {&#39;max_depth&#39;: 98, &#39;max_leaf_nodes&#39;: 64, &#39;max_samples&#39;: 97, &#39;min_samples_split&#39;: 4, &#39;n_estimators&#39;: 1500} . random_forest = RandomForestClassifier(random_state=42, max_depth= 98, max_leaf_nodes= 64, max_samples= 97, min_samples_split= 4, n_estimators= 1500) . Model optimized . random_forest.fit(X_train, y_train) . RandomForestClassifier(max_depth=98, max_leaf_nodes=64, max_samples=97, min_samples_split=4, n_estimators=1500, random_state=42) . forest_test = random_forest.predict(X_test) print(&quot;Accuracy: &quot; + str(accuracy_score(y_test, forest_test))) . Accuracy: 0.8654708520179372 . cm = confusion_matrix(y_test, forest_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . Accuracy: 0.8654708520179372 is a good margin for predictions . Conclusions . The next campain could be approach using the segments 2 and 3 and the 86% of the customer predicted with the random forest. .",
            "url": "https://pabloja4.github.io/portfolio/clasification/clustering/kmean/2022/03/19/Marketing-predictive-model.html",
            "relUrl": "/clasification/clustering/kmean/2022/03/19/Marketing-predictive-model.html",
            "date": " • Mar 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Forecasting de precios usando FB Prophet",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import datetime import prophet . Se cargan dos data-sets . sales_train_df = pd.read_csv(r&quot;C: Users pablo Documents Data_Science ventas train.csv&quot;, low_memory=False) . sales_train_df.head() . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | 0 | 1 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | 0 | 1 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | 0 | 1 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | 0 | 1 | . sales_train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1017209 entries, 0 to 1017208 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Store 1017209 non-null int64 1 DayOfWeek 1017209 non-null int64 2 Date 1017209 non-null object 3 Sales 1017209 non-null int64 4 Customers 1017209 non-null int64 5 Open 1017209 non-null int64 6 Promo 1017209 non-null int64 7 StateHoliday 1017209 non-null object 8 SchoolHoliday 1017209 non-null int64 dtypes: int64(7), object(2) memory usage: 69.8+ MB . sales_train_df.describe() . Store DayOfWeek Sales Customers Open Promo SchoolHoliday . count 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | 1.017209e+06 | . mean 5.584297e+02 | 3.998341e+00 | 5.773819e+03 | 6.331459e+02 | 8.301067e-01 | 3.815145e-01 | 1.786467e-01 | . std 3.219087e+02 | 1.997391e+00 | 3.849926e+03 | 4.644117e+02 | 3.755392e-01 | 4.857586e-01 | 3.830564e-01 | . min 1.000000e+00 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.800000e+02 | 2.000000e+00 | 3.727000e+03 | 4.050000e+02 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 50% 5.580000e+02 | 4.000000e+00 | 5.744000e+03 | 6.090000e+02 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 75% 8.380000e+02 | 6.000000e+00 | 7.856000e+03 | 8.370000e+02 | 1.000000e+00 | 1.000000e+00 | 0.000000e+00 | . max 1.115000e+03 | 7.000000e+00 | 4.155100e+04 | 7.388000e+03 | 1.000000e+00 | 1.000000e+00 | 1.000000e+00 | . store_info_df = pd.read_csv(r&quot;C: Users pablo Documents Data_Science ventas store.csv&quot;, low_memory=False) . store_info_df.head() . Store StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 1 2 | a | a | 570.0 | 11.0 | 2007.0 | 1 | 13.0 | 2010.0 | Jan,Apr,Jul,Oct | . 2 3 | a | a | 14130.0 | 12.0 | 2006.0 | 1 | 14.0 | 2011.0 | Jan,Apr,Jul,Oct | . 3 4 | c | c | 620.0 | 9.0 | 2009.0 | 0 | NaN | NaN | NaN | . 4 5 | a | a | 29910.0 | 4.0 | 2015.0 | 0 | NaN | NaN | NaN | . store_info_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1115 entries, 0 to 1114 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 Store 1115 non-null int64 1 StoreType 1115 non-null object 2 Assortment 1115 non-null object 3 CompetitionDistance 1112 non-null float64 4 CompetitionOpenSinceMonth 761 non-null float64 5 CompetitionOpenSinceYear 761 non-null float64 6 Promo2 1115 non-null int64 7 Promo2SinceWeek 571 non-null float64 8 Promo2SinceYear 571 non-null float64 9 PromoInterval 571 non-null object dtypes: float64(5), int64(2), object(3) memory usage: 87.2+ KB . store_info_df.describe() . Store CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear . count 1115.00000 | 1112.000000 | 761.000000 | 761.000000 | 1115.000000 | 571.000000 | 571.000000 | . mean 558.00000 | 5404.901079 | 7.224704 | 2008.668857 | 0.512108 | 23.595447 | 2011.763573 | . std 322.01708 | 7663.174720 | 3.212348 | 6.195983 | 0.500078 | 14.141984 | 1.674935 | . min 1.00000 | 20.000000 | 1.000000 | 1900.000000 | 0.000000 | 1.000000 | 2009.000000 | . 25% 279.50000 | 717.500000 | 4.000000 | 2006.000000 | 0.000000 | 13.000000 | 2011.000000 | . 50% 558.00000 | 2325.000000 | 8.000000 | 2010.000000 | 1.000000 | 22.000000 | 2012.000000 | . 75% 836.50000 | 6882.500000 | 10.000000 | 2013.000000 | 1.000000 | 37.000000 | 2013.000000 | . max 1115.00000 | 75860.000000 | 12.000000 | 2015.000000 | 1.000000 | 50.000000 | 2015.000000 | . EDA . sales_train_df.isna().sum() . Store 0 DayOfWeek 0 Date 0 Sales 0 Customers 0 Open 0 Promo 0 StateHoliday 0 SchoolHoliday 0 dtype: int64 . store_info_df.isna().sum() . Store 0 StoreType 0 Assortment 0 CompetitionDistance 3 CompetitionOpenSinceMonth 354 CompetitionOpenSinceYear 354 Promo2 0 Promo2SinceWeek 544 Promo2SinceYear 544 PromoInterval 544 dtype: int64 . Algunos nulos de los que hay que encargarse . sales_train_df.hist(bins = 30, figsize=(20,20), color= &quot;orange&quot;) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;Store&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;DayOfWeek&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Sales&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Customers&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Open&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;SchoolHoliday&#39;}&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . sales_open= sales_train_df[sales_train_df[&quot;Open&quot;]== 1] sales_closed = sales_train_df[sales_train_df[&quot;Open&quot;]== 0] . print(&quot;Porcentaje de tiendas cerradas = {}%&quot;.format(100.0*len(sales_closed)/len(sales_train_df))) . Porcentaje de tiendas cerradas = 16.98933060954042% . sales_train_df = sales_train_df[sales_train_df[&quot;Open&quot;]==1] . sales_train_df.drop(columns=&quot;Open&quot;, axis=1, inplace=True) . sales_train_df.describe() . Store DayOfWeek Sales Customers Promo SchoolHoliday . count 844392.000000 | 844392.000000 | 844392.000000 | 844392.000000 | 844392.000000 | 844392.000000 | . mean 558.422920 | 3.520361 | 6955.514291 | 762.728395 | 0.446352 | 0.193580 | . std 321.731914 | 1.723689 | 3104.214680 | 401.227674 | 0.497114 | 0.395103 | . min 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 280.000000 | 2.000000 | 4859.000000 | 519.000000 | 0.000000 | 0.000000 | . 50% 558.000000 | 3.000000 | 6369.000000 | 676.000000 | 0.000000 | 0.000000 | . 75% 837.000000 | 5.000000 | 8360.000000 | 893.000000 | 1.000000 | 0.000000 | . max 1115.000000 | 7.000000 | 41551.000000 | 7388.000000 | 1.000000 | 1.000000 | . store_info_df.isna().sum() . Store 0 StoreType 0 Assortment 0 CompetitionDistance 3 CompetitionOpenSinceMonth 354 CompetitionOpenSinceYear 354 Promo2 0 Promo2SinceWeek 544 Promo2SinceYear 544 PromoInterval 544 dtype: int64 . store_info_df.head() . Store StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 1 2 | a | a | 570.0 | 11.0 | 2007.0 | 1 | 13.0 | 2010.0 | Jan,Apr,Jul,Oct | . 2 3 | a | a | 14130.0 | 12.0 | 2006.0 | 1 | 14.0 | 2011.0 | Jan,Apr,Jul,Oct | . 3 4 | c | c | 620.0 | 9.0 | 2009.0 | 0 | NaN | NaN | NaN | . 4 5 | a | a | 29910.0 | 4.0 | 2015.0 | 0 | NaN | NaN | NaN | . Completamos los valores nan . replacing_cols = [&quot;Promo2SinceWeek&quot;, &quot;Promo2SinceYear&quot;, &quot;PromoInterval&quot;, &quot;CompetitionOpenSinceMonth&quot;, &quot;CompetitionOpenSinceYear&quot;] for col in replacing_cols: store_info_df[col].fillna(0, inplace=True) . store_info_df[&quot;CompetitionDistance&quot;].fillna(store_info_df[&quot;CompetitionDistance&quot;].mean(), inplace=True) . store_info_df.isnull().sum() . Store 0 StoreType 0 Assortment 0 CompetitionDistance 0 CompetitionOpenSinceMonth 0 CompetitionOpenSinceYear 0 Promo2 0 Promo2SinceWeek 0 Promo2SinceYear 0 PromoInterval 0 dtype: int64 . store_info_df.hist(bins= 30, figsize=(20,20), color = &quot;purple&quot;) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;Store&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;CompetitionDistance&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;CompetitionOpenSinceMonth&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;CompetitionOpenSinceYear&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo2&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo2SinceWeek&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Promo2SinceYear&#39;}&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . sales_store_df = pd.merge(sales_train_df, store_info_df, how = &quot;inner&quot;, on= &quot;Store&quot;) . sales_store_df.to_csv(&quot;sales_store.csv&quot;, index=False) . sales_store_df.head() . Store DayOfWeek Date Sales Customers Promo StateHoliday SchoolHoliday StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 1 1 | 4 | 2015-07-30 | 5020 | 546 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 2 1 | 3 | 2015-07-29 | 4782 | 523 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 3 1 | 2 | 2015-07-28 | 5011 | 560 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . 4 1 | 1 | 2015-07-27 | 6102 | 612 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | . f, ax = plt.subplots(figsize= (20,20)) df_corr = sales_store_df.corr().loc[:, [&#39;Sales&#39;]].sort_values(&#39;Sales&#39;, ascending=False) sns.heatmap(df_corr, annot=True, cmap=&quot;RdYlGn&quot;, vmin=-1, vmax=1) . &lt;AxesSubplot:&gt; . Preparamos las fechas para el an&#225;lisis . sales_store_df[&quot;Year&quot;] = pd.DatetimeIndex(sales_train_df[&quot;Date&quot;]).year sales_store_df[&quot;Month&quot;] = pd.DatetimeIndex(sales_train_df[&quot;Date&quot;]).month sales_store_df[&quot;Day&quot;] = pd.DatetimeIndex(sales_train_df[&quot;Date&quot;]).day . sales_month = sales_store_df.groupby(&quot;Month&quot;)[&quot;Sales&quot;].mean().plot(figsize = (10, 5), marker=&quot;x&quot;, color=&quot;b&quot;) sales_month.set_title(&quot;Ventas promedio al mes&quot;) plt.figure() sales_month = sales_store_df.groupby(&quot;Month&quot;)[&quot;Customers&quot;].mean().plot(figsize = (10, 5), marker=&quot;o&quot;, color=&quot;g&quot;) sales_month.set_title(&quot;Clientes promedio al mes&quot;) . Text(0.5, 1.0, &#39;Clientes promedio al mes&#39;) . sales_day = sales_store_df.groupby(&quot;Day&quot;)[&quot;Sales&quot;].mean().plot(figsize = (10, 5), marker=&quot;x&quot;, color=&quot;b&quot;) sales_day.set_title(&quot;Ventas promedio al día&quot;) plt.figure() sales_day = sales_store_df.groupby(&quot;Day&quot;)[&quot;Customers&quot;].mean().plot(figsize = (10, 5), marker=&quot;o&quot;, color=&quot;g&quot;) sales_day.set_title(&quot;Clientes promedio al día&quot;) . Text(0.5, 1.0, &#39;Clientes promedio al día&#39;) . sales_day_week = sales_store_df.groupby(&quot;DayOfWeek&quot;)[&quot;Sales&quot;].mean().plot(figsize = (10, 5), marker=&quot;x&quot;, color=&quot;b&quot;) sales_day_week.set_title(&quot;Ventas promedio al día&quot;) plt.figure() sales_day_week = sales_store_df.groupby(&quot;DayOfWeek&quot;)[&quot;Customers&quot;].mean().plot(figsize = (10, 5), marker=&quot;o&quot;, color=&quot;g&quot;) sales_day_week.set_title(&quot;Clientes promedio al día&quot;) . Text(0.5, 1.0, &#39;Clientes promedio al día&#39;) . Las ventas y los clientes están correctamente correlacionados | Los domingos se disparan las ventas | Hay estacionalidad en las ventas | . sales_store_df.head() . Store DayOfWeek Date Sales Customers Promo StateHoliday SchoolHoliday StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval Year Month Day . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 1 1 | 4 | 2015-07-30 | 5020 | 546 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 2 1 | 3 | 2015-07-29 | 4782 | 523 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 3 1 | 2 | 2015-07-28 | 5011 | 560 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . 4 1 | 1 | 2015-07-27 | 6102 | 612 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | 0.0 | 0.0 | 0 | 2015 | 7 | 31 | . fig, ax = plt.subplots(figsize=(20,10)) sales_stores = sales_store_df.groupby([&quot;Date&quot;, &quot;StoreType&quot;]).mean()[&quot;Sales&quot;].unstack().plot(ax=ax) . La tienda B es la que más factura. | La tienda A tiene el peor desempeño. | . Desplegamos un modelo de Prophet . def sales_predictions(Store_ID, data_frame, periods): data_frame = data_frame[data_frame[&quot;Store&quot;] == Store_ID] data_frame = data_frame[[&quot;Date&quot;, &quot;Sales&quot;]].rename(columns= {&quot;Date&quot;: &quot;ds&quot;, &quot;Sales&quot;:&quot;y&quot;}) data_frame = data_frame.sort_values(&quot;ds&quot;) m = prophet.Prophet(interval_width = 0.95, daily_seasonality=True) m.fit(data_frame) future = m.make_future_dataframe(periods = periods) forecast = m.predict(future) fig = m.plot(forecast, xlabel = &quot;Fecha&quot;, ylabel = &quot;Ventas&quot;) fig2 = m.plot_components(forecast) . sales_predictions(10, sales_store_df, 60) . def sales_predictions(Store_ID, data_frame, periods, holidays): data_frame = data_frame[data_frame[&quot;Store&quot;] == Store_ID] data_frame = data_frame[[&quot;Date&quot;, &quot;Sales&quot;]].rename(columns= {&quot;Date&quot;: &quot;ds&quot;, &quot;Sales&quot;:&quot;y&quot;}) data_frame = data_frame.sort_values(&quot;ds&quot;) m = prophet.Prophet(interval_width = 0.95, daily_seasonality=True, holidays=holidays) m.fit(data_frame) future = m.make_future_dataframe(periods = periods) forecast = m.predict(future) fig = m.plot(forecast, xlabel = &quot;Fecha&quot;, ylabel = &quot;Ventas&quot;) fig2 = m.plot_components(forecast) . school_holidays = sales_store_df[sales_store_df[&quot;SchoolHoliday&quot;] == 1].loc[:, &quot;Date&quot;].values school_holidays = np.unique(school_holidays) school_holidays = pd.DataFrame({&quot;ds&quot;:pd.to_datetime(school_holidays), &quot;holiday&quot;: &quot;school_holiday&quot;}) school_holidays . ds holiday . 0 2013-01-01 | school_holiday | . 1 2013-01-02 | school_holiday | . 2 2013-01-03 | school_holiday | . 3 2013-01-04 | school_holiday | . 4 2013-01-05 | school_holiday | . ... ... | ... | . 472 2015-07-27 | school_holiday | . 473 2015-07-28 | school_holiday | . 474 2015-07-29 | school_holiday | . 475 2015-07-30 | school_holiday | . 476 2015-07-31 | school_holiday | . 477 rows × 2 columns . state_holidays = sales_store_df[(sales_store_df[&quot;StateHoliday&quot;] == &quot;a&quot;)| (sales_store_df[&quot;StateHoliday&quot;] == &quot;b&quot;)| (sales_store_df[&quot;StateHoliday&quot;] == &quot;c&quot;)].loc[:, &quot;Date&quot;].values state_holidays = np.unique(state_holidays) state_holidays = pd.DataFrame({&quot;ds&quot;:pd.to_datetime(state_holidays), &quot;holiday&quot;: &quot;state_holiday&quot;}) state_holidays . ds holiday . 0 2013-01-01 | state_holiday | . 1 2013-01-06 | state_holiday | . 2 2013-03-29 | state_holiday | . 3 2013-04-01 | state_holiday | . 4 2013-05-01 | state_holiday | . 5 2013-05-09 | state_holiday | . 6 2013-05-20 | state_holiday | . 7 2013-05-30 | state_holiday | . 8 2013-08-15 | state_holiday | . 9 2013-10-03 | state_holiday | . 10 2013-10-31 | state_holiday | . 11 2013-11-01 | state_holiday | . 12 2013-12-25 | state_holiday | . 13 2013-12-26 | state_holiday | . 14 2014-01-01 | state_holiday | . 15 2014-01-06 | state_holiday | . 16 2014-04-18 | state_holiday | . 17 2014-04-21 | state_holiday | . 18 2014-05-01 | state_holiday | . 19 2014-05-29 | state_holiday | . 20 2014-06-09 | state_holiday | . 21 2014-06-19 | state_holiday | . 22 2014-10-03 | state_holiday | . 23 2014-10-31 | state_holiday | . 24 2014-11-01 | state_holiday | . 25 2014-12-25 | state_holiday | . 26 2014-12-26 | state_holiday | . 27 2015-01-01 | state_holiday | . 28 2015-01-06 | state_holiday | . 29 2015-04-03 | state_holiday | . 30 2015-04-06 | state_holiday | . 31 2015-05-01 | state_holiday | . 32 2015-05-14 | state_holiday | . 33 2015-05-25 | state_holiday | . 34 2015-06-04 | state_holiday | . holidays_df = pd.concat((school_holidays, state_holidays), axis=0) . sales_predictions (6, sales_store_df, 90, holidays_df) .",
            "url": "https://pabloja4.github.io/portfolio/forecasting/price/2022/02/15/Forecasting-ventas.html",
            "relUrl": "/forecasting/price/2022/02/15/Forecasting-ventas.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Modelos de clasificación para Recursos Humanos",
            "content": "El área de recursos humanos quiere disminuir la cantidad de empleados que abandonan la empresa. Se requiere de un análisis de los factores que llevan a las renuncias y la creación de un modelo de clasificación que prediga que empleados puedan tender a dejar su puesto. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.metrics import confusion_matrix, classification_report from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import Normalizer import tensorflow as tf pd.set_option(&quot;max_columns&quot;, None) . df = pd.read_csv(r&quot; Users pablo Documents Data_Science Data_sets Human_Resources.csv&quot;) . df.head() . Age Attrition BusinessTravel DailyRate Department DistanceFromHome Education EducationField EmployeeCount EmployeeNumber EnvironmentSatisfaction Gender HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked Over18 OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction StandardHours StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager . 0 41 | Yes | Travel_Rarely | 1102 | Sales | 1 | 2 | Life Sciences | 1 | 1 | 2 | Female | 94 | 3 | 2 | Sales Executive | 4 | Single | 5993 | 19479 | 8 | Y | Yes | 11 | 3 | 1 | 80 | 0 | 8 | 0 | 1 | 6 | 4 | 0 | 5 | . 1 49 | No | Travel_Frequently | 279 | Research &amp; Development | 8 | 1 | Life Sciences | 1 | 2 | 3 | Male | 61 | 2 | 2 | Research Scientist | 2 | Married | 5130 | 24907 | 1 | Y | No | 23 | 4 | 4 | 80 | 1 | 10 | 3 | 3 | 10 | 7 | 1 | 7 | . 2 37 | Yes | Travel_Rarely | 1373 | Research &amp; Development | 2 | 2 | Other | 1 | 4 | 4 | Male | 92 | 2 | 1 | Laboratory Technician | 3 | Single | 2090 | 2396 | 6 | Y | Yes | 15 | 3 | 2 | 80 | 0 | 7 | 3 | 3 | 0 | 0 | 0 | 0 | . 3 33 | No | Travel_Frequently | 1392 | Research &amp; Development | 3 | 4 | Life Sciences | 1 | 5 | 4 | Female | 56 | 3 | 1 | Research Scientist | 3 | Married | 2909 | 23159 | 1 | Y | Yes | 11 | 3 | 3 | 80 | 0 | 8 | 3 | 3 | 8 | 7 | 3 | 0 | . 4 27 | No | Travel_Rarely | 591 | Research &amp; Development | 2 | 1 | Medical | 1 | 7 | 1 | Male | 40 | 3 | 1 | Laboratory Technician | 2 | Married | 3468 | 16632 | 9 | Y | No | 12 | 3 | 4 | 80 | 1 | 6 | 3 | 3 | 2 | 2 | 2 | 2 | . df.shape . (1470, 35) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1470 entries, 0 to 1469 Data columns (total 35 columns): # Column Non-Null Count Dtype -- -- 0 Age 1470 non-null int64 1 Attrition 1470 non-null object 2 BusinessTravel 1470 non-null object 3 DailyRate 1470 non-null int64 4 Department 1470 non-null object 5 DistanceFromHome 1470 non-null int64 6 Education 1470 non-null int64 7 EducationField 1470 non-null object 8 EmployeeCount 1470 non-null int64 9 EmployeeNumber 1470 non-null int64 10 EnvironmentSatisfaction 1470 non-null int64 11 Gender 1470 non-null object 12 HourlyRate 1470 non-null int64 13 JobInvolvement 1470 non-null int64 14 JobLevel 1470 non-null int64 15 JobRole 1470 non-null object 16 JobSatisfaction 1470 non-null int64 17 MaritalStatus 1470 non-null object 18 MonthlyIncome 1470 non-null int64 19 MonthlyRate 1470 non-null int64 20 NumCompaniesWorked 1470 non-null int64 21 Over18 1470 non-null object 22 OverTime 1470 non-null object 23 PercentSalaryHike 1470 non-null int64 24 PerformanceRating 1470 non-null int64 25 RelationshipSatisfaction 1470 non-null int64 26 StandardHours 1470 non-null int64 27 StockOptionLevel 1470 non-null int64 28 TotalWorkingYears 1470 non-null int64 29 TrainingTimesLastYear 1470 non-null int64 30 WorkLifeBalance 1470 non-null int64 31 YearsAtCompany 1470 non-null int64 32 YearsInCurrentRole 1470 non-null int64 33 YearsSinceLastPromotion 1470 non-null int64 34 YearsWithCurrManager 1470 non-null int64 dtypes: int64(26), object(9) memory usage: 402.1+ KB . df.nunique() . Age 43 Attrition 2 BusinessTravel 3 DailyRate 886 Department 3 DistanceFromHome 29 Education 5 EducationField 6 EmployeeCount 1 EmployeeNumber 1470 EnvironmentSatisfaction 4 Gender 2 HourlyRate 71 JobInvolvement 4 JobLevel 5 JobRole 9 JobSatisfaction 4 MaritalStatus 3 MonthlyIncome 1349 MonthlyRate 1427 NumCompaniesWorked 10 Over18 1 OverTime 2 PercentSalaryHike 15 PerformanceRating 2 RelationshipSatisfaction 4 StandardHours 1 StockOptionLevel 4 TotalWorkingYears 40 TrainingTimesLastYear 7 WorkLifeBalance 4 YearsAtCompany 37 YearsInCurrentRole 19 YearsSinceLastPromotion 16 YearsWithCurrManager 18 dtype: int64 . EDA . La variable Attrition es nuestro target | No hay valores faltantes | Las variables Atrittion, Over18 y OverTime pueden convertirse en variable binarias | . df[&quot;Attrition&quot;] = df[&quot;Attrition&quot;].apply(lambda x: 1 if x == &quot;Yes&quot; else 0) df[&quot;OverTime&quot;] = df[&quot;OverTime&quot;].apply(lambda x: 1 if x == &quot;Yes&quot; else 0) df[&quot;Over18&quot;] = df[&quot;Over18&quot;].apply(lambda x: 1 if x == &quot;Y&quot; else 0) . df.hist(bins=30, figsize=(20,20), color=&quot;r&quot;) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;Age&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Attrition&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;DailyRate&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;DistanceFromHome&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;Education&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;EmployeeCount&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;EmployeeNumber&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;EnvironmentSatisfaction&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;HourlyRate&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;JobInvolvement&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;JobLevel&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;JobSatisfaction&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;MonthlyIncome&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;MonthlyRate&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;NumCompaniesWorked&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;Over18&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;OverTime&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;PercentSalaryHike&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;PerformanceRating&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;RelationshipSatisfaction&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;StandardHours&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;StockOptionLevel&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;TotalWorkingYears&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;TrainingTimesLastYear&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;WorkLifeBalance&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;YearsAtCompany&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;YearsInCurrentRole&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;YearsSinceLastPromotion&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;YearsWithCurrManager&#39;}&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . La mayoria de los empleados están en el rango de los 30 a 40 años. | Solo un 16% de los datos corresponden a empleados que han renunciado. | . df[&quot;Attrition&quot;].value_counts(normalize=True) . 0 0.838776 1 0.161224 Name: Attrition, dtype: float64 . Borrado de variables . Algunas variables no cambian o no generan información útil para el análisis y el modelo. . df.drop(columns=[&quot;Over18&quot;, &quot;StandardHours&quot;, &quot;EmployeeNumber&quot;, &quot;EmployeeCount&quot;], inplace=True) . Divisi&#243;n del dataset para el an&#225;lisis . left_df = df[df[&quot;Attrition&quot;] == 1] stayed_df = df[df[&quot;Attrition&quot;] == 0] . left_df.describe() . Age Attrition DailyRate DistanceFromHome Education EnvironmentSatisfaction HourlyRate JobInvolvement JobLevel JobSatisfaction MonthlyIncome MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager . count 237.000000 | 237.0 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | 237.000000 | . mean 33.607595 | 1.0 | 750.362869 | 10.632911 | 2.839662 | 2.464135 | 65.573840 | 2.518987 | 1.637131 | 2.468354 | 4787.092827 | 14559.308017 | 2.940928 | 0.535865 | 15.097046 | 3.156118 | 2.599156 | 0.527426 | 8.244726 | 2.624473 | 2.658228 | 5.130802 | 2.902954 | 1.945148 | 2.852321 | . std 9.689350 | 0.0 | 401.899519 | 8.452525 | 1.008244 | 1.169791 | 20.099958 | 0.773405 | 0.940594 | 1.118058 | 3640.210367 | 7208.153264 | 2.678519 | 0.499768 | 3.770294 | 0.363735 | 1.125437 | 0.856361 | 7.169204 | 1.254784 | 0.816453 | 5.949984 | 3.174827 | 3.153077 | 3.143349 | . min 18.000000 | 1.0 | 103.000000 | 1.000000 | 1.000000 | 1.000000 | 31.000000 | 1.000000 | 1.000000 | 1.000000 | 1009.000000 | 2326.000000 | 0.000000 | 0.000000 | 11.000000 | 3.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 28.000000 | 1.0 | 408.000000 | 3.000000 | 2.000000 | 1.000000 | 50.000000 | 2.000000 | 1.000000 | 1.000000 | 2373.000000 | 8870.000000 | 1.000000 | 0.000000 | 12.000000 | 3.000000 | 2.000000 | 0.000000 | 3.000000 | 2.000000 | 2.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 32.000000 | 1.0 | 699.000000 | 9.000000 | 3.000000 | 3.000000 | 66.000000 | 3.000000 | 1.000000 | 3.000000 | 3202.000000 | 14618.000000 | 1.000000 | 1.000000 | 14.000000 | 3.000000 | 3.000000 | 0.000000 | 7.000000 | 2.000000 | 3.000000 | 3.000000 | 2.000000 | 1.000000 | 2.000000 | . 75% 39.000000 | 1.0 | 1092.000000 | 17.000000 | 4.000000 | 4.000000 | 84.000000 | 3.000000 | 2.000000 | 3.000000 | 5916.000000 | 21081.000000 | 5.000000 | 1.000000 | 17.000000 | 3.000000 | 4.000000 | 1.000000 | 10.000000 | 3.000000 | 3.000000 | 7.000000 | 4.000000 | 2.000000 | 5.000000 | . max 58.000000 | 1.0 | 1496.000000 | 29.000000 | 5.000000 | 4.000000 | 100.000000 | 4.000000 | 5.000000 | 4.000000 | 19859.000000 | 26999.000000 | 9.000000 | 1.000000 | 25.000000 | 4.000000 | 4.000000 | 3.000000 | 40.000000 | 6.000000 | 4.000000 | 40.000000 | 15.000000 | 15.000000 | 14.000000 | . stayed_df.describe() . Age Attrition DailyRate DistanceFromHome Education EnvironmentSatisfaction HourlyRate JobInvolvement JobLevel JobSatisfaction MonthlyIncome MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager . count 1233.000000 | 1233.0 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.00000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | 1233.000000 | . mean 37.561233 | 0.0 | 812.504461 | 8.915653 | 2.927007 | 2.771290 | 65.952149 | 2.770479 | 2.145985 | 2.778589 | 6832.739659 | 14265.779400 | 2.64558 | 0.234388 | 15.231144 | 3.153285 | 2.733982 | 0.845093 | 11.862936 | 2.832928 | 2.781022 | 7.369019 | 4.484185 | 2.234388 | 4.367397 | . std 8.888360 | 0.0 | 403.208379 | 8.012633 | 1.027002 | 1.071132 | 20.380754 | 0.692050 | 1.117933 | 1.093277 | 4818.208001 | 7102.260749 | 2.46009 | 0.423787 | 3.639511 | 0.360408 | 1.071603 | 0.841985 | 7.760719 | 1.293585 | 0.681907 | 6.096298 | 3.649402 | 3.234762 | 3.594116 | . min 18.000000 | 0.0 | 102.000000 | 1.000000 | 1.000000 | 1.000000 | 30.000000 | 1.000000 | 1.000000 | 1.000000 | 1051.000000 | 2094.000000 | 0.00000 | 0.000000 | 11.000000 | 3.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 31.000000 | 0.0 | 477.000000 | 2.000000 | 2.000000 | 2.000000 | 48.000000 | 2.000000 | 1.000000 | 2.000000 | 3211.000000 | 7973.000000 | 1.00000 | 0.000000 | 12.000000 | 3.000000 | 2.000000 | 0.000000 | 6.000000 | 2.000000 | 2.000000 | 3.000000 | 2.000000 | 0.000000 | 2.000000 | . 50% 36.000000 | 0.0 | 817.000000 | 7.000000 | 3.000000 | 3.000000 | 66.000000 | 3.000000 | 2.000000 | 3.000000 | 5204.000000 | 14120.000000 | 2.00000 | 0.000000 | 14.000000 | 3.000000 | 3.000000 | 1.000000 | 10.000000 | 3.000000 | 3.000000 | 6.000000 | 3.000000 | 1.000000 | 3.000000 | . 75% 43.000000 | 0.0 | 1176.000000 | 13.000000 | 4.000000 | 4.000000 | 83.000000 | 3.000000 | 3.000000 | 4.000000 | 8834.000000 | 20364.000000 | 4.00000 | 0.000000 | 18.000000 | 3.000000 | 4.000000 | 1.000000 | 16.000000 | 3.000000 | 3.000000 | 10.000000 | 7.000000 | 3.000000 | 7.000000 | . max 60.000000 | 0.0 | 1499.000000 | 29.000000 | 5.000000 | 4.000000 | 100.000000 | 4.000000 | 5.000000 | 4.000000 | 19999.000000 | 26997.000000 | 9.00000 | 1.000000 | 25.000000 | 4.000000 | 4.000000 | 3.000000 | 38.000000 | 6.000000 | 4.000000 | 37.000000 | 18.000000 | 15.000000 | 17.000000 | . La edad promedio de los que se quedan es de 37.5 con respecto a los que se van que es 33.6. | En promedio los empleados que se van viven más lejos. | Los empleados que se van tienen un nivel en la empresa más bajo. | El sueldo mensual de los que se van es menor. | Los que se van hacen más horas extra. | Los que se van tienen menos acciones de la empresa. | Los que se van han trabajado menos años en la compañía. | . f, ax = plt.subplots(figsize= (20,20)) df_corr = df.corr().loc[:, [&#39;Attrition&#39;]].sort_values(&#39;Attrition&#39;, ascending=False) sns.heatmap(df_corr, annot=True, cmap=&quot;RdYlGn&quot;, vmin=-1, vmax=1) . &lt;AxesSubplot:&gt; . f, ax = plt.subplots(figsize= (20,20)) sns.heatmap(df.corr(), cmap=&quot;RdYlGn&quot;, annot=True) . &lt;AxesSubplot:&gt; . Comparamos los datos . Renuncia según la edad . plt.figure(figsize=[25,12]) sns.countplot(x=&quot;Age&quot;, hue = &quot;Attrition&quot;, data= df) plt.xlabel(&quot;Renuncia según la edad&quot;) . Text(0.5, 0, &#39;Renuncia según la edad&#39;) . plt.figure(figsize=[20,20]) plt.subplot(411) sns.countplot(x=&quot;JobRole&quot;, hue = &quot;Attrition&quot;, data= df) plt.subplot(412) sns.countplot(x=&quot;MaritalStatus&quot;, hue = &quot;Attrition&quot;, data= df) plt.subplot(413) sns.countplot(x=&quot;JobInvolvement&quot;, hue = &quot;Attrition&quot;, data= df) plt.subplot(414) sns.countplot(x=&quot;JobLevel&quot;, hue = &quot;Attrition&quot;, data= df) . &lt;AxesSubplot:xlabel=&#39;JobLevel&#39;, ylabel=&#39;count&#39;&gt; . Distancia de la casa al trabajo: . plt.figure(figsize=[12,7]) sns.kdeplot(left_df[&quot;DistanceFromHome&quot;], label = &quot;Empleados que se marchan&quot;, shade = True, color = &quot;r&quot;, cbar=True) sns.kdeplot(stayed_df[&quot;DistanceFromHome&quot;], label = &quot;Empleados que se quedan&quot;, shade = True, color = &quot;g&quot;, cbar=True) plt.xlabel(&quot;Distancia de casa al trabajo&quot;) . Text(0.5, 0, &#39;Distancia de casa al trabajo&#39;) . Años con el mismo manager: . plt.figure(figsize=[12,7]) sns.kdeplot(left_df[&quot;YearsWithCurrManager&quot;], label = [&quot;Empleados que se marchan&quot;], shade = True, color = &quot;r&quot;) sns.kdeplot(stayed_df[&quot;YearsWithCurrManager&quot;], label = &quot;Empleados que se quedan&quot;, shade = True, color = &quot;b&quot;) plt.xlabel(&quot;Años con el mismo manager&quot;) . Text(0.5, 0, &#39;Años con el mismo manager&#39;) . Años totales trabajados: . plt.figure(figsize=[12,7]) sns.kdeplot(left_df[&quot;TotalWorkingYears&quot;], label = &quot;Empleados que se marchan&quot;, shade = True, color = &quot;r&quot;) sns.kdeplot(stayed_df[&quot;TotalWorkingYears&quot;], label = &quot;Empleados que se quedan&quot;, shade = True, color = &quot;y&quot;) . &lt;AxesSubplot:xlabel=&#39;TotalWorkingYears&#39;, ylabel=&#39;Density&#39;&gt; . Relación del salario con el sexo de los empleados . plt.figure(figsize=[15,10]) sns.boxplot(data=df, x=&quot;MonthlyIncome&quot;, y=&quot;Gender&quot;) . &lt;AxesSubplot:xlabel=&#39;MonthlyIncome&#39;, ylabel=&#39;Gender&#39;&gt; . plt.figure(figsize=[15,10]) sns.boxplot(x = &quot;MonthlyIncome&quot;, y = &quot;JobRole&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;MonthlyIncome&#39;, ylabel=&#39;JobRole&#39;&gt; . val_cat = df[[&quot;BusinessTravel&quot;, &quot;Department&quot;, &quot;EducationField&quot;, &quot;Gender&quot;, &quot;JobRole&quot;, &quot;MaritalStatus&quot;]] . ohe = OneHotEncoder() val_cat = ohe.fit_transform(val_cat).toarray() val_cat = pd.DataFrame(val_cat) . val_cat.sample(2) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 . 950 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 352 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . numerical_cols = [cname for cname in df.columns if df[cname].dtype in [&#39;int64&#39;, &#39;float64&#39;]] numerical = df[numerical_cols] numerical = numerical.drop(columns=&quot;Attrition&quot;) . df_ohe = pd.concat([val_cat, numerical], axis=1) . df_ohe.sample(3) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Age DailyRate DistanceFromHome Education EnvironmentSatisfaction HourlyRate JobInvolvement JobLevel JobSatisfaction MonthlyIncome MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager . 249 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 45 | 1199 | 7 | 4 | 1 | 77 | 4 | 2 | 3 | 6434 | 5118 | 4 | 0 | 17 | 3 | 4 | 1 | 9 | 1 | 3 | 3 | 2 | 0 | 2 | . 1023 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 56 | 1255 | 1 | 2 | 1 | 90 | 3 | 1 | 1 | 2066 | 10494 | 2 | 0 | 22 | 4 | 4 | 1 | 5 | 3 | 4 | 3 | 2 | 1 | 0 | . 1190 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 32 | 977 | 2 | 3 | 4 | 45 | 3 | 2 | 2 | 5470 | 25518 | 0 | 0 | 13 | 3 | 3 | 2 | 10 | 4 | 2 | 9 | 5 | 1 | 6 | . scaler = MinMaxScaler() X = scaler.fit_transform(df_ohe.values) . X . array([[0. , 0. , 1. , ..., 0.22222222, 0. , 0.29411765], [0. , 1. , 0. , ..., 0.38888889, 0.06666667, 0.41176471], [0. , 0. , 1. , ..., 0. , 0. , 0. ], ..., [0. , 0. , 1. , ..., 0.11111111, 0. , 0.17647059], [0. , 1. , 0. , ..., 0.33333333, 0. , 0.47058824], [0. , 0. , 1. , ..., 0.16666667, 0.06666667, 0.11764706]]) . y = df[&quot;Attrition&quot;] . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) . Entrenamiento de modelos . Entrenamos tres modelos de clasificación y evaluamos su desempeño. . Logistic regression . modelo = LogisticRegression() . modelo.fit(X_train, y_train) . LogisticRegression() . predict = modelo.predict(X_test) . cm = confusion_matrix(y_test, predict) sns.heatmap(cm, annot=True) . &lt;AxesSubplot:&gt; . print(classification_report(y_test, predict)) . precision recall f1-score support 0 0.86 0.98 0.92 305 1 0.71 0.24 0.36 63 accuracy 0.85 368 macro avg 0.79 0.61 0.64 368 weighted avg 0.84 0.85 0.82 368 . Random Forest . rf = RandomForestClassifier() rf.fit(X_train, y_train) . RandomForestClassifier() . rf_predict = rf.predict(X_test) print(classification_report(y_test, rf_predict)) . precision recall f1-score support 0 0.85 0.99 0.91 305 1 0.75 0.14 0.24 63 accuracy 0.85 368 macro avg 0.80 0.57 0.58 368 weighted avg 0.83 0.85 0.80 368 . cm = confusion_matrix(y_test, rf_predict) sns.heatmap(cm, annot=True) . &lt;AxesSubplot:&gt; . NNA . modelo_nna = tf.keras.models.Sequential() modelo_nna.add(tf.keras.layers.Dense(units = 500, activation= &quot;relu&quot;, input_shape=(50, ))) modelo_nna.add(tf.keras.layers.Dense(units = 500, activation=&quot;relu&quot;)) modelo_nna.add(tf.keras.layers.Dense(units = 500, activation=&quot;relu&quot;)) modelo_nna.add(tf.keras.layers.Dense(units = 1, activation=&quot;sigmoid&quot;)) . modelo_nna.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 500) 25500 dense_1 (Dense) (None, 500) 250500 dense_2 (Dense) (None, 500) 250500 dense_3 (Dense) (None, 1) 501 ================================================================= Total params: 527,001 Trainable params: 527,001 Non-trainable params: 0 _________________________________________________________________ . modelo_nna.compile(optimizer=&quot;Adam&quot;, loss = &quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . epochs_hist = modelo_nna.fit(X_train, y_train, epochs=100, batch_size=50) . Epoch 1/100 23/23 [==============================] - 1s 5ms/step - loss: 0.4428 - accuracy: 0.8321 Epoch 2/100 23/23 [==============================] - 0s 5ms/step - loss: 0.3322 - accuracy: 0.8566 Epoch 3/100 23/23 [==============================] - 0s 5ms/step - loss: 0.3295 - accuracy: 0.8657 Epoch 4/100 23/23 [==============================] - 0s 5ms/step - loss: 0.2866 - accuracy: 0.8893 Epoch 5/100 23/23 [==============================] - 0s 5ms/step - loss: 0.2597 - accuracy: 0.9011 Epoch 6/100 23/23 [==============================] - 0s 5ms/step - loss: 0.2639 - accuracy: 0.8984 Epoch 7/100 23/23 [==============================] - 0s 5ms/step - loss: 0.2363 - accuracy: 0.9056 Epoch 8/100 23/23 [==============================] - 0s 5ms/step - loss: 0.1932 - accuracy: 0.9238 Epoch 9/100 23/23 [==============================] - 0s 4ms/step - loss: 0.1536 - accuracy: 0.9356 Epoch 10/100 23/23 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9555 Epoch 11/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9682 Epoch 12/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0980 - accuracy: 0.9637 Epoch 13/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9646 Epoch 14/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9728 Epoch 15/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0489 - accuracy: 0.9855 Epoch 16/100 23/23 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9918 Epoch 17/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 0.9946 Epoch 18/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 0.9936 Epoch 19/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 0.9982 Epoch 20/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 0.9991 Epoch 21/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000 Epoch 22/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9746 Epoch 23/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.9628 Epoch 24/100 23/23 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9646 Epoch 25/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0314 - accuracy: 0.9927 Epoch 26/100 23/23 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.9982 Epoch 27/100 23/23 [==============================] - 0s 6ms/step - loss: 0.0034 - accuracy: 1.0000 Epoch 28/100 23/23 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 Epoch 29/100 23/23 [==============================] - 0s 6ms/step - loss: 7.4154e-04 - accuracy: 1.0000 Epoch 30/100 23/23 [==============================] - 0s 5ms/step - loss: 5.2107e-04 - accuracy: 1.0000 Epoch 31/100 23/23 [==============================] - 0s 6ms/step - loss: 3.9695e-04 - accuracy: 1.0000 Epoch 32/100 23/23 [==============================] - 0s 6ms/step - loss: 3.3564e-04 - accuracy: 1.0000 Epoch 33/100 23/23 [==============================] - 0s 6ms/step - loss: 2.8039e-04 - accuracy: 1.0000 Epoch 34/100 23/23 [==============================] - 0s 6ms/step - loss: 2.3019e-04 - accuracy: 1.0000 Epoch 35/100 23/23 [==============================] - 0s 5ms/step - loss: 1.9405e-04 - accuracy: 1.0000 Epoch 36/100 23/23 [==============================] - 0s 6ms/step - loss: 1.6340e-04 - accuracy: 1.0000 Epoch 37/100 23/23 [==============================] - 0s 5ms/step - loss: 1.3346e-04 - accuracy: 1.0000 Epoch 38/100 23/23 [==============================] - 0s 6ms/step - loss: 1.1154e-04 - accuracy: 1.0000 Epoch 39/100 23/23 [==============================] - 0s 5ms/step - loss: 1.0519e-04 - accuracy: 1.0000 Epoch 40/100 23/23 [==============================] - 0s 5ms/step - loss: 8.1914e-05 - accuracy: 1.0000 Epoch 41/100 23/23 [==============================] - 0s 5ms/step - loss: 6.8305e-05 - accuracy: 1.0000 Epoch 42/100 23/23 [==============================] - 0s 5ms/step - loss: 5.6947e-05 - accuracy: 1.0000 Epoch 43/100 23/23 [==============================] - 0s 5ms/step - loss: 5.0913e-05 - accuracy: 1.0000 Epoch 44/100 23/23 [==============================] - 0s 5ms/step - loss: 4.3832e-05 - accuracy: 1.0000 Epoch 45/100 23/23 [==============================] - 0s 5ms/step - loss: 3.9068e-05 - accuracy: 1.0000 Epoch 46/100 23/23 [==============================] - 0s 5ms/step - loss: 3.4659e-05 - accuracy: 1.0000 Epoch 47/100 23/23 [==============================] - 0s 5ms/step - loss: 3.1020e-05 - accuracy: 1.0000 Epoch 48/100 23/23 [==============================] - 0s 5ms/step - loss: 2.7841e-05 - accuracy: 1.0000 Epoch 49/100 23/23 [==============================] - 0s 5ms/step - loss: 2.5350e-05 - accuracy: 1.0000 Epoch 50/100 23/23 [==============================] - 0s 5ms/step - loss: 2.3100e-05 - accuracy: 1.0000 Epoch 51/100 23/23 [==============================] - 0s 6ms/step - loss: 2.1331e-05 - accuracy: 1.0000 Epoch 52/100 23/23 [==============================] - 0s 6ms/step - loss: 1.9426e-05 - accuracy: 1.0000 Epoch 53/100 23/23 [==============================] - 0s 6ms/step - loss: 1.7662e-05 - accuracy: 1.0000 Epoch 54/100 23/23 [==============================] - 0s 5ms/step - loss: 1.6365e-05 - accuracy: 1.0000 Epoch 55/100 23/23 [==============================] - 0s 5ms/step - loss: 1.5136e-05 - accuracy: 1.0000 Epoch 56/100 23/23 [==============================] - 0s 4ms/step - loss: 1.3877e-05 - accuracy: 1.0000 Epoch 57/100 23/23 [==============================] - 0s 5ms/step - loss: 1.2903e-05 - accuracy: 1.0000 Epoch 58/100 23/23 [==============================] - 0s 4ms/step - loss: 1.1815e-05 - accuracy: 1.0000 Epoch 59/100 23/23 [==============================] - 0s 5ms/step - loss: 1.1047e-05 - accuracy: 1.0000 Epoch 60/100 23/23 [==============================] - 0s 5ms/step - loss: 1.0332e-05 - accuracy: 1.0000 Epoch 61/100 23/23 [==============================] - 0s 6ms/step - loss: 9.5431e-06 - accuracy: 1.0000 Epoch 62/100 23/23 [==============================] - 0s 5ms/step - loss: 9.0279e-06 - accuracy: 1.0000 Epoch 63/100 23/23 [==============================] - 0s 5ms/step - loss: 8.4768e-06 - accuracy: 1.0000 Epoch 64/100 23/23 [==============================] - 0s 5ms/step - loss: 7.9426e-06 - accuracy: 1.0000 Epoch 65/100 23/23 [==============================] - 0s 5ms/step - loss: 7.5058e-06 - accuracy: 1.0000 Epoch 66/100 23/23 [==============================] - 0s 4ms/step - loss: 7.1029e-06 - accuracy: 1.0000 Epoch 67/100 23/23 [==============================] - 0s 5ms/step - loss: 6.6971e-06 - accuracy: 1.0000 Epoch 68/100 23/23 [==============================] - 0s 5ms/step - loss: 6.3585e-06 - accuracy: 1.0000 Epoch 69/100 23/23 [==============================] - 0s 5ms/step - loss: 6.0275e-06 - accuracy: 1.0000 Epoch 70/100 23/23 [==============================] - 0s 5ms/step - loss: 5.7293e-06 - accuracy: 1.0000 Epoch 71/100 23/23 [==============================] - 0s 5ms/step - loss: 5.4434e-06 - accuracy: 1.0000 Epoch 72/100 23/23 [==============================] - 0s 5ms/step - loss: 5.1959e-06 - accuracy: 1.0000 Epoch 73/100 23/23 [==============================] - 0s 5ms/step - loss: 4.9394e-06 - accuracy: 1.0000 Epoch 74/100 23/23 [==============================] - 0s 4ms/step - loss: 4.7091e-06 - accuracy: 1.0000 Epoch 75/100 23/23 [==============================] - 0s 4ms/step - loss: 4.4987e-06 - accuracy: 1.0000 Epoch 76/100 23/23 [==============================] - 0s 4ms/step - loss: 4.2826e-06 - accuracy: 1.0000 Epoch 77/100 23/23 [==============================] - 0s 5ms/step - loss: 4.1002e-06 - accuracy: 1.0000 Epoch 78/100 23/23 [==============================] - 0s 4ms/step - loss: 3.9318e-06 - accuracy: 1.0000 Epoch 79/100 23/23 [==============================] - 0s 5ms/step - loss: 3.7528e-06 - accuracy: 1.0000 Epoch 80/100 23/23 [==============================] - 0s 5ms/step - loss: 3.5951e-06 - accuracy: 1.0000 Epoch 81/100 23/23 [==============================] - 0s 5ms/step - loss: 3.4542e-06 - accuracy: 1.0000 Epoch 82/100 23/23 [==============================] - 0s 5ms/step - loss: 3.3188e-06 - accuracy: 1.0000 Epoch 83/100 23/23 [==============================] - 0s 5ms/step - loss: 3.1574e-06 - accuracy: 1.0000 Epoch 84/100 23/23 [==============================] - 0s 5ms/step - loss: 3.0269e-06 - accuracy: 1.0000 Epoch 85/100 23/23 [==============================] - 0s 5ms/step - loss: 2.9037e-06 - accuracy: 1.0000 Epoch 86/100 23/23 [==============================] - 0s 5ms/step - loss: 2.7984e-06 - accuracy: 1.0000 Epoch 87/100 23/23 [==============================] - 0s 5ms/step - loss: 2.6960e-06 - accuracy: 1.0000 Epoch 88/100 23/23 [==============================] - 0s 6ms/step - loss: 2.5960e-06 - accuracy: 1.0000 Epoch 89/100 23/23 [==============================] - 0s 5ms/step - loss: 2.5008e-06 - accuracy: 1.0000 Epoch 90/100 23/23 [==============================] - 0s 6ms/step - loss: 2.4834e-06 - accuracy: 1.0000 Epoch 91/100 23/23 [==============================] - 0s 5ms/step - loss: 2.4359e-06 - accuracy: 1.0000 Epoch 92/100 23/23 [==============================] - 0s 5ms/step - loss: 2.2543e-06 - accuracy: 1.0000 Epoch 93/100 23/23 [==============================] - 0s 5ms/step - loss: 2.1368e-06 - accuracy: 1.0000 Epoch 94/100 23/23 [==============================] - 0s 4ms/step - loss: 2.0583e-06 - accuracy: 1.0000 Epoch 95/100 23/23 [==============================] - 0s 5ms/step - loss: 1.9902e-06 - accuracy: 1.0000 Epoch 96/100 23/23 [==============================] - 0s 5ms/step - loss: 1.9227e-06 - accuracy: 1.0000 Epoch 97/100 23/23 [==============================] - 0s 6ms/step - loss: 1.8660e-06 - accuracy: 1.0000 Epoch 98/100 23/23 [==============================] - 0s 5ms/step - loss: 1.7967e-06 - accuracy: 1.0000 Epoch 99/100 23/23 [==============================] - 0s 5ms/step - loss: 1.7144e-06 - accuracy: 1.0000 Epoch 100/100 23/23 [==============================] - 0s 5ms/step - loss: 1.6648e-06 - accuracy: 1.0000 . predict_nna = modelo_nna.predict(X_test) prediction_nna = (predict_nna &gt; 0.5) . plt.plot(epochs_hist.history[&quot;loss&quot;]) plt.title(&quot;Pérdida del modelo&quot;) . Text(0.5, 1.0, &#39;Pérdida del modelo&#39;) . cm = confusion_matrix(y_test, prediction_nna) sns.heatmap(cm, annot=True) . &lt;AxesSubplot:&gt; . nna_predict = modelo_nna.predict(X_test) print(classification_report(y_test, prediction_nna)) . precision recall f1-score support 0 0.91 0.94 0.92 316 1 0.52 0.42 0.47 52 accuracy 0.86 368 macro avg 0.72 0.68 0.70 368 weighted avg 0.85 0.86 0.86 368 .",
            "url": "https://pabloja4.github.io/portfolio/randomforest/onehotencoder/2021/12/12/Recursos-humanos.html",
            "relUrl": "/randomforest/onehotencoder/2021/12/12/Recursos-humanos.html",
            "date": " • Dec 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Cancelación de reservas de una empresa hotelera",
            "content": "La cancelación de reservas es un inconveniente para el gremio hotelero. Para sopesar las pérdidas es común la práctica de la sobre reserva, en la que una habitación se agenda a dos clientes contando con que uno de ellos cancele. Esto puede causar molestias a los visitantes del hotel si no se hace con cuidado, por lo que una solución basada en ML podría aumentar la cantidad de habitaciones ocupadas en épocas de muy alta demanda con el mínimo de inconvenientes. . import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns import scipy as sp from scipy.stats import randint import random mpl.style.use(&#39;seaborn&#39;) sns.set(rc={&quot;figure.figsize&quot;:(35, 20)}) from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV from lightgbm import LGBMClassifier from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.metrics import classification_report, accuracy_score from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline from sklearn.decomposition import PCA, TruncatedSVD from sklearn.metrics import roc_curve, auc from sklearn.feature_selection import VarianceThreshold from sklearn.metrics import confusion_matrix from sklearn.model_selection import RandomizedSearchCV from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer pd.set_option(&quot;max_columns&quot;, None) %matplotlib inline . df = pd.read_csv(r&quot;C: Users pablo Documents Data_Science Data_sets hotel_booking.csv&quot;) . df.head(3) . hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies meal country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled reserved_room_type assigned_room_type booking_changes deposit_type agent company days_in_waiting_list customer_type adr required_car_parking_spaces total_of_special_requests reservation_status reservation_status_date name email phone-number credit_card . 0 Resort Hotel | 0 | 342 | 2015 | July | 27 | 1 | 0 | 0 | 2 | 0.0 | 0 | BB | PRT | Direct | Direct | 0 | 0 | 0 | C | C | 3 | No Deposit | NaN | NaN | 0 | Transient | 0.0 | 0 | 0 | Check-Out | 2015-07-01 | Ernest Barnes | Ernest.Barnes31@outlook.com | 669-792-1661 | ************4322 | . 1 Resort Hotel | 0 | 737 | 2015 | July | 27 | 1 | 0 | 0 | 2 | 0.0 | 0 | BB | PRT | Direct | Direct | 0 | 0 | 0 | C | C | 4 | No Deposit | NaN | NaN | 0 | Transient | 0.0 | 0 | 0 | Check-Out | 2015-07-01 | Andrea Baker | Andrea_Baker94@aol.com | 858-637-6955 | ************9157 | . 2 Resort Hotel | 0 | 7 | 2015 | July | 27 | 1 | 0 | 1 | 1 | 0.0 | 0 | BB | GBR | Direct | Direct | 0 | 0 | 0 | A | C | 0 | No Deposit | NaN | NaN | 0 | Transient | 75.0 | 0 | 0 | Check-Out | 2015-07-02 | Rebecca Parker | Rebecca_Parker@comcast.net | 652-885-2745 | ************3734 | . EDA . El problema . Este data set muestra el historial de reservas de dos hoteles de Portugal, un hotel en la ciudad y otro a las afueras. La columna is_canceled muestra si la recerva se canceló. . df.shape . (119390, 36) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 119390 entries, 0 to 119389 Data columns (total 36 columns): # Column Non-Null Count Dtype -- -- 0 hotel 119390 non-null object 1 is_canceled 119390 non-null int64 2 lead_time 119390 non-null int64 3 arrival_date_year 119390 non-null int64 4 arrival_date_month 119390 non-null object 5 arrival_date_week_number 119390 non-null int64 6 arrival_date_day_of_month 119390 non-null int64 7 stays_in_weekend_nights 119390 non-null int64 8 stays_in_week_nights 119390 non-null int64 9 adults 119390 non-null int64 10 children 119386 non-null float64 11 babies 119390 non-null int64 12 meal 119390 non-null object 13 country 118902 non-null object 14 market_segment 119390 non-null object 15 distribution_channel 119390 non-null object 16 is_repeated_guest 119390 non-null int64 17 previous_cancellations 119390 non-null int64 18 previous_bookings_not_canceled 119390 non-null int64 19 reserved_room_type 119390 non-null object 20 assigned_room_type 119390 non-null object 21 booking_changes 119390 non-null int64 22 deposit_type 119390 non-null object 23 agent 103050 non-null float64 24 company 6797 non-null float64 25 days_in_waiting_list 119390 non-null int64 26 customer_type 119390 non-null object 27 adr 119390 non-null float64 28 required_car_parking_spaces 119390 non-null int64 29 total_of_special_requests 119390 non-null int64 30 reservation_status 119390 non-null object 31 reservation_status_date 119390 non-null object 32 name 119390 non-null object 33 email 119390 non-null object 34 phone-number 119390 non-null object 35 credit_card 119390 non-null object dtypes: float64(4), int64(16), object(16) memory usage: 32.8+ MB . En este dataset se encuentran datos personales de las personas que reservaron tales como nombre, teléfono, email y los últimos numeros de su tarjeta de crédito. Teniendo ya columnas que nos cuentan el rango de edad de las personas, podemos borrar la información personal. . Agrupaci&#243;n de los datos . Variables categ&#243;ricas . Tipo de hotel . df.hotel.unique() . array([&#39;Resort Hotel&#39;, &#39;City Hotel&#39;], dtype=object) . Hay dos tipos de hoteles, puede convertirse en 1 y 0 . sns.histplot(data=df, x= &quot;hotel&quot;, kde=False, bins=5) . &lt;AxesSubplot:xlabel=&#39;hotel&#39;, ylabel=&#39;Count&#39;&gt; . df.groupby(&quot;hotel&quot;)[&quot;is_canceled&quot;].mean()*100 . hotel City Hotel 41.726963 Resort Hotel 27.763355 Name: is_canceled, dtype: float64 . City hotel tiene un 42% de las entradas . Target . df.is_canceled.unique() . array([0, 1], dtype=int64) . Este es el target, esto nos permite desplegar modelos supervisados de clasificación . sns.histplot(data=df, x= &quot;is_canceled&quot;, kde=False, bins=5) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;Count&#39;&gt; . df.groupby(&quot;is_canceled&quot;).describe() . lead_time arrival_date_year arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies is_repeated_guest previous_cancellations previous_bookings_not_canceled booking_changes agent company days_in_waiting_list adr required_car_parking_spaces total_of_special_requests . count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max . is_canceled . 0 75166.0 | 79.984687 | 91.109888 | 0.0 | 9.0 | 45.0 | 124.0 | 737.0 | 75166.0 | 2016.147514 | 0.703124 | 2015.0 | 2016.0 | 2016.0 | 2017.0 | 2017.0 | 75166.0 | 27.080143 | 13.902478 | 1.0 | 16.0 | 28.0 | 38.0 | 53.0 | 75166.0 | 15.839529 | 8.776422 | 1.0 | 8.0 | 16.0 | 23.0 | 31.0 | 75166.0 | 0.928971 | 0.993371 | 0.0 | 0.0 | 1.0 | 2.0 | 19.0 | 75166.0 | 2.464053 | 1.924803 | 0.0 | 1.0 | 2.0 | 3.0 | 50.0 | 75166.0 | 1.829737 | 0.510451 | 0.0 | 2.0 | 2.0 | 2.0 | 4.0 | 75166.0 | 0.102347 | 0.390836 | 0.0 | 0.0 | 0.0 | 0.0 | 3.0 | 75166.0 | 0.010377 | 0.113007 | 0.0 | 0.0 | 0.0 | 0.0 | 10.0 | 75166.0 | 0.043344 | 0.203632 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 75166.0 | 0.015792 | 0.272421 | 0.0 | 0.0 | 0.0 | 0.0 | 13.0 | 75166.0 | 0.202977 | 1.810713 | 0.0 | 0.0 | 0.0 | 0.0 | 72.0 | 75166.0 | 0.293364 | 0.736266 | 0.0 | 0.0 | 0.0 | 0.0 | 21.0 | 62856.0 | 94.055794 | 113.947162 | 1.0 | 9.0 | 14.0 | 240.0 | 535.0 | 5606.0 | 190.519265 | 132.349286 | 6.0 | 51.0 | 183.0 | 270.0 | 541.0 | 75166.0 | 1.589868 | 14.784875 | 0.0 | 0.0 | 0.0 | 0.0 | 379.0 | 75166.0 | 99.987693 | 49.206263 | -6.38 | 67.500 | 92.5 | 125.00 | 510.0 | 75166.0 | 0.0993 | 0.303176 | 0.0 | 0.0 | 0.0 | 0.0 | 8.0 | 75166.0 | 0.714060 | 0.833887 | 0.0 | 0.0 | 1.0 | 1.0 | 5.0 | . 1 44224.0 | 144.848815 | 118.624829 | 0.0 | 48.0 | 113.0 | 214.0 | 629.0 | 44224.0 | 2016.171920 | 0.714557 | 2015.0 | 2016.0 | 2016.0 | 2017.0 | 2017.0 | 44224.0 | 27.309696 | 13.083155 | 1.0 | 17.0 | 27.0 | 38.0 | 53.0 | 44224.0 | 15.728066 | 8.787969 | 1.0 | 8.0 | 16.0 | 23.0 | 31.0 | 44224.0 | 0.925267 | 1.007468 | 0.0 | 0.0 | 1.0 | 2.0 | 16.0 | 44224.0 | 2.561912 | 1.878296 | 0.0 | 1.0 | 2.0 | 3.0 | 40.0 | 44224.0 | 1.901728 | 0.678038 | 0.0 | 2.0 | 2.0 | 2.0 | 55.0 | 44220.0 | 0.106513 | 0.411352 | 0.0 | 0.0 | 0.0 | 0.0 | 10.0 | 44224.0 | 0.003821 | 0.062429 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 44224.0 | 0.012482 | 0.111024 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 44224.0 | 0.208348 | 1.332346 | 0.0 | 0.0 | 0.0 | 0.0 | 26.0 | 44224.0 | 0.025122 | 0.678941 | 0.0 | 0.0 | 0.0 | 0.0 | 58.0 | 44224.0 | 0.098340 | 0.451008 | 0.0 | 0.0 | 0.0 | 0.0 | 16.0 | 40194.0 | 75.179927 | 104.589834 | 1.0 | 9.0 | 9.0 | 149.0 | 531.0 | 1191.0 | 183.371117 | 128.226814 | 9.0 | 67.0 | 169.0 | 270.0 | 543.0 | 44224.0 | 3.564083 | 21.488768 | 0.0 | 0.0 | 0.0 | 0.0 | 391.0 | 44224.0 | 104.964333 | 52.571142 | 0.00 | 72.415 | 96.2 | 127.62 | 5400.0 | 44224.0 | 0.0000 | 0.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 44224.0 | 0.328826 | 0.649234 | 0.0 | 0.0 | 0.0 | 0.0 | 5.0 | . Las reservas no canceladas son mayoría. . Temporalidad . df.arrival_date_year.unique() . array([2015, 2016, 2017], dtype=int64) . sns.barplot(data=df, x= &quot;arrival_date_year&quot;, y=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;arrival_date_year&#39;, ylabel=&#39;is_canceled&#39;&gt; . df.arrival_date_month.unique() . array([&#39;July&#39;, &#39;August&#39;, &#39;September&#39;, &#39;October&#39;, &#39;November&#39;, &#39;December&#39;, &#39;January&#39;, &#39;February&#39;, &#39;March&#39;, &#39;April&#39;, &#39;May&#39;, &#39;June&#39;], dtype=object) . plt.figure(figsize=[20,10]) arrival_date_graph = sns.barplot(data=df, x= &quot;arrival_date_month&quot;, y=&quot;is_canceled&quot;, hue=&quot;arrival_date_year&quot;) . plt.figure(figsize=[20,10]) sns.lineplot(data=df, x= &quot;arrival_date_month&quot;, y=&quot;is_canceled&quot;, hue=&quot;arrival_date_year&quot;) . &lt;AxesSubplot:xlabel=&#39;arrival_date_month&#39;, ylabel=&#39;is_canceled&#39;&gt; . Los datos están tomados desde julio del 2015 hasta agosto del 2017. Temporadas como el verano parecen tener un mayor . Tipo de comida . df.meal.unique() . array([&#39;BB&#39;, &#39;FB&#39;, &#39;HB&#39;, &#39;SC&#39;, &#39;Undefined&#39;], dtype=object) . No parece ser una variable muy relevante, vemos su correlación con el target. . df[[&quot;meal&quot;, &quot;is_canceled&quot;]].groupby(&quot;meal&quot;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . meal is_canceled . 1 FB | 0.598997 | . 0 BB | 0.373849 | . 3 SC | 0.372394 | . 2 HB | 0.344603 | . 4 Undefined | 0.244654 | . Solo uno de los valores tiene una correlación &gt;0.5 . plt.figure(figsize=[20,10]) sns.boxplot(data=df, x= &quot;meal&quot;, y=df[&quot;meal&quot;].index, hue=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;meal&#39;&gt; . Es una columna con muchos datos anómalos. . df.groupby(&quot;meal&quot;).mean() . is_canceled lead_time arrival_date_year arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies is_repeated_guest previous_cancellations previous_bookings_not_canceled booking_changes agent company days_in_waiting_list adr required_car_parking_spaces total_of_special_requests . meal . BB 0.373849 | 101.797010 | 2016.143961 | 27.164413 | 15.839097 | 0.891485 | 2.435955 | 1.844654 | 0.111954 | 0.007475 | 0.037623 | 0.078226 | 0.171087 | 0.208721 | 87.411964 | 187.552977 | 2.338122 | 99.407041 | 0.066472 | 0.565258 | . FB 0.598997 | 116.666667 | 2015.656642 | 30.713033 | 15.523810 | 1.140351 | 2.855890 | 1.958647 | 0.097744 | 0.030075 | 0.011278 | 1.567669 | 0.026316 | 0.302005 | 114.411326 | 167.823529 | 0.041353 | 109.040476 | 0.068922 | 0.233083 | . HB 0.344603 | 143.338865 | 2016.046118 | 28.570836 | 15.675309 | 1.210676 | 3.124525 | 1.932932 | 0.120791 | 0.012584 | 0.012860 | 0.120307 | 0.022056 | 0.319782 | 126.154098 | 137.307018 | 3.689345 | 120.307041 | 0.068312 | 0.531840 | . SC 0.372394 | 70.678779 | 2016.447512 | 25.408920 | 15.522723 | 0.815775 | 2.115962 | 1.850235 | 0.017559 | 0.003474 | 0.010329 | 0.010610 | 0.021033 | 0.172394 | 17.057674 | 242.101695 | 0.032394 | 98.295869 | 0.022629 | 0.746667 | . Undefined 0.244654 | 87.341317 | 2016.207870 | 23.412318 | 16.790419 | 1.150556 | 3.117194 | 1.823781 | 0.048760 | 0.013687 | 0.027374 | 0.065013 | 0.009410 | 0.368691 | 225.139591 | 441.824561 | 6.461078 | 91.948306 | 0.037639 | 0.176219 | . Pa&#237;ses . df.country.unique() . array([&#39;PRT&#39;, &#39;GBR&#39;, &#39;USA&#39;, &#39;ESP&#39;, &#39;IRL&#39;, &#39;FRA&#39;, nan, &#39;ROU&#39;, &#39;NOR&#39;, &#39;OMN&#39;, &#39;ARG&#39;, &#39;POL&#39;, &#39;DEU&#39;, &#39;BEL&#39;, &#39;CHE&#39;, &#39;CN&#39;, &#39;GRC&#39;, &#39;ITA&#39;, &#39;NLD&#39;, &#39;DNK&#39;, &#39;RUS&#39;, &#39;SWE&#39;, &#39;AUS&#39;, &#39;EST&#39;, &#39;CZE&#39;, &#39;BRA&#39;, &#39;FIN&#39;, &#39;MOZ&#39;, &#39;BWA&#39;, &#39;LUX&#39;, &#39;SVN&#39;, &#39;ALB&#39;, &#39;IND&#39;, &#39;CHN&#39;, &#39;MEX&#39;, &#39;MAR&#39;, &#39;UKR&#39;, &#39;SMR&#39;, &#39;LVA&#39;, &#39;PRI&#39;, &#39;SRB&#39;, &#39;CHL&#39;, &#39;AUT&#39;, &#39;BLR&#39;, &#39;LTU&#39;, &#39;TUR&#39;, &#39;ZAF&#39;, &#39;AGO&#39;, &#39;ISR&#39;, &#39;CYM&#39;, &#39;ZMB&#39;, &#39;CPV&#39;, &#39;ZWE&#39;, &#39;DZA&#39;, &#39;KOR&#39;, &#39;CRI&#39;, &#39;HUN&#39;, &#39;ARE&#39;, &#39;TUN&#39;, &#39;JAM&#39;, &#39;HRV&#39;, &#39;HKG&#39;, &#39;IRN&#39;, &#39;GEO&#39;, &#39;AND&#39;, &#39;GIB&#39;, &#39;URY&#39;, &#39;JEY&#39;, &#39;CAF&#39;, &#39;CYP&#39;, &#39;COL&#39;, &#39;GGY&#39;, &#39;KWT&#39;, &#39;NGA&#39;, &#39;MDV&#39;, &#39;VEN&#39;, &#39;SVK&#39;, &#39;FJI&#39;, &#39;KAZ&#39;, &#39;PAK&#39;, &#39;IDN&#39;, &#39;LBN&#39;, &#39;PHL&#39;, &#39;SEN&#39;, &#39;SYC&#39;, &#39;AZE&#39;, &#39;BHR&#39;, &#39;NZL&#39;, &#39;THA&#39;, &#39;DOM&#39;, &#39;MKD&#39;, &#39;MYS&#39;, &#39;ARM&#39;, &#39;JPN&#39;, &#39;LKA&#39;, &#39;CUB&#39;, &#39;CMR&#39;, &#39;BIH&#39;, &#39;MUS&#39;, &#39;COM&#39;, &#39;SUR&#39;, &#39;UGA&#39;, &#39;BGR&#39;, &#39;CIV&#39;, &#39;JOR&#39;, &#39;SYR&#39;, &#39;SGP&#39;, &#39;BDI&#39;, &#39;SAU&#39;, &#39;VNM&#39;, &#39;PLW&#39;, &#39;QAT&#39;, &#39;EGY&#39;, &#39;PER&#39;, &#39;MLT&#39;, &#39;MWI&#39;, &#39;ECU&#39;, &#39;MDG&#39;, &#39;ISL&#39;, &#39;UZB&#39;, &#39;NPL&#39;, &#39;BHS&#39;, &#39;MAC&#39;, &#39;TGO&#39;, &#39;TWN&#39;, &#39;DJI&#39;, &#39;STP&#39;, &#39;KNA&#39;, &#39;ETH&#39;, &#39;IRQ&#39;, &#39;HND&#39;, &#39;RWA&#39;, &#39;KHM&#39;, &#39;MCO&#39;, &#39;BGD&#39;, &#39;IMN&#39;, &#39;TJK&#39;, &#39;NIC&#39;, &#39;BEN&#39;, &#39;VGB&#39;, &#39;TZA&#39;, &#39;GAB&#39;, &#39;GHA&#39;, &#39;TMP&#39;, &#39;GLP&#39;, &#39;KEN&#39;, &#39;LIE&#39;, &#39;GNB&#39;, &#39;MNE&#39;, &#39;UMI&#39;, &#39;MYT&#39;, &#39;FRO&#39;, &#39;MMR&#39;, &#39;PAN&#39;, &#39;BFA&#39;, &#39;LBY&#39;, &#39;MLI&#39;, &#39;NAM&#39;, &#39;BOL&#39;, &#39;PRY&#39;, &#39;BRB&#39;, &#39;ABW&#39;, &#39;AIA&#39;, &#39;SLV&#39;, &#39;DMA&#39;, &#39;PYF&#39;, &#39;GUY&#39;, &#39;LCA&#39;, &#39;ATA&#39;, &#39;GTM&#39;, &#39;ASM&#39;, &#39;MRT&#39;, &#39;NCL&#39;, &#39;KIR&#39;, &#39;SDN&#39;, &#39;ATF&#39;, &#39;SLE&#39;, &#39;LAO&#39;], dtype=object) . df.country.nunique() . 177 . df.country.value_counts().reset_index() . index country . 0 PRT | 48590 | . 1 GBR | 12129 | . 2 FRA | 10415 | . 3 ESP | 8568 | . 4 DEU | 7287 | . ... ... | ... | . 172 DJI | 1 | . 173 BWA | 1 | . 174 HND | 1 | . 175 VGB | 1 | . 176 NAM | 1 | . 177 rows × 2 columns . Este dataset es de un grupo hotelero de Portugal, tiene sentido que haya tantos visitantes de este país y naciones aledañas. . Segmento de mercado . df.market_segment.unique() . array([&#39;Direct&#39;, &#39;Corporate&#39;, &#39;Online TA&#39;, &#39;Offline TA/TO&#39;, &#39;Complementary&#39;, &#39;Groups&#39;, &#39;Undefined&#39;, &#39;Aviation&#39;], dtype=object) . plt.figure(figsize=[20,10]) sns.boxplot(data=df, x=df[&quot;market_segment&quot;].index, y=df[&quot;market_segment&quot;]) . &lt;AxesSubplot:ylabel=&#39;market_segment&#39;&gt; . df.distribution_channel.unique() . array([&#39;Direct&#39;, &#39;Corporate&#39;, &#39;TA/TO&#39;, &#39;Undefined&#39;, &#39;GDS&#39;], dtype=object) . plt.figure(figsize=[20,10]) sns.boxplot(data=df, x=df[&quot;distribution_channel&quot;].index, y=df[&quot;distribution_channel&quot;]) . &lt;AxesSubplot:ylabel=&#39;distribution_channel&#39;&gt; . Aviación representa un segmento de mercado importante. . df.is_repeated_guest.unique() . array([0, 1], dtype=int64) . plt.figure(figsize=[20,10]) sns.lineplot(data=df, hue=&#39;is_repeated_guest&#39;, x=&quot;arrival_date_month&quot;, y=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;arrival_date_month&#39;, ylabel=&#39;is_canceled&#39;&gt; . df[[&#39;is_repeated_guest&#39;, &quot;is_canceled&quot;]].groupby(&#39;is_repeated_guest&#39;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . is_repeated_guest is_canceled . 0 0 | 0.377851 | . 1 1 | 0.144882 | . Los clientes que ya se han quedado en el hotel tienden a cancelar menos, salvo en verano. . Asignaci&#243;n de habitaciones . df.reserved_room_type.unique() . array([&#39;C&#39;, &#39;A&#39;, &#39;D&#39;, &#39;E&#39;, &#39;G&#39;, &#39;F&#39;, &#39;H&#39;, &#39;L&#39;, &#39;P&#39;, &#39;B&#39;], dtype=object) . df.assigned_room_type.unique() . array([&#39;C&#39;, &#39;A&#39;, &#39;D&#39;, &#39;E&#39;, &#39;G&#39;, &#39;F&#39;, &#39;I&#39;, &#39;B&#39;, &#39;H&#39;, &#39;P&#39;, &#39;L&#39;, &#39;K&#39;], dtype=object) . La habitación tipo &#39;I&#39; y &#39;K&#39; no estan en la columna de reservadas . Cancelaciones previas . df.deposit_type.unique() . array([&#39;No Deposit&#39;, &#39;Refundable&#39;, &#39;Non Refund&#39;], dtype=object) . plt.figure(figsize=[20,10]) sns.barplot(data=df, x=&#39;deposit_type&#39;, hue=&quot;arrival_date_year&quot;, y=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;deposit_type&#39;, ylabel=&#39;is_canceled&#39;&gt; . Agentes o compa&#241;&#237;as de viajes . df.agent.unique() . array([ nan, 304., 240., 303., 15., 241., 8., 250., 115., 5., 175., 134., 156., 243., 242., 3., 105., 40., 147., 306., 184., 96., 2., 127., 95., 146., 9., 177., 6., 143., 244., 149., 167., 300., 171., 305., 67., 196., 152., 142., 261., 104., 36., 26., 29., 258., 110., 71., 181., 88., 251., 275., 69., 248., 208., 256., 314., 126., 281., 273., 253., 185., 330., 334., 328., 326., 321., 324., 313., 38., 155., 68., 335., 308., 332., 94., 348., 310., 339., 375., 66., 327., 387., 298., 91., 245., 385., 257., 393., 168., 405., 249., 315., 75., 128., 307., 11., 436., 1., 201., 183., 223., 368., 336., 291., 464., 411., 481., 10., 154., 468., 410., 390., 440., 495., 492., 493., 434., 57., 531., 420., 483., 526., 472., 429., 16., 446., 34., 78., 139., 252., 270., 47., 114., 301., 193., 182., 135., 350., 195., 352., 355., 159., 363., 384., 360., 331., 367., 64., 406., 163., 414., 333., 427., 431., 430., 426., 438., 433., 418., 441., 282., 432., 72., 450., 180., 454., 455., 59., 451., 254., 358., 469., 165., 467., 510., 337., 476., 502., 527., 479., 508., 535., 302., 497., 187., 13., 7., 27., 14., 22., 17., 28., 42., 20., 19., 45., 37., 61., 39., 21., 24., 41., 50., 30., 54., 52., 12., 44., 31., 83., 32., 63., 60., 55., 56., 89., 87., 118., 86., 85., 210., 214., 129., 179., 138., 174., 170., 153., 93., 151., 119., 35., 173., 58., 53., 133., 79., 235., 192., 191., 236., 162., 215., 157., 287., 132., 234., 98., 77., 103., 107., 262., 220., 121., 205., 378., 23., 296., 290., 229., 33., 286., 276., 425., 484., 323., 403., 219., 394., 509., 111., 423., 4., 70., 82., 81., 74., 92., 99., 90., 112., 117., 106., 148., 158., 144., 211., 213., 216., 232., 150., 267., 227., 247., 278., 280., 285., 289., 269., 295., 265., 288., 122., 294., 325., 341., 344., 346., 359., 283., 364., 370., 371., 25., 141., 391., 397., 416., 404., 299., 197., 73., 354., 444., 408., 461., 388., 453., 459., 474., 475., 480., 449.]) . Estos son IDs . df.company.unique() . array([ nan, 110., 113., 270., 178., 240., 154., 144., 307., 268., 59., 204., 312., 318., 94., 174., 274., 195., 223., 317., 281., 118., 53., 286., 12., 47., 324., 342., 373., 371., 383., 86., 82., 218., 88., 31., 397., 392., 405., 331., 367., 20., 83., 416., 51., 395., 102., 34., 84., 360., 394., 457., 382., 461., 478., 386., 112., 486., 421., 9., 308., 135., 224., 504., 269., 356., 498., 390., 513., 203., 263., 477., 521., 169., 515., 445., 337., 251., 428., 292., 388., 130., 250., 355., 254., 543., 531., 528., 62., 120., 42., 81., 116., 530., 103., 39., 16., 92., 61., 501., 165., 291., 290., 43., 325., 192., 108., 200., 465., 287., 297., 490., 482., 207., 282., 437., 225., 329., 272., 28., 77., 338., 72., 246., 319., 146., 159., 380., 323., 511., 407., 278., 80., 403., 399., 14., 137., 343., 346., 347., 349., 289., 351., 353., 54., 99., 358., 361., 362., 366., 372., 365., 277., 109., 377., 379., 22., 378., 330., 364., 401., 232., 255., 384., 167., 212., 514., 391., 400., 376., 402., 396., 302., 398., 6., 370., 369., 409., 168., 104., 408., 413., 148., 10., 333., 419., 415., 424., 425., 423., 422., 435., 439., 442., 448., 443., 454., 444., 52., 459., 458., 456., 460., 447., 470., 466., 484., 184., 485., 32., 487., 491., 494., 193., 516., 496., 499., 29., 78., 520., 507., 506., 512., 126., 64., 242., 518., 523., 539., 534., 436., 525., 541., 40., 455., 410., 45., 38., 49., 48., 67., 68., 65., 91., 37., 8., 179., 209., 219., 221., 227., 153., 186., 253., 202., 216., 275., 233., 280., 309., 321., 93., 316., 85., 107., 350., 279., 334., 348., 150., 73., 385., 418., 197., 450., 452., 115., 46., 76., 96., 100., 105., 101., 122., 11., 139., 142., 127., 143., 140., 149., 163., 160., 180., 238., 183., 222., 185., 217., 215., 213., 237., 230., 234., 35., 245., 158., 258., 259., 260., 411., 257., 271., 18., 106., 210., 273., 71., 284., 301., 305., 293., 264., 311., 304., 313., 288., 320., 314., 332., 341., 352., 243., 368., 393., 132., 220., 412., 420., 426., 417., 429., 433., 446., 357., 479., 483., 489., 229., 481., 497., 451., 492.]) . tipo de cliente . df.customer_type.unique() . array([&#39;Transient&#39;, &#39;Contract&#39;, &#39;Transient-Party&#39;, &#39;Group&#39;], dtype=object) . df.reservation_status.unique() . array([&#39;Check-Out&#39;, &#39;Canceled&#39;, &#39;No-Show&#39;], dtype=object) . df[[&quot;reservation_status&quot;, &quot;is_canceled&quot;]].groupby(&quot;reservation_status&quot;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . reservation_status is_canceled . 0 Canceled | 1.0 | . 2 No-Show | 1.0 | . 1 Check-Out | 0.0 | . Esta variable puede estar muy correlacionada con el target . df.describe(include=[&#39;O&#39;]) . hotel arrival_date_month meal country market_segment distribution_channel reserved_room_type assigned_room_type deposit_type customer_type reservation_status reservation_status_date name email phone-number credit_card . count 119390 | 119390 | 119390 | 118902 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | 119390 | . unique 2 | 12 | 5 | 177 | 8 | 5 | 10 | 12 | 3 | 4 | 3 | 926 | 81503 | 115889 | 119390 | 9000 | . top City Hotel | August | BB | PRT | Online TA | TA/TO | A | A | No Deposit | Transient | Check-Out | 2015-10-21 | Michael Johnson | Michael.C@gmail.com | 669-792-1661 | ************4923 | . freq 79330 | 13877 | 92310 | 48590 | 56477 | 97870 | 85994 | 74053 | 104641 | 89613 | 75166 | 1461 | 48 | 6 | 1 | 28 | . Variables num&#233;ricas . Llegada y tiempo de la visita . df.lead_time.sample(10) . 62851 59 30587 0 15921 159 19127 11 3025 40 109693 4 63960 134 46124 125 4407 153 966 137 Name: lead_time, dtype: int64 . sns.FacetGrid(df, col=&#39;is_canceled&#39;).map(plt.hist, &#39;lead_time&#39;, bins=20) . &lt;seaborn.axisgrid.FacetGrid at 0x221bdf92520&gt; . Clientes frecuentres cancelan menos . df.arrival_date_week_number.unique() . array([27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], dtype=int64) . df.arrival_date_day_of_month.unique() . array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], dtype=int64) . N&#250;mero de visitantes . df.adults.unique() . array([ 2, 1, 3, 4, 40, 26, 50, 27, 55, 0, 20, 6, 5, 10], dtype=int64) . df.adults.describe() . count 119390.000000 mean 1.856403 std 0.579261 min 0.000000 25% 2.000000 50% 2.000000 75% 2.000000 max 55.000000 Name: adults, dtype: float64 . No puede haber reservaciones con cero adultos . df.children.unique() . array([ 0., 1., 2., 10., 3., nan]) . Debería ser un int . df.babies.unique() . array([ 0, 1, 2, 10, 9], dtype=int64) . Estos tres pueden ser una feature &quot;Guests&quot; . Cancelaciones previas . df.previous_cancellations.unique() . array([ 0, 1, 2, 3, 26, 25, 14, 4, 24, 19, 5, 21, 6, 13, 11], dtype=int64) . df.previous_cancellations.value_counts() . 0 112906 1 6051 2 116 3 65 24 48 11 35 4 31 26 26 25 25 6 22 19 19 5 19 14 14 13 12 21 1 Name: previous_cancellations, dtype: int64 . sns.barplot(data=df, x=df[&quot;is_canceled&quot;], y=df[&quot;previous_cancellations&quot;]) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;previous_cancellations&#39;&gt; . plt.figure(figsize=[20,10]) sns.barplot(data=df, hue=df[&quot;is_canceled&quot;], x=df[&quot;previous_cancellations&quot;], y=df[&quot;arrival_date_month&quot;]) . &lt;AxesSubplot:xlabel=&#39;previous_cancellations&#39;, ylabel=&#39;arrival_date_month&#39;&gt; . Septiembre y octubre son mesas con alta cancelación . df.previous_bookings_not_canceled.unique() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 19, 26, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72], dtype=int64) . sns.barplot(data=df, x=df[&quot;is_canceled&quot;], y=df[&quot;previous_bookings_not_canceled&quot;]) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;previous_bookings_not_canceled&#39;&gt; . (df[&quot;previous_bookings_not_canceled&quot;] &gt; 0).sum() / df.shape[0] . 0.030320797386715805 . df.booking_changes.unique() . array([ 3, 4, 0, 1, 2, 5, 17, 6, 8, 7, 10, 16, 9, 13, 12, 20, 14, 15, 11, 21, 18], dtype=int64) . N&#250;mero de noches . df.stays_in_weekend_nights.unique() . array([ 0, 1, 2, 4, 3, 6, 13, 8, 5, 7, 12, 9, 16, 18, 19, 10, 14], dtype=int64) . df.stays_in_week_nights.unique() . array([ 0, 1, 2, 3, 4, 5, 10, 11, 8, 6, 7, 15, 9, 12, 33, 20, 14, 16, 21, 13, 30, 19, 24, 40, 22, 42, 50, 25, 17, 32, 26, 18, 34, 35, 41], dtype=int64) . Estas dos podrían juntarse en una nueva columna . D&#237;as en lista de espera . df.days_in_waiting_list.unique() . array([ 0, 50, 47, 65, 122, 75, 101, 150, 125, 14, 60, 34, 100, 22, 121, 61, 39, 5, 1, 8, 107, 43, 52, 2, 11, 142, 116, 13, 44, 97, 83, 4, 113, 18, 20, 185, 93, 109, 6, 37, 105, 154, 64, 99, 38, 48, 33, 77, 21, 80, 59, 40, 58, 89, 53, 49, 69, 87, 91, 57, 111, 79, 98, 85, 63, 15, 3, 41, 224, 31, 56, 187, 176, 71, 55, 96, 236, 259, 207, 215, 160, 120, 30, 32, 27, 62, 24, 108, 147, 379, 70, 35, 178, 330, 223, 174, 162, 391, 68, 193, 10, 76, 16, 28, 9, 165, 17, 25, 46, 7, 84, 175, 183, 23, 117, 12, 54, 26, 73, 45, 19, 42, 72, 81, 92, 74, 167, 36], dtype=int64) . sns.barplot(data=df, y=&quot;days_in_waiting_list&quot;, x=&quot;is_canceled&quot;) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;days_in_waiting_list&#39;&gt; . A más tiempo de espera más cancelaciones . sns.boxplot(data=df, x=&quot;is_canceled&quot;, y=&quot;days_in_waiting_list&quot;) . &lt;AxesSubplot:xlabel=&#39;is_canceled&#39;, ylabel=&#39;days_in_waiting_list&#39;&gt; . Algunas reservas se hacen hasta un año antes. . Adr . df.adr.unique() . array([ 0. , 75. , 98. , ..., 266.75, 209.25, 157.71]) . df.adr.describe() . count 119390.000000 mean 101.831122 std 50.535790 min -6.380000 25% 69.290000 50% 94.575000 75% 126.000000 max 5400.000000 Name: adr, dtype: float64 . Hay valores negativos, el minimo debería ser cero. . df.required_car_parking_spaces.unique() . array([0, 1, 2, 8, 3], dtype=int64) . df.total_of_special_requests.unique() . array([0, 1, 3, 2, 4, 5], dtype=int64) . Fecha de cambio de status . df.reservation_status_date.sample(3) . 25678 2016-07-07 43985 2015-09-22 99249 2016-10-11 Name: reservation_status_date, dtype: object . Esto debería ser formato int no object . df[[&quot;reservation_status_date&quot;, &quot;is_canceled&quot;]].groupby(&quot;reservation_status_date&quot;, as_index=False).mean().sort_values(by=&quot;is_canceled&quot;, ascending=False) . reservation_status_date is_canceled . 0 2014-10-17 | 1.0 | . 91 2015-05-28 | 1.0 | . 89 2015-05-26 | 1.0 | . 88 2015-05-25 | 1.0 | . 87 2015-05-23 | 1.0 | . ... ... | ... | . 237 2015-10-25 | 0.0 | . 251 2015-11-08 | 0.0 | . 272 2015-11-29 | 0.0 | . 258 2015-11-15 | 0.0 | . 925 2017-09-14 | 0.0 | . 926 rows × 2 columns . Buscamos valores faltantes . df.isna().sum() . hotel 0 is_canceled 0 lead_time 0 arrival_date_year 0 arrival_date_month 0 arrival_date_week_number 0 arrival_date_day_of_month 0 stays_in_weekend_nights 0 stays_in_week_nights 0 adults 0 children 4 babies 0 meal 0 country 488 market_segment 0 distribution_channel 0 is_repeated_guest 0 previous_cancellations 0 previous_bookings_not_canceled 0 reserved_room_type 0 assigned_room_type 0 booking_changes 0 deposit_type 0 agent 16340 company 112593 days_in_waiting_list 0 customer_type 0 adr 0 required_car_parking_spaces 0 total_of_special_requests 0 reservation_status 0 reservation_status_date 0 name 0 email 0 phone-number 0 credit_card 0 dtype: int64 . Las variables Country, agent y company tienen nulos. . Correlaci&#243;n . Medimos la correlación entre las variables . f, ax = plt.subplots(figsize = (20, 20)) sns.heatmap(df.corr(), annot=True, cmap=&quot;RdYlGn&quot;) . &lt;AxesSubplot:&gt; . plt.figure(figsize=(12, 8)) df_correlation = df.corr() target_correlation = df_correlation.loc[:, [&#39;is_canceled&#39;]].sort_values(&#39;is_canceled&#39;, ascending=False) sns.heatmap(target_correlation, annot=True, cmap=&quot;RdYlGn&quot;, vmin=-1, vmax=1) . &lt;AxesSubplot:&gt; . Conclusiones del EDA . 42% de la información corresponde a City hotel. | Este dataset cuenta con 119390 filas y 36 columnas. | Hay dos tipos de hotel: Resort y City hotel, este último cuenta con más registros. | El target se encuentra en la variable “is_canceled” que muestra con uno las reservas canceladas y con cero las que no. | Hay visitantes de 177 países, los mayores visitantes vienen de Portugal, seguido por Reino Unido, Francia y España. De algunos países solo se hizo una reserva. | Parece que aquellos que se han hospedado en el hotel con anterioridad tienden a cancelar menos que los nuevos visitantes. | Septiembre y octubre son mesas con alta cancelación | . Manejo de valores faltantes, borrado de columnas y cambio de tipos . Se borran columnas seg&#250;n el EDA . Según el EDA y la correlación de las variables se decidieron eliminar las siguientes variables para evitar contaminar los resultados de los modelos, aligerarlos y simplificar. . df.drop(columns=[&quot;name&quot;, &quot;email&quot;, &quot;phone-number&quot;, &quot;credit_card&quot;, &quot;agent&quot;, &quot;meal&quot;, &quot;reservation_status&quot;, &quot;company&quot;, &quot;reserved_room_type&quot;, &quot;total_of_special_requests&quot;, &quot;booking_changes&quot;, &quot;required_car_parking_spaces&quot;, &quot;arrival_date_week_number&quot;], inplace=True) . df.sample(5) . hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled assigned_room_type deposit_type days_in_waiting_list customer_type adr reservation_status_date . 94145 City Hotel | 0 | 73 | 2016 | July | 29 | 1 | 2 | 3 | 1.0 | 0 | FRA | Direct | Direct | 0 | 0 | 0 | E | No Deposit | 0 | Transient | 173.7 | 2016-08-01 | . 100662 City Hotel | 0 | 36 | 2016 | October | 28 | 1 | 2 | 2 | 0.0 | 0 | ITA | Online TA | TA/TO | 0 | 0 | 0 | A | No Deposit | 0 | Transient | 109.0 | 2016-10-31 | . 83687 City Hotel | 0 | 59 | 2016 | February | 6 | 2 | 2 | 2 | 0.0 | 0 | AUT | Groups | TA/TO | 0 | 0 | 0 | D | No Deposit | 0 | Transient-Party | 62.0 | 2016-02-10 | . 12676 Resort Hotel | 1 | 23 | 2017 | July | 11 | 0 | 5 | 2 | 0.0 | 0 | ESP | Direct | Direct | 0 | 0 | 0 | E | No Deposit | 0 | Transient | 275.0 | 2017-06-20 | . 80597 City Hotel | 0 | 5 | 2016 | February | 3 | 0 | 1 | 1 | 0.0 | 0 | PRT | Corporate | Corporate | 1 | 0 | 4 | A | No Deposit | 0 | Transient | 66.0 | 2016-02-04 | . Manejo de variables categ&#243;ricas . Para aprovechar mejor estas variables las convertiremos en binarias. . df[&quot;previous_cancellations&quot;] = df[&quot;previous_cancellations&quot;].apply(lambda x: 0 if x == 0 else 1) df[&quot;previous_cancellations&quot;].value_counts() . 0 112906 1 6484 Name: previous_cancellations, dtype: int64 . df[&quot;previous_bookings_not_canceled&quot;] = df[&quot;previous_bookings_not_canceled&quot;].apply(lambda x: 0 if x == 0 else 1) df[&quot;previous_bookings_not_canceled&quot;].value_counts() . 0 115770 1 3620 Name: previous_bookings_not_canceled, dtype: int64 . Manejo de variables num&#233;ricas . Reservation_status_date como tipo Date . df[&quot;reservation_status_date&quot;] = pd.to_datetime(df[&quot;reservation_status_date&quot;], format= &quot;%Y/%m/%d&quot;) . df[&quot;reservation_status_date&quot;].dt.month.value_counts() . 7 12106 8 11249 10 11143 1 10681 5 10304 3 10230 4 9999 2 9498 9 9403 6 9278 11 8099 12 7400 Name: reservation_status_date, dtype: int64 . Creamos una columna de estaciones . df[&quot;reservation_status_seasone&quot;] = (df[&quot;reservation_status_date&quot;].dt.month - 1) // 3 . df.drop(columns=&quot;reservation_status_date&quot;, inplace=True) . Manejo de outliers . El valor minimo del ADR debe ser cero . df.drop(df[df[&quot;adr&quot;] &lt; 0].index, inplace=True) . df[&quot;adr&quot;].describe() . count 119389.000000 mean 101.832028 std 50.535032 min 0.000000 25% 69.290000 50% 94.590000 75% 126.000000 max 5400.000000 Name: adr, dtype: float64 . El valor minimo de adultos debe ser 1 . df.drop(df[df[&quot;adults&quot;] == 0].index, inplace=True) . df[&quot;adults&quot;].describe() . count 118986.000000 mean 1.862690 std 0.570062 min 1.000000 25% 2.000000 50% 2.000000 75% 2.000000 max 55.000000 Name: adults, dtype: float64 . df[&quot;adults&quot;] = df[&quot;adults&quot;].astype(int) . Creaci&#243;n de variables . Creamos la variable &quot;guests&quot; usando los valores de adults, children y babies, . df[&quot;guests&quot;] = df[&quot;adults&quot;] + df[&quot;children&quot;] + df[&quot;babies&quot;] . Creamos la variable &quot;total nights&quot; usando los valores de weekend y week nights . df[&quot;total_nights&quot;] = df[&quot;stays_in_weekend_nights&quot;] + df[&quot;stays_in_week_nights&quot;] . Nos quedamos solo con &quot;total nights&quot; para evitar redundancia . df.drop(columns = [&quot;stays_in_weekend_nights&quot;, &quot;stays_in_week_nights&quot;], inplace=True) . df.sample(5) . hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_day_of_month adults children babies country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled assigned_room_type deposit_type days_in_waiting_list customer_type adr reservation_status_seasone guests total_nights . 79187 City Hotel | 0 | 34 | 2015 | October | 19 | 2 | 0.0 | 0 | ESP | Offline TA/TO | TA/TO | 0 | 0 | 0 | D | No Deposit | 0 | Transient | 105.12 | 3 | 2.0 | 4 | . 107205 City Hotel | 0 | 18 | 2017 | March | 6 | 3 | 0.0 | 0 | CHE | Online TA | TA/TO | 0 | 0 | 0 | D | No Deposit | 0 | Transient | 158.00 | 0 | 3.0 | 3 | . 23613 Resort Hotel | 0 | 12 | 2016 | April | 27 | 2 | 0.0 | 0 | PRT | Groups | Direct | 0 | 0 | 0 | A | No Deposit | 0 | Transient-Party | 85.33 | 1 | 2.0 | 3 | . 66633 City Hotel | 1 | 100 | 2017 | April | 22 | 2 | 0.0 | 0 | PRT | Offline TA/TO | TA/TO | 0 | 0 | 0 | A | Non Refund | 0 | Transient | 105.00 | 0 | 2.0 | 4 | . 87019 City Hotel | 0 | 86 | 2016 | April | 9 | 2 | 0.0 | 0 | NLD | Offline TA/TO | TA/TO | 0 | 0 | 0 | B | No Deposit | 0 | Transient-Party | 80.75 | 1 | 2.0 | 4 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 118986 entries, 0 to 119389 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 hotel 118986 non-null object 1 is_canceled 118986 non-null int64 2 lead_time 118986 non-null int64 3 arrival_date_year 118986 non-null int64 4 arrival_date_month 118986 non-null object 5 arrival_date_day_of_month 118986 non-null int64 6 adults 118986 non-null int32 7 children 118982 non-null float64 8 babies 118986 non-null int64 9 country 118508 non-null object 10 market_segment 118986 non-null object 11 distribution_channel 118986 non-null object 12 is_repeated_guest 118986 non-null int64 13 previous_cancellations 118986 non-null int64 14 previous_bookings_not_canceled 118986 non-null int64 15 assigned_room_type 118986 non-null object 16 deposit_type 118986 non-null object 17 days_in_waiting_list 118986 non-null int64 18 customer_type 118986 non-null object 19 adr 118986 non-null float64 20 reservation_status_seasone 118986 non-null int64 21 guests 118982 non-null float64 22 total_nights 118986 non-null int64 dtypes: float64(3), int32(1), int64(11), object(8) memory usage: 21.3+ MB . Los nulos que quedan, la estandarización, el encoding y demás procesos se harán en el pipeline. . Creaci&#243;n del pipeline . Agrupamos las columnas con valores categóricos. . categorical = [cname for cname in df.columns if df[cname].nunique() and df[cname].dtype == &quot;object&quot;] print(categorical) print(&quot;-&quot;*90) print(len(categorical)) . [&#39;hotel&#39;, &#39;arrival_date_month&#39;, &#39;country&#39;, &#39;market_segment&#39;, &#39;distribution_channel&#39;, &#39;assigned_room_type&#39;, &#39;deposit_type&#39;, &#39;customer_type&#39;] 8 . Agrupamos las columnas con valores númericos. . numerical = [cname for cname in df.columns if df[cname].dtype in [&#39;int64&#39;, &#39;float64&#39;]] numerical.remove(&quot;is_canceled&quot;) print(numerical) print(&quot;-&quot;*90) print(len(numerical)) . [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;previous_bookings_not_canceled&#39;, &#39;days_in_waiting_list&#39;, &#39;adr&#39;, &#39;reservation_status_seasone&#39;, &#39;guests&#39;, &#39;total_nights&#39;] 13 . Activamos el truncateSVD para hacer reducción de dimensionalidad. . tsvd = TruncatedSVD() . Creamos un transformer para las variables numéricas que agregue valores en los nulos, estandarice y reduzca dimensionalidad . numerical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;mean&#39;)), (&#39;scaler&#39;, StandardScaler()), (&quot;tsvd&quot;, tsvd) ]) . Creamos otro para las variables categóricas que agregue valores en los nulos, haga OHE y reduzca dimensionalidad . categorical_transformer = Pipeline(steps = [ (&quot;imputer&quot;, SimpleImputer(strategy= &quot;most_frequent&quot;)), (&quot;onehot&quot;, OneHotEncoder(handle_unknown= &quot;ignore&quot;)), (&quot;tsvd&quot;, tsvd) ]) . preprocessor = ColumnTransformer( transformers=[ (&quot;num&quot;, numerical_transformer, numerical), (&quot;cat&quot;, categorical_transformer, categorical), ], remainder = &quot;passthrough&quot;) . y = df[&quot;is_canceled&quot;] X = df.drop([&quot;is_canceled&quot;], axis=1) . Divisi&#243;n del dataset . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =.3, random_state=42, stratify=y) . Despliegue de modelos . tree = DecisionTreeClassifier(random_state=42, class_weight=&#39;balanced&#39;) . tree_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;model&quot;, tree) ]) . Buscamos los mejores hiperparmetros para regularizar el modelo . tree_pipeline.get_params().keys() . dict_keys([&#39;memory&#39;, &#39;steps&#39;, &#39;verbose&#39;, &#39;preprocessor&#39;, &#39;model&#39;, &#39;preprocessor__n_jobs&#39;, &#39;preprocessor__remainder&#39;, &#39;preprocessor__sparse_threshold&#39;, &#39;preprocessor__transformer_weights&#39;, &#39;preprocessor__transformers&#39;, &#39;preprocessor__verbose&#39;, &#39;preprocessor__verbose_feature_names_out&#39;, &#39;preprocessor__num&#39;, &#39;preprocessor__cat&#39;, &#39;preprocessor__num__memory&#39;, &#39;preprocessor__num__steps&#39;, &#39;preprocessor__num__verbose&#39;, &#39;preprocessor__num__imputer&#39;, &#39;preprocessor__num__scaler&#39;, &#39;preprocessor__num__tsvd&#39;, &#39;preprocessor__num__imputer__add_indicator&#39;, &#39;preprocessor__num__imputer__copy&#39;, &#39;preprocessor__num__imputer__fill_value&#39;, &#39;preprocessor__num__imputer__missing_values&#39;, &#39;preprocessor__num__imputer__strategy&#39;, &#39;preprocessor__num__imputer__verbose&#39;, &#39;preprocessor__num__scaler__copy&#39;, &#39;preprocessor__num__scaler__with_mean&#39;, &#39;preprocessor__num__scaler__with_std&#39;, &#39;preprocessor__num__tsvd__algorithm&#39;, &#39;preprocessor__num__tsvd__n_components&#39;, &#39;preprocessor__num__tsvd__n_iter&#39;, &#39;preprocessor__num__tsvd__random_state&#39;, &#39;preprocessor__num__tsvd__tol&#39;, &#39;preprocessor__cat__memory&#39;, &#39;preprocessor__cat__steps&#39;, &#39;preprocessor__cat__verbose&#39;, &#39;preprocessor__cat__imputer&#39;, &#39;preprocessor__cat__onehot&#39;, &#39;preprocessor__cat__tsvd&#39;, &#39;preprocessor__cat__imputer__add_indicator&#39;, &#39;preprocessor__cat__imputer__copy&#39;, &#39;preprocessor__cat__imputer__fill_value&#39;, &#39;preprocessor__cat__imputer__missing_values&#39;, &#39;preprocessor__cat__imputer__strategy&#39;, &#39;preprocessor__cat__imputer__verbose&#39;, &#39;preprocessor__cat__onehot__categories&#39;, &#39;preprocessor__cat__onehot__drop&#39;, &#39;preprocessor__cat__onehot__dtype&#39;, &#39;preprocessor__cat__onehot__handle_unknown&#39;, &#39;preprocessor__cat__onehot__sparse&#39;, &#39;preprocessor__cat__tsvd__algorithm&#39;, &#39;preprocessor__cat__tsvd__n_components&#39;, &#39;preprocessor__cat__tsvd__n_iter&#39;, &#39;preprocessor__cat__tsvd__random_state&#39;, &#39;preprocessor__cat__tsvd__tol&#39;, &#39;model__ccp_alpha&#39;, &#39;model__class_weight&#39;, &#39;model__criterion&#39;, &#39;model__max_depth&#39;, &#39;model__max_features&#39;, &#39;model__max_leaf_nodes&#39;, &#39;model__min_impurity_decrease&#39;, &#39;model__min_samples_leaf&#39;, &#39;model__min_samples_split&#39;, &#39;model__min_weight_fraction_leaf&#39;, &#39;model__random_state&#39;, &#39;model__splitter&#39;]) . Hacemos un random search . param_dist = { &quot;preprocessor__num__tsvd__n_components&quot;: randint(5, 13), &quot;preprocessor__cat__tsvd__n_components&quot;: randint(5, 8), &quot;model__min_samples_leaf&quot;: randint(10, 100), &#39;model__min_samples_split&#39; : randint(2, 100), &#39;model__ccp_alpha&#39;: list(np.arange(0.0, 1., step=0.05)) } . rs = RandomizedSearchCV(estimator = tree_pipeline, n_iter=100, param_distributions = param_dist, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=5, random_state=42, verbose= 1) . %%time rs.fit(X_train, y_train) . Fitting 5 folds for each of 100 candidates, totalling 500 fits Wall time: 4min 58s . RandomizedSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;model__min_samples_split&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE538918E0&gt;, &#39;preprocessor__cat__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE53891CA0&gt;, &#39;preprocessor__num__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5387EB20&gt;}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . rs.best_params_ . {&#39;model__ccp_alpha&#39;: 0.0, &#39;model__min_samples_leaf&#39;: 18, &#39;model__min_samples_split&#39;: 44, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 12} . rs.best_score_ . 0.7616077335487703 . search_space_tree = { &#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12], &#39;model__max_depth&#39;: [10, 17, 20], &#39;model__min_samples_split&#39;: [42, 44], &#39;model__min_samples_leaf&#39;: [17, 18], } . gs = GridSearchCV(estimator = tree_pipeline, param_grid = search_space_tree, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 5, verbose = 5) . %%time gs.fit(X_train, y_train) . Fitting 5 folds for each of 48 candidates, totalling 240 fits Wall time: 2min 58s . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... &#39;deposit_type&#39;, &#39;customer_type&#39;])])), (&#39;model&#39;, DecisionTreeClassifier(class_weight=&#39;balanced&#39;, random_state=42))]), n_jobs=-1, param_grid={&#39;model__max_depth&#39;: [10, 17, 20], &#39;model__min_samples_leaf&#39;: [17, 18], &#39;model__min_samples_split&#39;: [42, 44], &#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=5) . score = gs.best_score_ score . 0.7619156331548821 . bp = gs.best_params_ bp . {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_leaf&#39;: 18, &#39;model__min_samples_split&#39;: 42, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 12} . pred_train = gs.predict(X_train) print(classification_report(y_train, pred_train)) . precision recall f1-score support 0 0.91 0.88 0.90 52410 1 0.81 0.86 0.83 30880 accuracy 0.87 83290 macro avg 0.86 0.87 0.86 83290 weighted avg 0.87 0.87 0.87 83290 . pred_test = gs.predict(X_test) print(classification_report(y_test, pred_test)) . precision recall f1-score support 0 0.87 0.84 0.86 22461 1 0.75 0.79 0.77 13235 accuracy 0.83 35696 macro avg 0.81 0.82 0.81 35696 weighted avg 0.83 0.83 0.83 35696 . pd.DataFrame(gs.cv_results_).sort_values(by=&quot;rank_test_f1&quot;) . mean_fit_time std_fit_time mean_score_time std_score_time param_model__max_depth param_model__min_samples_leaf param_model__min_samples_split param_preprocessor__cat__tsvd__n_components param_preprocessor__num__tsvd__n_components params split0_test_f1 split1_test_f1 split2_test_f1 split3_test_f1 split4_test_f1 mean_test_f1 std_test_f1 rank_test_f1 . 43 5.909593 | 0.352149 | 0.152775 | 0.023438 | 20 | 18 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.764428 | 0.762749 | 0.766819 | 0.755563 | 0.760019 | 0.761916 | 0.003873 | 1 | . 39 5.752533 | 0.367464 | 0.137742 | 0.032969 | 20 | 17 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.766283 | 0.763309 | 0.764757 | 0.755963 | 0.759150 | 0.761892 | 0.003799 | 2 | . 47 4.120231 | 0.743095 | 0.074698 | 0.030800 | 20 | 18 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.763757 | 0.764424 | 0.767201 | 0.754657 | 0.757861 | 0.761580 | 0.004608 | 3 | . 35 5.727160 | 0.528002 | 0.132678 | 0.021309 | 20 | 17 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.765735 | 0.762226 | 0.765700 | 0.756449 | 0.756529 | 0.761328 | 0.004152 | 4 | . 46 5.756054 | 0.231365 | 0.133034 | 0.008300 | 20 | 18 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762425 | 0.766105 | 0.763892 | 0.750373 | 0.755390 | 0.759637 | 0.005858 | 5 | . 41 5.663632 | 0.329802 | 0.158780 | 0.025666 | 20 | 18 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762486 | 0.763880 | 0.765496 | 0.754287 | 0.750889 | 0.759408 | 0.005750 | 6 | . 37 5.593201 | 0.358663 | 0.135199 | 0.011575 | 20 | 17 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.764592 | 0.763674 | 0.764103 | 0.754797 | 0.749691 | 0.759371 | 0.006046 | 7 | . 42 5.671552 | 0.482549 | 0.134612 | 0.015834 | 20 | 18 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762150 | 0.766201 | 0.762195 | 0.753074 | 0.753226 | 0.759369 | 0.005287 | 8 | . 38 5.326279 | 0.401823 | 0.145958 | 0.014537 | 20 | 17 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.761957 | 0.766412 | 0.761332 | 0.749922 | 0.757015 | 0.759327 | 0.005567 | 9 | . 45 5.545277 | 0.403864 | 0.136949 | 0.035746 | 20 | 18 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.762021 | 0.763899 | 0.765270 | 0.753521 | 0.751085 | 0.759159 | 0.005744 | 10 | . 33 5.452450 | 0.431252 | 0.147094 | 0.041165 | 20 | 17 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.764660 | 0.762265 | 0.761381 | 0.756090 | 0.751025 | 0.759084 | 0.004907 | 11 | . 34 5.631814 | 0.182973 | 0.136370 | 0.015566 | 20 | 17 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.760535 | 0.765312 | 0.760327 | 0.751518 | 0.757014 | 0.758941 | 0.004558 | 12 | . 32 5.200071 | 0.186772 | 0.150597 | 0.018287 | 20 | 17 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.760050 | 0.764669 | 0.758803 | 0.754770 | 0.754897 | 0.758638 | 0.003670 | 13 | . 36 5.205832 | 0.188727 | 0.136098 | 0.023822 | 20 | 17 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.759957 | 0.765523 | 0.761472 | 0.753748 | 0.749750 | 0.758090 | 0.005632 | 14 | . 44 6.017212 | 0.269164 | 0.147854 | 0.030257 | 20 | 18 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.760102 | 0.765679 | 0.759881 | 0.752082 | 0.749445 | 0.757438 | 0.005892 | 15 | . 40 5.521703 | 0.373788 | 0.145786 | 0.028774 | 20 | 18 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 20, &#39;model__min_samples_l... | 0.759539 | 0.764462 | 0.760592 | 0.753281 | 0.748905 | 0.757356 | 0.005545 | 16 | . 26 5.775870 | 0.169365 | 0.124763 | 0.007315 | 17 | 18 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759635 | 0.762282 | 0.760687 | 0.753786 | 0.748934 | 0.757065 | 0.004976 | 17 | . 29 5.442686 | 0.424129 | 0.162982 | 0.031839 | 17 | 18 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759709 | 0.757191 | 0.759077 | 0.751491 | 0.757690 | 0.757032 | 0.002916 | 18 | . 23 5.573463 | 0.447110 | 0.143347 | 0.027625 | 17 | 17 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760422 | 0.760298 | 0.759224 | 0.754334 | 0.749557 | 0.756767 | 0.004237 | 19 | . 27 5.562768 | 0.301798 | 0.152453 | 0.026827 | 17 | 18 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760155 | 0.759820 | 0.758599 | 0.756722 | 0.748430 | 0.756745 | 0.004328 | 20 | . 19 5.609279 | 0.179823 | 0.139316 | 0.034424 | 17 | 17 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.761472 | 0.759902 | 0.757056 | 0.755834 | 0.749313 | 0.756715 | 0.004207 | 21 | . 22 5.353494 | 0.597877 | 0.140425 | 0.027570 | 17 | 17 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759429 | 0.762238 | 0.758808 | 0.752536 | 0.750101 | 0.756622 | 0.004548 | 22 | . 30 5.405706 | 0.523146 | 0.138758 | 0.024643 | 17 | 18 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.758724 | 0.762427 | 0.760786 | 0.752353 | 0.747971 | 0.756452 | 0.005447 | 23 | . 18 5.340284 | 0.329550 | 0.118122 | 0.010312 | 17 | 17 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759586 | 0.762116 | 0.758478 | 0.753319 | 0.748210 | 0.756342 | 0.004974 | 24 | . 16 5.366972 | 0.369566 | 0.124007 | 0.024215 | 17 | 17 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759838 | 0.761351 | 0.759101 | 0.752141 | 0.749154 | 0.756317 | 0.004780 | 25 | . 20 4.855590 | 0.299030 | 0.142960 | 0.020580 | 17 | 17 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760715 | 0.760539 | 0.759486 | 0.752509 | 0.746450 | 0.755940 | 0.005628 | 26 | . 31 6.347360 | 0.327202 | 0.120130 | 0.010828 | 17 | 18 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759632 | 0.760041 | 0.759046 | 0.754569 | 0.745993 | 0.755856 | 0.005308 | 27 | . 24 5.259175 | 0.565762 | 0.123623 | 0.016688 | 17 | 18 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760047 | 0.760213 | 0.758373 | 0.752628 | 0.745966 | 0.755446 | 0.005482 | 28 | . 17 5.774585 | 0.356408 | 0.132201 | 0.020936 | 17 | 17 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760727 | 0.758933 | 0.757967 | 0.751886 | 0.746811 | 0.755265 | 0.005167 | 29 | . 21 5.718334 | 0.614460 | 0.160697 | 0.040621 | 17 | 17 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.760866 | 0.757819 | 0.757797 | 0.752102 | 0.747639 | 0.755245 | 0.004743 | 30 | . 28 5.322633 | 0.210339 | 0.139555 | 0.027456 | 17 | 18 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759452 | 0.760342 | 0.758840 | 0.751166 | 0.745976 | 0.755155 | 0.005641 | 31 | . 25 5.501385 | 0.263432 | 0.139574 | 0.013089 | 17 | 18 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 17, &#39;model__min_samples_l... | 0.759434 | 0.757225 | 0.758698 | 0.751745 | 0.748453 | 0.755111 | 0.004282 | 32 | . 9 5.172902 | 0.566289 | 0.134767 | 0.016438 | 10 | 18 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738211 | 0.734577 | 0.732961 | 0.726533 | 0.722196 | 0.730895 | 0.005763 | 33 | . 7 5.321097 | 0.341320 | 0.147221 | 0.022727 | 10 | 17 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.740003 | 0.734880 | 0.731690 | 0.727588 | 0.718374 | 0.730507 | 0.007300 | 34 | . 11 5.984165 | 0.308396 | 0.181516 | 0.024323 | 10 | 18 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.739367 | 0.734794 | 0.731912 | 0.727604 | 0.716532 | 0.730042 | 0.007765 | 35 | . 15 5.170612 | 0.244348 | 0.139610 | 0.024521 | 10 | 18 | 44 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.739474 | 0.734707 | 0.731882 | 0.727543 | 0.716544 | 0.730030 | 0.007779 | 36 | . 3 5.551793 | 0.557981 | 0.171418 | 0.056321 | 10 | 17 | 42 | 6 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.739441 | 0.734794 | 0.731865 | 0.727408 | 0.716355 | 0.729973 | 0.007855 | 37 | . 1 4.712395 | 0.380196 | 0.129655 | 0.024373 | 10 | 17 | 42 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738284 | 0.734827 | 0.732896 | 0.726337 | 0.716732 | 0.729815 | 0.007610 | 38 | . 5 5.143289 | 0.265635 | 0.156563 | 0.036970 | 10 | 17 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738846 | 0.734740 | 0.732343 | 0.726729 | 0.715682 | 0.729668 | 0.008017 | 39 | . 13 5.684822 | 0.246634 | 0.127606 | 0.014110 | 10 | 18 | 44 | 5 | 12 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.738318 | 0.734551 | 0.732867 | 0.726562 | 0.715253 | 0.729510 | 0.008077 | 40 | . 14 5.170579 | 0.331523 | 0.142670 | 0.029044 | 10 | 18 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.736111 | 0.731863 | 0.729397 | 0.728114 | 0.717235 | 0.728544 | 0.006278 | 41 | . 6 5.063322 | 0.442805 | 0.131332 | 0.021350 | 10 | 17 | 44 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.736565 | 0.731927 | 0.728715 | 0.727982 | 0.717327 | 0.728503 | 0.006355 | 42 | . 8 5.362791 | 0.333177 | 0.140469 | 0.042554 | 10 | 18 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.735071 | 0.732265 | 0.730214 | 0.728389 | 0.716082 | 0.728404 | 0.006549 | 43 | . 0 4.590914 | 0.331202 | 0.123669 | 0.027797 | 10 | 17 | 42 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.735020 | 0.732581 | 0.729609 | 0.728192 | 0.716378 | 0.728356 | 0.006439 | 44 | . 2 4.822303 | 0.391141 | 0.150000 | 0.035955 | 10 | 17 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.736060 | 0.731927 | 0.728809 | 0.728058 | 0.716913 | 0.728354 | 0.006377 | 45 | . 4 5.316231 | 0.158647 | 0.153395 | 0.018886 | 10 | 17 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.735550 | 0.732493 | 0.729515 | 0.728177 | 0.715972 | 0.728341 | 0.006687 | 46 | . 10 5.595861 | 0.525995 | 0.222177 | 0.086424 | 10 | 18 | 42 | 6 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.734943 | 0.731863 | 0.729032 | 0.728194 | 0.717235 | 0.728253 | 0.005997 | 47 | . 12 5.678165 | 0.310659 | 0.175668 | 0.040308 | 10 | 18 | 44 | 5 | 11 | {&#39;model__max_depth&#39;: 10, &#39;model__min_samples_l... | 0.734970 | 0.732413 | 0.729659 | 0.728192 | 0.715982 | 0.728243 | 0.006557 | 48 | . Entrenamos nuevamente el modelo usando los nuevos hiperparametros . final_tree_cv = cross_val_score(gs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) final_tree_cv . Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits Fitting 3 folds for each of 48 candidates, totalling 144 fits . array([0.80378292, 0.80038238, 0.80181624, 0.79704733, 0.79958445]) . cm = confusion_matrix(y_test, pred_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = gs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.9032494101726963 . Regresi&#243;n log&#237;stica . regression = LogisticRegression(random_state=0, class_weight=&#39;balanced&#39;) . regression_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;regression&quot;, regression) ]) . regression_pipeline.get_params().keys() . dict_keys([&#39;memory&#39;, &#39;steps&#39;, &#39;verbose&#39;, &#39;preprocessor&#39;, &#39;regression&#39;, &#39;preprocessor__n_jobs&#39;, &#39;preprocessor__remainder&#39;, &#39;preprocessor__sparse_threshold&#39;, &#39;preprocessor__transformer_weights&#39;, &#39;preprocessor__transformers&#39;, &#39;preprocessor__verbose&#39;, &#39;preprocessor__verbose_feature_names_out&#39;, &#39;preprocessor__num&#39;, &#39;preprocessor__cat&#39;, &#39;preprocessor__num__memory&#39;, &#39;preprocessor__num__steps&#39;, &#39;preprocessor__num__verbose&#39;, &#39;preprocessor__num__imputer&#39;, &#39;preprocessor__num__scaler&#39;, &#39;preprocessor__num__tsvd&#39;, &#39;preprocessor__num__imputer__add_indicator&#39;, &#39;preprocessor__num__imputer__copy&#39;, &#39;preprocessor__num__imputer__fill_value&#39;, &#39;preprocessor__num__imputer__missing_values&#39;, &#39;preprocessor__num__imputer__strategy&#39;, &#39;preprocessor__num__imputer__verbose&#39;, &#39;preprocessor__num__scaler__copy&#39;, &#39;preprocessor__num__scaler__with_mean&#39;, &#39;preprocessor__num__scaler__with_std&#39;, &#39;preprocessor__num__tsvd__algorithm&#39;, &#39;preprocessor__num__tsvd__n_components&#39;, &#39;preprocessor__num__tsvd__n_iter&#39;, &#39;preprocessor__num__tsvd__random_state&#39;, &#39;preprocessor__num__tsvd__tol&#39;, &#39;preprocessor__cat__memory&#39;, &#39;preprocessor__cat__steps&#39;, &#39;preprocessor__cat__verbose&#39;, &#39;preprocessor__cat__imputer&#39;, &#39;preprocessor__cat__onehot&#39;, &#39;preprocessor__cat__tsvd&#39;, &#39;preprocessor__cat__imputer__add_indicator&#39;, &#39;preprocessor__cat__imputer__copy&#39;, &#39;preprocessor__cat__imputer__fill_value&#39;, &#39;preprocessor__cat__imputer__missing_values&#39;, &#39;preprocessor__cat__imputer__strategy&#39;, &#39;preprocessor__cat__imputer__verbose&#39;, &#39;preprocessor__cat__onehot__categories&#39;, &#39;preprocessor__cat__onehot__drop&#39;, &#39;preprocessor__cat__onehot__dtype&#39;, &#39;preprocessor__cat__onehot__handle_unknown&#39;, &#39;preprocessor__cat__onehot__sparse&#39;, &#39;preprocessor__cat__tsvd__algorithm&#39;, &#39;preprocessor__cat__tsvd__n_components&#39;, &#39;preprocessor__cat__tsvd__n_iter&#39;, &#39;preprocessor__cat__tsvd__random_state&#39;, &#39;preprocessor__cat__tsvd__tol&#39;, &#39;regression__C&#39;, &#39;regression__class_weight&#39;, &#39;regression__dual&#39;, &#39;regression__fit_intercept&#39;, &#39;regression__intercept_scaling&#39;, &#39;regression__l1_ratio&#39;, &#39;regression__max_iter&#39;, &#39;regression__multi_class&#39;, &#39;regression__n_jobs&#39;, &#39;regression__penalty&#39;, &#39;regression__random_state&#39;, &#39;regression__solver&#39;, &#39;regression__tol&#39;, &#39;regression__verbose&#39;, &#39;regression__warm_start&#39;]) . Hacemos un random search . parameters ={ &#39;preprocessor__num__tsvd__n_components&#39;: randint(5, 13), &#39;preprocessor__cat__tsvd__n_components&#39;: randint(5, 8), &#39;regression__multi_class&#39;:[&quot;auto&quot;, &quot;ovr&quot;], &#39;regression__fit_intercept&#39;: [True,False], &#39;regression__intercept_scaling&#39;: randint(1, 10), &#39;regression__max_iter&#39;: randint(100, 500) } . randomregression = RandomizedSearchCV(estimator = regression_pipeline, n_iter=50, param_distributions = parameters, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=3, random_state=42, verbose= 1) . %%time randomregression.fit(X_train, y_train) . Fitting 3 folds for each of 50 candidates, totalling 150 fits Wall time: 1min 6s . RandomizedSearchCV(cv=3, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;preprocessor__num__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55F2F9A0&gt;, &#39;regression__fit_intercept&#39;: [True, False], &#39;regression__intercept_scaling&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5BF26310&gt;, &#39;regression__max_iter&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55F2FDF0&gt;, &#39;regression__multi_class&#39;: [&#39;auto&#39;, &#39;ovr&#39;]}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . randomregression.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 7, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;regression__fit_intercept&#39;: False, &#39;regression__intercept_scaling&#39;: 7, &#39;regression__max_iter&#39;: 351, &#39;regression__multi_class&#39;: &#39;auto&#39;} . randomregression.best_score_ . 0.6929315835247353 . search_logistic_regression = { &#39;preprocessor__cat__tsvd__n_components&#39;: [7, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12], &#39;regression__fit_intercept&#39;: [False], &#39;regression__intercept_scaling&#39;: [7, 5, 6], &#39;regression__max_iter&#39;: [351, 350], &#39;regression__multi_class&#39;: [&#39;auto&#39;] } . logisticgs = GridSearchCV(estimator = regression_pipeline, param_grid = search_logistic_regression, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 4, verbose = 4) . %%time logisticgs.fit(X_train, y_train) . Fitting 4 folds for each of 24 candidates, totalling 96 fits Wall time: 51.6 s . GridSearchCV(cv=4, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... LogisticRegression(class_weight=&#39;balanced&#39;, random_state=0))]), n_jobs=-1, param_grid={&#39;preprocessor__cat__tsvd__n_components&#39;: [7, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [11, 12], &#39;regression__fit_intercept&#39;: [False], &#39;regression__intercept_scaling&#39;: [7, 5, 6], &#39;regression__max_iter&#39;: [351, 350], &#39;regression__multi_class&#39;: [&#39;auto&#39;]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=4) . logisticgs.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 7, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;regression__fit_intercept&#39;: False, &#39;regression__intercept_scaling&#39;: 5, &#39;regression__max_iter&#39;: 350, &#39;regression__multi_class&#39;: &#39;auto&#39;} . logisticgs.best_score_ . 0.6928219124999713 . logisticgs_test = logisticgs.predict(X_test) print(classification_report(y_test, logisticgs_test)) . precision recall f1-score support 0 0.82 0.81 0.82 22461 1 0.69 0.70 0.70 13235 accuracy 0.77 35696 macro avg 0.76 0.76 0.76 35696 weighted avg 0.77 0.77 0.77 35696 . cross_val_score(logisticgs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) . Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits Fitting 4 folds for each of 24 candidates, totalling 96 fits . array([0.69400681, 0.69014763, 0.69361397, 0.69150212, 0.69391983]) . cm = confusion_matrix(y_test, logisticgs_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = logisticgs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.8511245004500685 . Random forest . randomf = RandomForestClassifier(random_state=42, class_weight=&#39;balanced&#39;) . randomf_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;randomf&quot;, randomf) ]) . randomf_pipeline.get_params().keys() . dict_keys([&#39;memory&#39;, &#39;steps&#39;, &#39;verbose&#39;, &#39;preprocessor&#39;, &#39;randomf&#39;, &#39;preprocessor__n_jobs&#39;, &#39;preprocessor__remainder&#39;, &#39;preprocessor__sparse_threshold&#39;, &#39;preprocessor__transformer_weights&#39;, &#39;preprocessor__transformers&#39;, &#39;preprocessor__verbose&#39;, &#39;preprocessor__verbose_feature_names_out&#39;, &#39;preprocessor__num&#39;, &#39;preprocessor__cat&#39;, &#39;preprocessor__num__memory&#39;, &#39;preprocessor__num__steps&#39;, &#39;preprocessor__num__verbose&#39;, &#39;preprocessor__num__imputer&#39;, &#39;preprocessor__num__scaler&#39;, &#39;preprocessor__num__tsvd&#39;, &#39;preprocessor__num__imputer__add_indicator&#39;, &#39;preprocessor__num__imputer__copy&#39;, &#39;preprocessor__num__imputer__fill_value&#39;, &#39;preprocessor__num__imputer__missing_values&#39;, &#39;preprocessor__num__imputer__strategy&#39;, &#39;preprocessor__num__imputer__verbose&#39;, &#39;preprocessor__num__scaler__copy&#39;, &#39;preprocessor__num__scaler__with_mean&#39;, &#39;preprocessor__num__scaler__with_std&#39;, &#39;preprocessor__num__tsvd__algorithm&#39;, &#39;preprocessor__num__tsvd__n_components&#39;, &#39;preprocessor__num__tsvd__n_iter&#39;, &#39;preprocessor__num__tsvd__random_state&#39;, &#39;preprocessor__num__tsvd__tol&#39;, &#39;preprocessor__cat__memory&#39;, &#39;preprocessor__cat__steps&#39;, &#39;preprocessor__cat__verbose&#39;, &#39;preprocessor__cat__imputer&#39;, &#39;preprocessor__cat__onehot&#39;, &#39;preprocessor__cat__tsvd&#39;, &#39;preprocessor__cat__imputer__add_indicator&#39;, &#39;preprocessor__cat__imputer__copy&#39;, &#39;preprocessor__cat__imputer__fill_value&#39;, &#39;preprocessor__cat__imputer__missing_values&#39;, &#39;preprocessor__cat__imputer__strategy&#39;, &#39;preprocessor__cat__imputer__verbose&#39;, &#39;preprocessor__cat__onehot__categories&#39;, &#39;preprocessor__cat__onehot__drop&#39;, &#39;preprocessor__cat__onehot__dtype&#39;, &#39;preprocessor__cat__onehot__handle_unknown&#39;, &#39;preprocessor__cat__onehot__sparse&#39;, &#39;preprocessor__cat__tsvd__algorithm&#39;, &#39;preprocessor__cat__tsvd__n_components&#39;, &#39;preprocessor__cat__tsvd__n_iter&#39;, &#39;preprocessor__cat__tsvd__random_state&#39;, &#39;preprocessor__cat__tsvd__tol&#39;, &#39;randomf__bootstrap&#39;, &#39;randomf__ccp_alpha&#39;, &#39;randomf__class_weight&#39;, &#39;randomf__criterion&#39;, &#39;randomf__max_depth&#39;, &#39;randomf__max_features&#39;, &#39;randomf__max_leaf_nodes&#39;, &#39;randomf__max_samples&#39;, &#39;randomf__min_impurity_decrease&#39;, &#39;randomf__min_samples_leaf&#39;, &#39;randomf__min_samples_split&#39;, &#39;randomf__min_weight_fraction_leaf&#39;, &#39;randomf__n_estimators&#39;, &#39;randomf__n_jobs&#39;, &#39;randomf__oob_score&#39;, &#39;randomf__random_state&#39;, &#39;randomf__verbose&#39;, &#39;randomf__warm_start&#39;]) . Hacemos un random search . param_dist = { &quot;preprocessor__num__tsvd__n_components&quot;: randint(5, 13), &quot;preprocessor__cat__tsvd__n_components&quot;: randint(5, 8), &#39;randomf__n_estimators&#39;: randint(100, 2000), &#39;randomf__max_depth&#39;: randint(10, 100), &#39;randomf__max_leaf_nodes&#39;: randint(3, 100), &#39;randomf__max_samples&#39;: randint(1, 100), &#39;randomf__min_samples_split&#39;: randint(4, 100) } . rsrandomf = RandomizedSearchCV(estimator = randomf_pipeline, n_iter=100, param_distributions = param_dist, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=4, random_state=42, verbose= 1) . %%time rsrandomf.fit(X_train, y_train) . Fitting 4 folds for each of 100 candidates, totalling 400 fits Wall time: 17min 23s . RandomizedSearchCV(cv=4, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;randomf__max_leaf_nodes&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55F247C0&gt;, &#39;randomf__max_samples&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55CBF520&gt;, &#39;randomf__min_samples_split&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55B69CA0&gt;, &#39;randomf__n_estimators&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE537BAA30&gt;}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . rsrandomf.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 5, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;randomf__max_depth&#39;: 26, &#39;randomf__max_leaf_nodes&#39;: 46, &#39;randomf__max_samples&#39;: 92, &#39;randomf__min_samples_split&#39;: 33, &#39;randomf__n_estimators&#39;: 576} . rsrandomf.best_score_ . 0.6644808092260801 . search_random_forest = { &#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [12, 11], &#39;randomf__max_depth&#39;: [23, 25, 26], &#39;randomf__max_samples&#39;: [93, 92], &#39;randomf__n_estimators&#39;: [576] } . randomgs = GridSearchCV(estimator = randomf_pipeline, param_grid = search_random_forest, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 5, verbose = 4) . %%time randomgs.fit(X_train, y_train) . Fitting 5 folds for each of 24 candidates, totalling 120 fits Wall time: 3min 24s . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... &#39;deposit_type&#39;, &#39;customer_type&#39;])])), (&#39;randomf&#39;, RandomForestClassifier(class_weight=&#39;balanced&#39;, random_state=42))]), n_jobs=-1, param_grid={&#39;preprocessor__cat__tsvd__n_components&#39;: [5, 6], &#39;preprocessor__num__tsvd__n_components&#39;: [12, 11], &#39;randomf__max_depth&#39;: [23, 25, 26], &#39;randomf__max_samples&#39;: [93, 92], &#39;randomf__n_estimators&#39;: [576]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=4) . randomgs.best_params_ . {&#39;preprocessor__cat__tsvd__n_components&#39;: 5, &#39;preprocessor__num__tsvd__n_components&#39;: 12, &#39;randomf__max_depth&#39;: 25, &#39;randomf__max_samples&#39;: 92, &#39;randomf__n_estimators&#39;: 576} . randomgs.best_score_ . 0.5972750870383681 . randomgs_test = randomgs.predict(X_test) print(classification_report(y_test, randomgs_test)) . precision recall f1-score support 0 0.75 0.98 0.85 22461 1 0.94 0.43 0.59 13235 accuracy 0.78 35696 macro avg 0.84 0.71 0.72 35696 weighted avg 0.82 0.78 0.76 35696 . cross_val_score(randomgs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) . Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits Fitting 5 folds for each of 24 candidates, totalling 120 fits . array([0.59878654, 0.59763248, 0.60024168, 0.58686229, 0.59151488]) . cm = confusion_matrix(y_test, randomgs_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = randomgs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.8616904872445909 . LightGBM . light = LGBMClassifier(random_state=0, class_weight=&#39;balanced&#39;) . light_pipeline = Pipeline(steps=[(&quot;preprocessor&quot;, preprocessor), (&quot;light&quot;, light) ]) . lgbm_parameters = { &#39;preprocessor__num__tsvd__n_components&#39;: randint(5, 12), &#39;preprocessor__cat__tsvd__n_components&#39;: randint(5, 8), &#39;light__n_estimators&#39;: randint(2000, 5000), &#39;light__max_depth&#39;: randint(1, 20), &#39;light__num_leaves&#39;: randint(10, 50), } . random_light = RandomizedSearchCV(estimator = light_pipeline, n_iter=100, param_distributions = lgbm_parameters, n_jobs=-1, scoring = [&quot;f1&quot;], refit = &quot;f1&quot;, cv=4, random_state=42, verbose= 1) . %%time random_light.fit(X_train, y_train) . Fitting 4 folds for each of 100 candidates, totalling 400 fits Wall time: 1h 1min 58s . RandomizedSearchCV(cv=4, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellation... &#39;light__num_leaves&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE55B8A1C0&gt;, &#39;preprocessor__cat__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5387EFD0&gt;, &#39;preprocessor__num__tsvd__n_components&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EE5BD06EB0&gt;}, random_state=42, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=1) . random_light.best_params_ . {&#39;light__max_depth&#39;: 15, &#39;light__n_estimators&#39;: 3115, &#39;light__num_leaves&#39;: 43, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 11} . random_light.best_score_ . 0.8377610877611602 . search_light = { &#39;light__max_depth&#39;: [15, 16, 17], &#39;light__n_estimators&#39;: [3115], &#39;light__num_leaves&#39;: [43, 44, 45], &#39;preprocessor__cat__tsvd__n_components&#39;: [6], &#39;preprocessor__num__tsvd__n_components&#39;: [11] } . lightgs = GridSearchCV(estimator = light_pipeline, param_grid = search_light, scoring = [&quot;f1&quot;], n_jobs=-1, refit = &quot;f1&quot;, cv= 5, verbose = 4) . %%time lightgs.fit(X_train, y_train) . Fitting 5 folds for each of 9 candidates, totalling 45 fits Wall time: 7min 33s . GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#39;preprocessor&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer()), (&#39;scaler&#39;, StandardScaler()), (&#39;tsvd&#39;, TruncatedSVD())]), [&#39;lead_time&#39;, &#39;arrival_date_year&#39;, &#39;arrival_date_day_of_month&#39;, &#39;children&#39;, &#39;babies&#39;, &#39;is_repeated_guest&#39;, &#39;previous_cancellations&#39;, &#39;pr... &#39;assigned_room_type&#39;, &#39;deposit_type&#39;, &#39;customer_type&#39;])])), (&#39;light&#39;, LGBMClassifier(class_weight=&#39;balanced&#39;, random_state=0))]), n_jobs=-1, param_grid={&#39;light__max_depth&#39;: [15, 16, 17], &#39;light__n_estimators&#39;: [3115], &#39;light__num_leaves&#39;: [43, 44, 45], &#39;preprocessor__cat__tsvd__n_components&#39;: [6], &#39;preprocessor__num__tsvd__n_components&#39;: [11]}, refit=&#39;f1&#39;, scoring=[&#39;f1&#39;], verbose=4) . lightgs.best_params_ . {&#39;light__max_depth&#39;: 16, &#39;light__n_estimators&#39;: 3115, &#39;light__num_leaves&#39;: 45, &#39;preprocessor__cat__tsvd__n_components&#39;: 6, &#39;preprocessor__num__tsvd__n_components&#39;: 11} . lightgs.best_score_ . 0.8406825152421916 . lightgs_test = lightgs.predict(X_test) print(classification_report(y_test, lightgs_test)) . precision recall f1-score support 0 0.90 0.94 0.92 22461 1 0.88 0.82 0.85 13235 accuracy 0.89 35696 macro avg 0.89 0.88 0.88 35696 weighted avg 0.89 0.89 0.89 35696 . cross_val_score(lightgs, X_train, y_train, cv=5, scoring=&quot;f1&quot;) . Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits Fitting 5 folds for each of 9 candidates, totalling 45 fits . array([0.84282116, 0.84072666, 0.83988882, 0.8364557 , 0.8356315 ]) . cm = confusion_matrix(y_test, lightgs_test) sns.heatmap(cm, annot=True, fmt=&quot;g&quot;) . &lt;AxesSubplot:&gt; . class_probabilities = lightgs.predict_proba(X_test) preds = class_probabilities[:, 1] fpr, tpr, threshold = roc_curve(y_test, preds) roc_auc = auc(fpr, tpr) # AUC print(f&quot;AUC for our classifier is: {roc_auc}&quot;) # Gráfica de la Curva ROC plt.title(&#39;Receiver Operating Characteristic&#39;) plt.plot(fpr, tpr, &#39;b&#39;, label = &#39;AUC = %0.2f&#39; % roc_auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0, 1], [0, 1],&#39;r--&#39;) plt.xlim([0, 1]) plt.ylim([0, 1]) plt.ylabel(&#39;Verdaderos positivos&#39;) plt.xlabel(&#39;Falsos positivos&#39;) plt.show() . AUC for our classifier is: 0.9515071138628284 .",
            "url": "https://pabloja4.github.io/portfolio/clasification/ensemble/pipeline/2021/11/02/Booking.html",
            "relUrl": "/clasification/ensemble/pipeline/2021/11/02/Booking.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Clustering para segmentación de clientes",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D from tensorflow.keras.layers import BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout from tensorflow.keras.models import Model, load_model from tensorflow.keras.initializers import glorot_uniform from tensorflow.keras.optimizers import SGD from sklearn.decomposition import PCA from sklearn.preprocessing import MinMaxScaler, StandardScaler from sklearn.cluster import KMeans %matplotlib inline . df = pd.read_csv(&quot;./Marketing_data.csv&quot;) . df.head() . CUST_ID BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE . 0 C10001 | 40.900749 | 0.818182 | 95.40 | 0.00 | 95.4 | 0.000000 | 0.166667 | 0.000000 | 0.083333 | 0.000000 | 0 | 2 | 1000.0 | 201.802084 | 139.509787 | 0.000000 | 12 | . 1 C10002 | 3202.467416 | 0.909091 | 0.00 | 0.00 | 0.0 | 6442.945483 | 0.000000 | 0.000000 | 0.000000 | 0.250000 | 4 | 0 | 7000.0 | 4103.032597 | 1072.340217 | 0.222222 | 12 | . 2 C10003 | 2495.148862 | 1.000000 | 773.17 | 773.17 | 0.0 | 0.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0 | 12 | 7500.0 | 622.066742 | 627.284787 | 0.000000 | 12 | . 3 C10004 | 1666.670542 | 0.636364 | 1499.00 | 1499.00 | 0.0 | 205.788017 | 0.083333 | 0.083333 | 0.000000 | 0.083333 | 1 | 1 | 7500.0 | 0.000000 | NaN | 0.000000 | 12 | . 4 C10005 | 817.714335 | 1.000000 | 16.00 | 16.00 | 0.0 | 0.000000 | 0.083333 | 0.083333 | 0.000000 | 0.000000 | 0 | 1 | 1200.0 | 678.334763 | 244.791237 | 0.000000 | 12 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 8950 entries, 0 to 8949 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 CUST_ID 8950 non-null object 1 BALANCE 8950 non-null float64 2 BALANCE_FREQUENCY 8950 non-null float64 3 PURCHASES 8950 non-null float64 4 ONEOFF_PURCHASES 8950 non-null float64 5 INSTALLMENTS_PURCHASES 8950 non-null float64 6 CASH_ADVANCE 8950 non-null float64 7 PURCHASES_FREQUENCY 8950 non-null float64 8 ONEOFF_PURCHASES_FREQUENCY 8950 non-null float64 9 PURCHASES_INSTALLMENTS_FREQUENCY 8950 non-null float64 10 CASH_ADVANCE_FREQUENCY 8950 non-null float64 11 CASH_ADVANCE_TRX 8950 non-null int64 12 PURCHASES_TRX 8950 non-null int64 13 CREDIT_LIMIT 8949 non-null float64 14 PAYMENTS 8950 non-null float64 15 MINIMUM_PAYMENTS 8637 non-null float64 16 PRC_FULL_PAYMENT 8950 non-null float64 17 TENURE 8950 non-null int64 dtypes: float64(14), int64(3), object(1) memory usage: 1.2+ MB . df.describe() . BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE . count 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8950.000000 | 8949.000000 | 8950.000000 | 8637.000000 | 8950.000000 | 8950.000000 | . mean 1564.474828 | 0.877271 | 1003.204834 | 592.437371 | 411.067645 | 978.871112 | 0.490351 | 0.202458 | 0.364437 | 0.135144 | 3.248827 | 14.709832 | 4494.449450 | 1733.143852 | 864.206542 | 0.153715 | 11.517318 | . std 2081.531879 | 0.236904 | 2136.634782 | 1659.887917 | 904.338115 | 2097.163877 | 0.401371 | 0.298336 | 0.397448 | 0.200121 | 6.824647 | 24.857649 | 3638.815725 | 2895.063757 | 2372.446607 | 0.292499 | 1.338331 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 50.000000 | 0.000000 | 0.019163 | 0.000000 | 6.000000 | . 25% 128.281915 | 0.888889 | 39.635000 | 0.000000 | 0.000000 | 0.000000 | 0.083333 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1600.000000 | 383.276166 | 169.123707 | 0.000000 | 12.000000 | . 50% 873.385231 | 1.000000 | 361.280000 | 38.000000 | 89.000000 | 0.000000 | 0.500000 | 0.083333 | 0.166667 | 0.000000 | 0.000000 | 7.000000 | 3000.000000 | 856.901546 | 312.343947 | 0.000000 | 12.000000 | . 75% 2054.140036 | 1.000000 | 1110.130000 | 577.405000 | 468.637500 | 1113.821139 | 0.916667 | 0.300000 | 0.750000 | 0.222222 | 4.000000 | 17.000000 | 6500.000000 | 1901.134317 | 825.485459 | 0.142857 | 12.000000 | . max 19043.138560 | 1.000000 | 49039.570000 | 40761.250000 | 22500.000000 | 47137.211760 | 1.000000 | 1.000000 | 1.000000 | 1.500000 | 123.000000 | 358.000000 | 30000.000000 | 50721.483360 | 76406.207520 | 1.000000 | 12.000000 | . Analizamos Overlays . df[df[&quot;PURCHASES&quot;] == 49039.570000] . CUST_ID BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE . 550 C10574 | 11547.52001 | 1.0 | 49039.57 | 40761.25 | 8278.32 | 558.166886 | 1.0 | 1.0 | 0.916667 | 0.083333 | 1 | 101 | 22500.0 | 46930.59824 | 2974.069421 | 0.25 | 12 | . df[df[&quot;CASH_ADVANCE&quot;] == 47137.211760] . CUST_ID BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE . 2159 C12226 | 10905.05381 | 1.0 | 431.93 | 133.5 | 298.43 | 47137.21176 | 0.583333 | 0.25 | 0.5 | 1.0 | 123 | 21 | 19600.0 | 39048.59762 | 5394.173671 | 0.0 | 12 | . Buscamos nulos . df.isna().sum() . CUST_ID 0 BALANCE 0 BALANCE_FREQUENCY 0 PURCHASES 0 ONEOFF_PURCHASES 0 INSTALLMENTS_PURCHASES 0 CASH_ADVANCE 0 PURCHASES_FREQUENCY 0 ONEOFF_PURCHASES_FREQUENCY 0 PURCHASES_INSTALLMENTS_FREQUENCY 0 CASH_ADVANCE_FREQUENCY 0 CASH_ADVANCE_TRX 0 PURCHASES_TRX 0 CREDIT_LIMIT 1 PAYMENTS 0 MINIMUM_PAYMENTS 313 PRC_FULL_PAYMENT 0 TENURE 0 dtype: int64 . Tratamos los valores nulos . df[&quot;MINIMUM_PAYMENTS&quot;]= df[&quot;MINIMUM_PAYMENTS&quot;].fillna(method=&quot;ffill&quot;) . Tratamos las variables faltantes . df[&quot;CREDIT_LIMIT&quot;]= df[&quot;CREDIT_LIMIT&quot;].fillna(method=&quot;ffill&quot;) . ¿Existen entradas duplicadas? . df.duplicated().sum() . 0 . Eliminamos variables no necesarias . df.drop(columns=&quot;CUST_ID&quot;, inplace = True) . EDA . n = len(df.columns) plt.figure(figsize = (5, 50)) for i in range(n): plt.subplot(n, 1, i+1) sns.histplot(df[df.columns[i]], kde_kws = {&quot;color&quot;:&quot;r&quot;, &quot;lw&quot;: 5, &quot;label&quot;: &quot;KDE&quot;}) plt.title(df.columns[i]) plt.tight_layout() . Una gran cantidad de usuarios tienen poco dinero en su cuenta, un monto menor a 2500. | La mayoria de los clientes hacen movimientos constantes. | Parte de los clientes no usa su tarjeta de crédito para hacer compras, otra parte la usa con mucha frecuencia. | Los usuarios que hacen compras directas son pocos. | La gente no suele pedir adelantos. | El limite de credito suele ser 5000 | Muchos de estos clientes lleva varios años en el banco. | . Correlaci&#243;n de las variables . f, ax = plt.subplots(figsize = (20, 20)) sns.heatmap(df.corr(), annot=True, cmap=&quot;RdYlGn&quot;) . &lt;AxesSubplot:&gt; . Escalamos los datos . scaler = StandardScaler() df_scaled = scaler.fit_transform(df) . df_scaled.shape . (8950, 17) . df_scaled . array([[-0.73198937, -0.24943448, -0.42489974, ..., -0.3053336 , -0.52555097, 0.36067954], [ 0.78696085, 0.13432467, -0.46955188, ..., 0.08714014, 0.2342269 , 0.36067954], [ 0.44713513, 0.51808382, -0.10766823, ..., -0.10010994, -0.52555097, 0.36067954], ..., [-0.7403981 , -0.18547673, -0.40196519, ..., -0.32935392, 0.32919999, -4.12276757], [-0.74517423, -0.18547673, -0.46955188, ..., -0.34057185, 0.32919999, -4.12276757], [-0.57257511, -0.88903307, 0.04214581, ..., -0.32688396, -0.52555097, -4.12276757]]) . Buscamos el numero K optimo con el método del codo . score_1 = [] range_values = range(1, 10) for i in range_values: kmeans = KMeans(n_clusters = i) kmeans.fit(df_scaled) score_1.append(kmeans.inertia_) plt.plot(range_values, score_1, &quot;bx-&quot;) plt.title(&quot;Cluster óptimo, método del codo&quot;) plt.xlabel(&quot;Cluster&quot;) plt.ylabel(&quot;wcss[k]&quot;) plt.show() . kmeans = KMeans(8) kmeans.fit(df_scaled) labels = kmeans.labels_ . kmeans.cluster_centers_.shape . (8, 17) . cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns=[df.columns]) cluster_centers . BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE . 0 1.568888 | 0.370746 | -0.229788 | -0.162065 | -0.245657 | 1.957159 | -0.485303 | -0.208406 | -0.434103 | 1.923017 | 1.861696 | -0.281118 | 0.954808 | 0.743978 | 0.346482 | -0.390602 | -0.104078 | . 1 -0.010770 | 0.371640 | -0.362072 | -0.245255 | -0.405451 | -0.078006 | -0.870118 | -0.402640 | -0.768313 | 0.142279 | -0.022701 | -0.490062 | -0.340454 | -0.266158 | -0.056275 | -0.454730 | -0.004587 | . 2 -0.698186 | -2.130099 | -0.317968 | -0.236205 | -0.317777 | -0.307237 | -0.566127 | -0.433397 | -0.461894 | -0.484350 | -0.360360 | -0.427697 | -0.214129 | -0.222520 | -0.269903 | 0.253007 | -0.159442 | . 3 0.995658 | 0.467020 | 2.461007 | 1.839828 | 2.439012 | -0.159306 | 1.161469 | 1.582384 | 1.261206 | -0.281606 | -0.150832 | 3.055189 | 1.274953 | 1.437698 | 0.350693 | 0.276077 | 0.338404 | . 4 1.819469 | 0.301643 | 12.207777 | 12.296446 | 6.272587 | 0.242957 | 1.002411 | 2.032081 | 0.866148 | -0.392172 | -0.124384 | 4.369530 | 3.261204 | 8.782991 | 0.983088 | 1.152351 | 0.300900 | . 5 -0.399106 | 0.306456 | -0.062625 | -0.257772 | 0.325451 | -0.360575 | 0.981288 | -0.414966 | 1.176334 | -0.460722 | -0.357442 | 0.136059 | -0.320913 | -0.242821 | -0.108020 | 0.362388 | -0.028869 | . 6 1.101446 | 0.088694 | -0.079810 | -0.278429 | 0.322153 | -0.039014 | -0.066883 | -0.536512 | 0.173728 | -0.268607 | -0.097221 | 0.131387 | -0.044379 | -0.141541 | 10.970774 | -0.525551 | 0.141975 | . 7 -0.122684 | 0.394978 | 0.501154 | 0.622501 | 0.041181 | -0.328081 | 0.952476 | 1.827281 | 0.172826 | -0.404520 | -0.328081 | 0.585319 | 0.417131 | 0.112907 | -0.153902 | 0.391319 | 0.200115 | . cluster_centers = scaler.inverse_transform(cluster_centers) cluster_centers = pd.DataFrame(data = cluster_centers, columns=[df.columns]) cluster_centers . BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE . 0 4829.982706 | 0.965097 | 512.258775 | 323.442798 | 188.923337 | 5083.124206 | 0.295575 | 0.140286 | 0.191914 | 0.519960 | 15.953537 | 7.722281 | 7968.378612 | 3886.886040 | 1688.743626 | 0.039470 | 11.378036 | . 1 1542.057698 | 0.965309 | 229.633192 | 185.363636 | 44.422938 | 815.288161 | 0.141130 | 0.082342 | 0.059090 | 0.163616 | 3.093911 | 2.528724 | 3255.681688 | 962.643042 | 731.472579 | 0.020714 | 11.511180 | . 2 111.260110 | 0.372670 | 323.861346 | 200.385151 | 123.705739 | 334.581498 | 0.263136 | 0.073167 | 0.180869 | 0.038221 | 0.789637 | 4.078886 | 3715.305061 | 1088.970726 | 223.722263 | 0.227715 | 11.303944 | . 3 3636.853500 | 0.987903 | 6261.185068 | 3646.175339 | 2616.635745 | 644.799306 | 0.956504 | 0.674513 | 0.865673 | 0.078792 | 2.219512 | 90.650407 | 9133.197832 | 5895.139975 | 1698.751279 | 0.234462 | 11.970190 | . 4 5351.546106 | 0.948727 | 27085.309200 | 21002.018800 | 6083.290400 | 1488.363253 | 0.892667 | 0.808667 | 0.708667 | 0.056667 | 2.400000 | 123.320000 | 16360.000000 | 27159.043527 | 3201.825604 | 0.490758 | 11.920000 | . 5 733.769471 | 0.949867 | 869.404666 | 164.588934 | 705.368689 | 222.729463 | 0.884189 | 0.078665 | 0.831943 | 0.042949 | 0.809546 | 18.091752 | 3326.778676 | 1030.201437 | 608.484219 | 0.259707 | 11.478684 | . 6 3857.042652 | 0.898282 | 832.689512 | 130.302683 | 702.386829 | 897.055936 | 0.463507 | 0.042406 | 0.433481 | 0.081393 | 2.585366 | 17.975610 | 4332.926829 | 1323.395577 | 26940.529021 | 0.000000 | 11.707317 | . 7 1309.119152 | 0.970837 | 2073.927463 | 1625.660785 | 448.307174 | 290.869232 | 0.872625 | 0.747571 | 0.433123 | 0.054196 | 1.009917 | 29.258678 | 6012.090158 | 2059.997741 | 499.430937 | 0.268169 | 11.785124 | . y_kmeans = kmeans.fit_predict(df_scaled) y_kmeans . array([1, 7, 0, ..., 5, 5, 5]) . df_cluster = pd.concat([df, pd.DataFrame({&quot;CLUSTER&quot;: labels})], axis = 1) df_cluster.sample(10) . BALANCE BALANCE_FREQUENCY PURCHASES ONEOFF_PURCHASES INSTALLMENTS_PURCHASES CASH_ADVANCE PURCHASES_FREQUENCY ONEOFF_PURCHASES_FREQUENCY PURCHASES_INSTALLMENTS_FREQUENCY CASH_ADVANCE_FREQUENCY CASH_ADVANCE_TRX PURCHASES_TRX CREDIT_LIMIT PAYMENTS MINIMUM_PAYMENTS PRC_FULL_PAYMENT TENURE CLUSTER . 2594 899.195559 | 0.909091 | 921.94 | 256.10 | 665.84 | 5903.784384 | 0.916667 | 0.250000 | 0.833333 | 0.333333 | 6 | 20 | 5000.0 | 6368.638423 | 288.469452 | 0.090909 | 12 | 0 | . 4110 1634.559235 | 1.000000 | 1062.94 | 1006.18 | 56.76 | 0.000000 | 0.583333 | 0.500000 | 0.166667 | 0.000000 | 0 | 15 | 1800.0 | 584.660133 | 1332.692469 | 0.000000 | 12 | 7 | . 467 52.229659 | 1.000000 | 565.50 | 0.00 | 565.50 | 101.137379 | 1.000000 | 0.000000 | 1.000000 | 0.083333 | 1 | 14 | 8500.0 | 2768.795805 | 199.803721 | 0.250000 | 12 | 5 | . 7367 4135.663046 | 1.000000 | 0.00 | 0.00 | 0.00 | 3168.349450 | 0.000000 | 0.000000 | 0.000000 | 0.500000 | 22 | 0 | 4500.0 | 1360.105727 | 1434.932593 | 0.000000 | 12 | 0 | . 7900 1712.340703 | 1.000000 | 1711.36 | 401.70 | 1309.66 | 0.000000 | 1.000000 | 0.250000 | 1.000000 | 0.000000 | 0 | 17 | 3500.0 | 1389.388725 | 1106.494745 | 0.000000 | 12 | 5 | . 3839 1382.376636 | 1.000000 | 77.91 | 0.00 | 77.91 | 300.531629 | 0.083333 | 0.000000 | 0.083333 | 0.166667 | 2 | 1 | 2500.0 | 639.362373 | 397.789389 | 0.000000 | 12 | 1 | . 1388 198.313970 | 0.909091 | 487.61 | 487.61 | 0.00 | 0.000000 | 0.166667 | 0.166667 | 0.000000 | 0.000000 | 0 | 8 | 3000.0 | 90.171900 | 198.000106 | 0.000000 | 12 | 1 | . 1347 130.977369 | 1.000000 | 527.00 | 415.00 | 112.00 | 0.000000 | 0.416667 | 0.250000 | 0.083333 | 0.000000 | 0 | 6 | 7500.0 | 493.086038 | 169.348870 | 0.300000 | 12 | 1 | . 5760 3160.087236 | 1.000000 | 0.00 | 0.00 | 0.00 | 757.003109 | 0.000000 | 0.000000 | 0.000000 | 0.083333 | 1 | 0 | 8000.0 | 744.908950 | 715.509134 | 0.000000 | 12 | 1 | . 8933 735.652303 | 1.000000 | 619.60 | 255.62 | 363.98 | 546.902403 | 1.000000 | 0.166667 | 0.833333 | 0.166667 | 5 | 16 | 1000.0 | 106.138603 | 337.294767 | 0.000000 | 6 | 5 | . for i in df.columns: plt.figure(figsize=(35, 5)) for j in range(8): plt.subplot(1, 8, j+1) cluster = df_cluster[df_cluster[&quot;CLUSTER&quot;]==j] cluster[i].hist(bins=20) plt.title(&quot;{} nCluster {}&quot;.format(i, j)) plt.show() . pca = PCA(n_components=2) principal_comp = pca.fit_transform(df_scaled) principal_comp . array([[-1.68059228, -1.07743971], [-1.14087901, 2.50597529], [ 0.97032296, -0.3821362 ], ..., [-0.92363727, -1.81049545], [-2.33494844, -0.65812574], [-0.55482262, -0.39780404]]) . pca_df = pd.DataFrame(data= principal_comp, columns=[&quot;ACP_1&quot;, &quot;ACP_2&quot;]) pca_df.head() . ACP_1 ACP_2 . 0 -1.680592 | -1.077440 | . 1 -1.140879 | 2.505975 | . 2 0.970323 | -0.382136 | . 3 -0.879490 | 0.026139 | . 4 -1.598361 | -0.689879 | . pca_df = pd.concat([pca_df, pd.DataFrame({&quot;CLUSTER&quot; : labels})], axis=1) pca_df.head() . ACP_1 ACP_2 CLUSTER . 0 -1.680592 | -1.077440 | 1 | . 1 -1.140879 | 2.505975 | 0 | . 2 0.970323 | -0.382136 | 7 | . 3 -0.879490 | 0.026139 | 1 | . 4 -1.598361 | -0.689879 | 1 | . plt.figure(figsize=(10,10)) ax = sns.scatterplot(x=&quot;ACP_1&quot;, y=&quot;ACP_2&quot;, data= pca_df, hue=&quot;CLUSTER&quot;, palette=[&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;purple&quot;, &quot;orange&quot;, &quot;pink&quot;, &quot;black&quot;, &quot;blue&quot;]) . encoding_dim = 7 input_df = Input(shape = (17, )) x = Dense(encoding_dim, activation = &quot;relu&quot;)(input_df) x = Dense(500, activation = &quot;relu&quot;, kernel_initializer = &quot;glorot_uniform&quot;)(x) x = Dense(500, activation = &quot;relu&quot;, kernel_initializer = &quot;glorot_uniform&quot;)(x) x = Dense(2000, activation = &quot;relu&quot;, kernel_initializer = &quot;glorot_uniform&quot;)(x) encoded = Dense(10, activation = &quot;relu&quot;, kernel_initializer = &quot;glorot_uniform&quot;)(x) x = Dense(2000, activation = &quot;relu&quot;, kernel_initializer = &quot;glorot_uniform&quot;)(encoded) x = Dense(500, activation = &quot;relu&quot;, kernel_initializer = &quot;glorot_uniform&quot;)(x) decoded = Dense(17, kernel_initializer = &quot;glorot_uniform&quot;)(x) autoencoder = Model(input_df, decoded) encoder = Model(input_df, encoded) autoencoder.compile(optimizer = &quot;adam&quot;, loss = &quot;mean_squared_error&quot;) . autoencoder.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 17)] 0 dense (Dense) (None, 7) 126 dense_1 (Dense) (None, 500) 4000 dense_2 (Dense) (None, 500) 250500 dense_3 (Dense) (None, 2000) 1002000 dense_4 (Dense) (None, 10) 20010 dense_5 (Dense) (None, 2000) 22000 dense_6 (Dense) (None, 500) 1000500 dense_7 (Dense) (None, 17) 8517 ================================================================= Total params: 2,307,653 Trainable params: 2,307,653 Non-trainable params: 0 _________________________________________________________________ . autoencoder.fit(df_scaled, df_scaled, batch_size=128, epochs = 25, verbose = 1) . Epoch 1/25 70/70 [==============================] - 3s 32ms/step - loss: 0.5328 Epoch 2/25 70/70 [==============================] - 2s 34ms/step - loss: 0.3169 Epoch 3/25 70/70 [==============================] - 2s 35ms/step - loss: 0.2384 Epoch 4/25 70/70 [==============================] - 2s 33ms/step - loss: 0.2080 Epoch 5/25 70/70 [==============================] - 2s 30ms/step - loss: 0.1679 Epoch 6/25 70/70 [==============================] - 2s 30ms/step - loss: 0.1427 Epoch 7/25 70/70 [==============================] - 2s 30ms/step - loss: 0.1273 Epoch 8/25 70/70 [==============================] - 2s 29ms/step - loss: 0.1200 Epoch 9/25 70/70 [==============================] - 2s 30ms/step - loss: 0.1164 Epoch 10/25 70/70 [==============================] - 2s 31ms/step - loss: 0.1045 Epoch 11/25 70/70 [==============================] - 2s 32ms/step - loss: 0.1000 Epoch 12/25 70/70 [==============================] - 2s 29ms/step - loss: 0.0907 Epoch 13/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0853 Epoch 14/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0846 Epoch 15/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0841 Epoch 16/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0775 Epoch 17/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0725 Epoch 18/25 70/70 [==============================] - 2s 32ms/step - loss: 0.0704 Epoch 19/25 70/70 [==============================] - 2s 32ms/step - loss: 0.0712 Epoch 20/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0645 Epoch 21/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0646 Epoch 22/25 70/70 [==============================] - 2s 30ms/step - loss: 0.0601 Epoch 23/25 70/70 [==============================] - 2s 31ms/step - loss: 0.0563 Epoch 24/25 70/70 [==============================] - 2s 32ms/step - loss: 0.0728 Epoch 25/25 70/70 [==============================] - 2s 32ms/step - loss: 0.0618 . &lt;keras.callbacks.History at 0x15118942eb0&gt; . autoencoder.save_weights(&quot;autoencoder.h5&quot;) . pred = encoder.predict(df_scaled) . score_1 = [] range_values = range(1, 10) for i in range_values: kmeans = KMeans(n_clusters = i) kmeans.fit(pred) score_1.append(kmeans.inertia_) plt.plot(range_values, score_1, &quot;bx-&quot;) plt.title(&quot;Cluster óptimo, método del codo&quot;) plt.xlabel(&quot;Cluster&quot;) plt.ylabel(&quot;wcss[k]&quot;) plt.show() . kmeans_auto = KMeans(6) kmeans_auto.fit(pred) labels = kmeans_auto.labels_ y_kmeans = kmeans.fit_predict(pred) y_kmeans . array([6, 0, 6, ..., 8, 1, 6]) . pca_auto = PCA(n_components=2) principal_auto = pca_auto.fit_transform(pred) principal_auto . array([[-1.5016032 , -0.34306574], [ 1.881164 , -2.334829 ], [-1.1413456 , 0.22069591], ..., [-0.49576515, 0.5711758 ], [-0.29179198, -0.6917084 ], [-0.7262886 , -0.12145777]], dtype=float32) . pca_auto = pd.DataFrame(data= principal_auto, columns=[&quot;ACP_1&quot;, &quot;ACP_2&quot;]) pca_auto.head() . ACP_1 ACP_2 . 0 -1.501603 | -0.343066 | . 1 1.881164 | -2.334829 | . 2 -1.141346 | 0.220696 | . 3 -0.513584 | -0.549184 | . 4 -1.465601 | -0.421032 | . pca_df = pd.concat([pca_auto, pd.DataFrame({&quot;CLUSTER&quot; : labels})], axis=1) pca_df.head() . ACP_1 ACP_2 CLUSTER . 0 -1.501603 | -0.343066 | 0 | . 1 1.881164 | -2.334829 | 1 | . 2 -1.141346 | 0.220696 | 0 | . 3 -0.513584 | -0.549184 | 1 | . 4 -1.465601 | -0.421032 | 0 | . plt.figure(figsize=(10,10)) ax = sns.scatterplot(x=&quot;ACP_1&quot;, y=&quot;ACP_2&quot;, data= pca_df, hue=&quot;CLUSTER&quot;, palette=[&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;purple&quot;, &quot;orange&quot;, &quot;pink&quot;]) .",
            "url": "https://pabloja4.github.io/portfolio/clustering/kmeans/pca/2021/10/13/Marketing.html",
            "relUrl": "/clustering/kmeans/pca/2021/10/13/Marketing.html",
            "date": " • Oct 13, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Comunicador Social y Periodista egresado de La Universidad de Manizales con formación en Inteligencia Artificial, Machine Learning (ML) y Ciencia de Datos. Experiencia desarrollando modelos supervisados de clasificación y regresión en ML. Conocimientos e interés en modelos no supervisados de clustering y reportería de datos. . Orientado al logro, con habilidades como la tolerancia al error, atención al detalle, resolución de problemas, trabajo en equipo y análisis de datos. Manejo de herramientas como Power BI, SQL server. Lenguajes como SQL y Python, con librerías tales como Pandas, Numpy, Seaborn, Matplotlib y Sklearn. . https://www.linkedin.com/in/pablojaramillojaramillo/ .",
          "url": "https://pabloja4.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pabloja4.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}